{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/AslanDevbrat/Seq2Seq/blob/dev/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"## Setup\n","metadata":{"id":"XtD33RcAAQeV"}},{"cell_type":"code","source":"%%capture\n!pip install wandb --upgrade\n!pip install tensorflow-addons","metadata":{"id":"UBUz4pd87YeP","execution":{"iopub.status.busy":"2022-06-23T14:14:52.113343Z","iopub.execute_input":"2022-06-23T14:14:52.113970Z","iopub.status.idle":"2022-06-23T14:15:15.686849Z","shell.execute_reply.started":"2022-06-23T14:14:52.113875Z","shell.execute_reply":"2022-06-23T14:15:15.685641Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Embedding, SimpleRNNCell, GRUCell, Dense, LSTMCell\nfrom tensorflow.keras import Input\nimport pandas as pd\nfrom numpy import argmax\nfrom math import log\nimport pprint\nimport math\nimport wandb\nimport os\nimport io\nfrom wandb.keras import WandbCallback\nimport time\nimport sys\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\")\n\n#wandb.login(key=wandb_api)\n! wandb login $wandb_api\n\nos.environ[\"WANDB_SILENT\"] = \"false\"\nwandb.login()","metadata":{"id":"EnvOeSibAQeV","outputId":"29e1a5a5-f33e-4558-bd51-d007e30b32a0","execution":{"iopub.status.busy":"2022-06-23T14:15:15.689844Z","iopub.execute_input":"2022-06-23T14:15:15.691518Z","iopub.status.idle":"2022-06-23T14:15:24.877317Z","shell.execute_reply.started":"2022-06-23T14:15:15.691468Z","shell.execute_reply":"2022-06-23T14:15:24.876245Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maslan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Download the data\n","metadata":{"id":"NbOpYTqYAQeX"}},{"cell_type":"markdown","source":"## Configuration\n","metadata":{"id":"1rKnaKIZAQeZ"}},{"cell_type":"code","source":"!wget  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n!tar -xf 'dakshina_dataset_v1.0.tar'\ntrain_file_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\nval_file_path= \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\ntest_file_path  = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"","metadata":{"id":"roFBlOuMAYkx","outputId":"835ba7da-83ac-4de8-acf2-5f90ce20440a","execution":{"iopub.status.busy":"2022-06-23T14:15:34.494255Z","iopub.execute_input":"2022-06-23T14:15:34.494868Z","iopub.status.idle":"2022-06-23T14:15:53.165631Z","shell.execute_reply.started":"2022-06-23T14:15:34.494836Z","shell.execute_reply":"2022-06-23T14:15:53.163958Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2022-06-23 14:15:35--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 74.125.134.128, 142.251.107.128, 142.250.98.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|74.125.134.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G   145MB/s    in 11s     \n\n2022-06-23 14:15:45 (182 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 100000  # Number of samples to train on.\n# Path to the data txt file on disk.\ndata_path = train_file_path\n","metadata":{"id":"akMCxfVHAQeZ","execution":{"iopub.status.busy":"2022-06-23T14:16:58.842910Z","iopub.execute_input":"2022-06-23T14:16:58.843619Z","iopub.status.idle":"2022-06-23T14:16:58.848989Z","shell.execute_reply.started":"2022-06-23T14:16:58.843581Z","shell.execute_reply":"2022-06-23T14:16:58.847912Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data\n","metadata":{"id":"U-3Djb36AQea"}},{"cell_type":"code","source":"def processData(filename,input_chars=set(),target_chars=set()):\n  input=[]\n  target=[]\n  with open(filename, \"r\", encoding=\"utf-8\") as f:\n    lines = f.read().split(\"\\n\")\n  for line in lines[:len(lines)-1]:\n      t_text,i_text, attestation = line.split(\"\\t\")\n       # We use \"\\t\" as the \"start sequence\" character and \"\\n\" as \"end sequence\" character for the target text.\n      input.append(\"\\t\"+i_text+\"\\n\")\n      target.append(\"\\t\"+t_text+\"\\n\")\n      for char in i_text:\n        if char not in input_chars:\n            input_chars.add(char)\n      for char in t_text:\n        if char not in target_chars:\n            target_chars.add(char)\n  target_chars.add(\"\\t\")\n  target_chars.add(\"\\n\")\n  input_chars.add(\"\\t\")\n  input_chars.add(\"\\n\")\n\n  input_chars = sorted(list(input_chars))\n  target_chars = sorted(list(target_chars))\n  num_encoder_tokens = len(input_chars)\n  num_decoder_tokens = len(target_chars)\n  max_encoder_seq_length = max([len(txt) for txt in input])\n  max_decoder_seq_length = max([len(txt) for txt in target])\n  return input,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length     ","metadata":{"id":"5CZtxlJmaYmb","execution":{"iopub.status.busy":"2022-06-23T14:17:00.868203Z","iopub.execute_input":"2022-06-23T14:17:00.869375Z","iopub.status.idle":"2022-06-23T14:17:00.880164Z","shell.execute_reply.started":"2022-06-23T14:17:00.869325Z","shell.execute_reply":"2022-06-23T14:17:00.879108Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Vectorize the data.\ninput,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length=processData(train_file_path)\nprint(\"Number of samples:\", len(input))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)","metadata":{"id":"pg8AcuMoab8U","outputId":"17e4d632-c169-456a-dbd8-8ad4ccd3b3f4","execution":{"iopub.status.busy":"2022-06-23T14:17:01.897525Z","iopub.execute_input":"2022-06-23T14:17:01.898413Z","iopub.status.idle":"2022-06-23T14:17:02.075027Z","shell.execute_reply.started":"2022-06-23T14:17:01.898368Z","shell.execute_reply":"2022-06-23T14:17:02.073897Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Number of samples: 44204\nNumber of unique input tokens: 28\nNumber of unique output tokens: 65\nMax sequence length for inputs: 22\nMax sequence length for outputs: 21\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vectorize the data.\n# Vectorize the data.\nvalidation_input,validation_target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, validation_max_encoder_seq_length, validation_max_decoder_seq_length=processData(val_file_path,set(input_chars),set(target_chars))\n\nprint(\"Number of validation samples:\", len(validation_input))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"validation Max sequence length for inputs:\", validation_max_encoder_seq_length)\nprint(\"validation Max sequence length for outputs:\", validation_max_decoder_seq_length)","metadata":{"id":"lmCnQKWNbsuh","outputId":"52ce3c82-4a02-48a0-9f02-b000ff849fa4","execution":{"iopub.status.busy":"2022-06-23T14:17:02.393284Z","iopub.execute_input":"2022-06-23T14:17:02.394147Z","iopub.status.idle":"2022-06-23T14:17:02.411838Z","shell.execute_reply.started":"2022-06-23T14:17:02.394103Z","shell.execute_reply":"2022-06-23T14:17:02.410911Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Number of validation samples: 4502\nNumber of unique input tokens: 28\nNumber of unique output tokens: 65\nvalidation Max sequence length for inputs: 18\nvalidation Max sequence length for outputs: 17\n","output_type":"stream"}]},{"cell_type":"code","source":"# Vectorize the data.\ntest_input,test_target,test_input_chars,test_target_chars,test_num_encoder_tokens,test_num_decoder_tokens, test_max_encoder_seq_length, test_max_decoder_seq_length=processData(test_file_path)\nprint(\"Number of validation samples:\", len(test_input))\nprint(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\nprint(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)","metadata":{"id":"Q9tUFlovbz5i","outputId":"7f953ea3-61d2-47d6-9186-d54f2e298138","execution":{"iopub.status.busy":"2022-06-23T14:17:02.752603Z","iopub.execute_input":"2022-06-23T14:17:02.753468Z","iopub.status.idle":"2022-06-23T14:17:02.771088Z","shell.execute_reply.started":"2022-06-23T14:17:02.753430Z","shell.execute_reply":"2022-06-23T14:17:02.769650Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Number of validation samples: 4358\nTest Max sequence length for inputs: 20\nTest Max sequence length for outputs: 16\n","output_type":"stream"}]},{"cell_type":"code","source":"# input_token = dict([(char, i) for i, char in enumerate(input_chars)])\n# target_token = dict([(char, i) for i, char in enumerate(target_chars)])\n\n# reverse_input_token = dict((i, char) for char, i in input_token.items())\n# reverse_target_token = dict((i, char) for char, i in target_token.items())\n\n\n# encoder_input_data = np.zeros(\n#     (len(input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n# )\n# validation_encoder_input_data=np.zeros(\n#     (len(validation_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n# )\n# test_encoder_input_data=np.zeros(\n#     (len(test_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n# )\n# decoder_input_data = np.zeros(\n#     (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )\n# validation_decoder_input_data =np.zeros(\n#     (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )\n# decoder_target_data = np.zeros(\n#     (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )\n# validation_decoder_target_data = np.zeros(\n#     (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n# )\n\n# for i, (input_text, target_text) in enumerate(zip(input, target)):\n#     for t, char in enumerate(input_text):\n#         encoder_input_data[i, t, input_token[char]] = 1.0\n#     for t, char in enumerate(target_text):\n#         # decoder_target_data is ahead of decoder_input_data by one timestep\n#         decoder_input_data[i, t, target_token[char]] = 1.0\n#         if t > 0:\n#             # decoder_target_data will be ahead by one timestep\n#             # and will not include the start character.\n#             decoder_target_data[i, t - 1, target_token[char]] = 1.0\n# # for validation data\n# for i, (validation_input_text, validation_target_text) in enumerate(zip(validation_input, validation_target)):\n#     for t, char in enumerate(validation_input_text):\n#         validation_encoder_input_data[i, t, input_token[char]] = 1.0\n#     for t, char in enumerate(validation_target_text):\n#         # decoder_target_data is ahead of decoder_input_data by one timestep\n#         validation_decoder_input_data[i, t, target_token[char]] = 1.0\n#         if t > 0:\n#             # decoder_target_data will be ahead by one timestep\n#             # and will not include the start character.\n#             validation_decoder_target_data[i, t - 1, target_token[char]] = 1.0\n\n# # for test data\n# for i, (test_input_text, test_target_text) in enumerate(zip(test_input, test_target)):\n#     for t, char in enumerate(test_input_text):\n#         test_encoder_input_data[i, t, input_token[char]] = 1.0","metadata":{"id":"oBF7Cdqrb5Hc","execution":{"iopub.status.busy":"2022-06-23T14:17:03.038030Z","iopub.execute_input":"2022-06-23T14:17:03.040130Z","iopub.status.idle":"2022-06-23T14:17:03.047702Z","shell.execute_reply.started":"2022-06-23T14:17:03.040097Z","shell.execute_reply":"2022-06-23T14:17:03.046428Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"2q7AdDj4P_8p","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_token = dict([(char, i) for i, char in enumerate(input_chars)])\ntarget_token = dict([(char, i) for i, char in enumerate(target_chars)])\n\nreverse_input_token = dict((i, char) for char, i in input_token.items())\nreverse_target_token = dict((i, char) for char, i in target_token.items())\n\nenc_input_data = np.zeros(\n    (len(input), max_encoder_seq_length), dtype=\"float32\"\n)\ndec_input_data = np.zeros(\n    (len(input), max_decoder_seq_length), dtype=\"float32\"\n)\ndec_target_data = np.ones(\n    (len(input), max_decoder_seq_length), dtype=\"float32\"\n)\n#Decoder Target Sequences are Padded to a maximum length of max_decoder SeqLen characters with a vocabulary of sizeofTeluguVocab different characters. \nfor i, (input_text, target_text) in enumerate(zip(input, target)):\n    for t, char in enumerate(input_text):\n        enc_input_data[i, t] = input_token[char]\n    #enc_input_data[i, t + 1 :] = input_token[\" \"]\n\n    for t, char in enumerate(target_text):\n        # dec_target_data is ahead of dec_input_data by one timestep\n        dec_input_data[i, t] = target_token[char]\n        if t > 0:\n            # dec_target_data will not include the start character.\n            dec_target_data[i, t - 1] = target_token[char]\n    #dec_input_data[i, t + 1: ] = target_token[\" \"]\n    #dec_target_data[i, t:, target_token[\" \"]] = 1.0\n    \nval_enc_input_data = np.zeros(\n    (len(validation_input), validation_max_encoder_seq_length), dtype=\"float32\"\n)\nval_dec_input_data = np.zeros(\n    (len(validation_input), validation_max_decoder_seq_length), dtype=\"float32\"\n)\nval_dec_target_data = np.ones(\n    (len(validation_input), validation_max_decoder_seq_length), dtype=\"float32\"\n)\n\nfor i, (input_text, target_text) in enumerate(zip(validation_input,validation_target)):\n    for t, char in enumerate(input_text):\n        # Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object. \n        # This enumerated object can then be used directly for loops or converted into a list of tuples using the list() method.\n        val_enc_input_data[i, t] = input_token[char]\n    #val_enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n\n    for t, char in enumerate(target_text):\n        val_dec_input_data[i, t] = target_token[char]\n        if t > 0:\n            # dec_target_data will be ahead by one timestep\n            # and will not include the start character.\n            val_dec_target_data[i, t - 1] =  target_token[char]\n    #val_dec_input_data[i, t + 1: ] = target_token_index[\" \"]","metadata":{"id":"6_L_o5BGrP3z","execution":{"iopub.status.busy":"2022-06-23T14:17:04.855349Z","iopub.execute_input":"2022-06-23T14:17:04.855783Z","iopub.status.idle":"2022-06-23T14:17:05.451703Z","shell.execute_reply.started":"2022-06-23T14:17:04.855748Z","shell.execute_reply":"2022-06-23T14:17:05.450705Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"xtmF5PX1Xo9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class NMTDataset:\n#     def __init__(self, problem_type='en-hi'):\n#         self.problem_type = 'en-'\n#         self.inp_lang_tokenizer = None\n#         self.targ_lang_tokenizer = None\n    \n\n#     def unicode_to_ascii(self, s):\n#         return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\n#     ## Step 1 and Step 2 \n#     def preprocess_sentence(self, w):\n#         # w = self.unicode_to_ascii(w.lower().strip())\n\n#         # # creating a space between a word and the punctuation following it\n#         # # eg: \"he is a boy.\" => \"he is a boy .\"\n#         # # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n#         # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n#         # w = re.sub(r'[\" \"]+', \" \", w)\n\n#         # # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n#         # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n\n#         # w = w.strip()\n\n#         # adding a start and an end token to the sentence\n#         # so that the model know when to start and stop predicting.\n#         #print(w)\n#         w = '\\t' + w + '\\n'\n        \n#         return w\n    \n#     def create_dataset(self, path, num_examples):\n#         # path : path to spa-eng.txt file\n#         # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n#         #lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n#         #word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n#         data =  pd.read_csv(path,delimiter=\"\\t\", header= None, nrows = num_examples )\n#         data = data.dropna()\n#         print(data.info())\n#         return data[0].apply(self.preprocess_sentence).values.astype(str), data[1].apply(self.preprocess_sentence).values.astype(str)\n\n#     # Step 3 and Step 4\n#     def tokenize(self, lang):\n#         # lang = list of sentences in a language\n        \n#         # print(len(lang), \"example sentence: {}\".format(lang[0]))\n#         lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True)\n#         lang_tokenizer.fit_on_texts(lang)\n\n#         ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n#         ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n#         tensor = lang_tokenizer.texts_to_sequences(lang) \n\n#         ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n#         ## and pads the sequences to match the longest sequences in the given input\n#         tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n#         return tensor, lang_tokenizer\n\n#     def load_dataset(self, path, num_examples=None):\n#         # creating cleaned input, output pairs\n#         targ_lang, inp_lang = self.create_dataset(path, num_examples)\n\n#         input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n#         target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n\n#         return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n\n#     def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n#         #file_path = download_dakshina()\n#         input_tensor_train, target_tensor_train, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(train_file_path, num_examples)\n#         input_tensor_val, target_tensor_val, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(val_file_path, num_examples)\n#         input_tensor_test, target_tensor_test, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(test_file_path, num_examples)\n#         x = input_tensor_train\n#         y  =target_tensor_train\n#         #input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.4)\n\n#         train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train))\n#         train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n#         val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n#         val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n#         test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, target_tensor_test))\n#         test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n#         return train_dataset, val_dataset, test_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer\n","metadata":{"id":"EOIwk0-PrP3z","execution":{"iopub.status.busy":"2022-06-23T14:17:06.266817Z","iopub.execute_input":"2022-06-23T14:17:06.267574Z","iopub.status.idle":"2022-06-23T14:17:06.276151Z","shell.execute_reply.started":"2022-06-23T14:17:06.267529Z","shell.execute_reply":"2022-06-23T14:17:06.274484Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# BUFFER_SIZE = 32000\n# BATCH_SIZE = 64\n# # Let's limit the #training examples for faster training\n# num_examples = 300000\n\n# dataset_creator = NMTDataset('en-spa')\n# train_dataset, val_dataset,test_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)","metadata":{"id":"s3Wn4CM1zgLH","outputId":"977b8f65-a1c2-4b15-e040-29a04fd3a88f","execution":{"iopub.status.busy":"2022-06-23T14:17:07.520082Z","iopub.execute_input":"2022-06-23T14:17:07.520974Z","iopub.status.idle":"2022-06-23T14:17:07.526127Z","shell.execute_reply.started":"2022-06-23T14:17:07.520924Z","shell.execute_reply":"2022-06-23T14:17:07.524820Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# next(iter(train_dataset))[1][0]","metadata":{"id":"9Rox45Q1hWgm","outputId":"05609fc2-54dc-4c20-e204-01bda96fe41e","execution":{"iopub.status.busy":"2022-06-23T14:17:08.068058Z","iopub.execute_input":"2022-06-23T14:17:08.068752Z","iopub.status.idle":"2022-06-23T14:17:08.073636Z","shell.execute_reply.started":"2022-06-23T14:17:08.068712Z","shell.execute_reply":"2022-06-23T14:17:08.072125Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# example_input_batch, example_target_batch = next(iter(train_dataset))\n# example_input_batch.shape, example_target_batch.shape","metadata":{"id":"9gaZrS4g1T5X","outputId":"97ad97b4-743b-44fc-81d2-ccd5ff884d00","execution":{"iopub.status.busy":"2022-06-23T14:17:09.004143Z","iopub.execute_input":"2022-06-23T14:17:09.004847Z","iopub.status.idle":"2022-06-23T14:17:09.009481Z","shell.execute_reply.started":"2022-06-23T14:17:09.004807Z","shell.execute_reply":"2022-06-23T14:17:09.008313Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Build the model\n","metadata":{"id":"eGM50wwWAQed"}},{"cell_type":"code","source":"class Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, num_of_layers, enc_unit_type, batch_sz, recurrent_dropout, dropout):\n    super(Encoder, self).__init__()\n\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.enc_unit_type = enc_unit_type\n    self.num_of_layers = num_of_layers\n    self.recurrent_dropout = recurrent_dropout\n    self.dropout = dropout\n    self.embedding = Embedding( vocab_size, embedding_dim)\n\n    self.encoder_layer = self.get_encoder_layer(self.enc_units,\n                                                self.num_of_layers, self.enc_unit_type)\n    \n\n  def get_encoder_layer(self, enc_units, num_of_layers, enc_unit_type):\n    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(enc_unit_type, \n                                                                                 enc_units) for i in range(num_of_layers)],),\n                                  return_sequences=True, return_state=True, name = \"Encoder\")\n\n  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n      #print(cell_type)\n      if cell_type == \"lstm\":\n        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n      elif cell_type == \"rnn\":\n        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      elif cell_type ==\"gru\":\n        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      else:\n        print(f\"Invalid cell type: {cell_type}\")\n\n    \n  def call(self, x, hidden):\n      x = self.embedding(x)\n      output = self.encoder_layer(x,initial_state = hidden)\n\n      #print(output)\n      return output\n    \n  def initialize_hidden_state(self):\n      print(\"Called\")\n        \n      if self.enc_unit_type == 'rnn' or self.enc_unit_type == \"gru\":\n        return [tf.zeros((self.batch_sz, self.enc_units))]*self.num_of_layers\n      else:\n        return [[tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]]*self.num_of_layers","metadata":{"id":"-_IMrNwXsYR4","execution":{"iopub.status.busy":"2022-06-23T18:16:09.337641Z","iopub.execute_input":"2022-06-23T18:16:09.338022Z","iopub.status.idle":"2022-06-23T18:16:09.353395Z","shell.execute_reply.started":"2022-06-23T18:16:09.337989Z","shell.execute_reply":"2022-06-23T18:16:09.352311Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"# enc_input_data[0].shape","metadata":{"id":"3iT2EuNf1TDo","outputId":"7a08f4c6-f141-4984-9b5f-544a2c3aceec","execution":{"iopub.status.busy":"2022-06-23T18:16:11.201047Z","iopub.execute_input":"2022-06-23T18:16:11.201425Z","iopub.status.idle":"2022-06-23T18:16:11.205695Z","shell.execute_reply.started":"2022-06-23T18:16:11.201395Z","shell.execute_reply":"2022-06-23T18:16:11.204669Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"\n#encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\nencoder = Encoder( num_encoder_tokens, 2, 16, 2, \"lstm\", 64, 0.0, 0.0)\nsample_hidden = encoder.initialize_hidden_state()\n# encoder.build(input_shape =(None,26))\n# encoder.summary()\nsample_output = encoder(enc_input_data[:64], sample_hidden)\nout , state = sample_output[0], sample_output[1:]","metadata":{"id":"oG6E1qaLv92s","execution":{"iopub.status.busy":"2022-06-23T18:19:14.175320Z","iopub.execute_input":"2022-06-23T18:19:14.175885Z","iopub.status.idle":"2022-06-23T18:19:14.249524Z","shell.execute_reply.started":"2022-06-23T18:19:14.175844Z","shell.execute_reply":"2022-06-23T18:19:14.248493Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"Called\n","output_type":"stream"}]},{"cell_type":"code","source":"# out, state","metadata":{"id":"yHmvx-1zRHJ8","execution":{"iopub.status.busy":"2022-06-23T18:08:13.726846Z","iopub.execute_input":"2022-06-23T18:08:13.727549Z","iopub.status.idle":"2022-06-23T18:08:13.732065Z","shell.execute_reply.started":"2022-06-23T18:08:13.727491Z","shell.execute_reply":"2022-06-23T18:08:13.730800Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, num_of_layers, \n               dec_unit_type, batch_sz, recurrent_dropout, dropout, \n               attention_type = None):\n    \n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.dec_unit_type = dec_unit_type\n    self.num_of_layers = num_of_layers\n    self.attention_type = attention_type\n    self.recurrent_dropout = recurrent_dropout\n    self.dropout = dropout\n    #print(\"decoder embedding dim\", embedding_dim)\n    self.embedding = Embedding( vocab_size, embedding_dim)\n\n    self.fc  = tf.keras.layers.Dense(vocab_size, activation = \"softmax\")\n\n    self.decoder_cells = self.get_stacked_rnn_cell()\n    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n    if attention_type:        \n        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, None\n                                                                  , self.batch_sz*[max_encoder_seq_length], \n                                                                  self.attention_type)\n\n        self.cell = self.build_cell()\n\n        #print(self.cell)\n\n        self.decoder = tfa.seq2seq.BasicDecoder(self.cell, sampler = self.sampler, output_layer = self.fc)\n\n    else:\n        self.decoder = tfa.seq2seq.BasicDecoder(self.decoder_cells, self.sampler, self.fc)\n\n  def build_cell(self):\n    cell = tfa.seq2seq.AttentionWrapper(self.decoder_cells, self.attention_mechanism,\n                                        attention_layer_size = self.dec_units)\n    return cell\n  \n  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n    # ------------- #\n    # typ: Which sort of attention (Bahdanau, Luong)\n    # dec_units: final dimension of attention outputs \n    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n\n    if(attention_type=='bahdanau'):\n      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n    else:\n      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n\n  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n      #print(cell_type)\n      if cell_type == \"lstm\":\n        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n      elif cell_type == \"rnn\":\n        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      elif cell_type ==\"gru\":\n        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      else:\n        print(f\"Invalid cell type: {cell_type}\")\n\n  def get_stacked_rnn_cell(self,):\n    return tf.keras.layers.StackedRNNCells( [self.get_cell(self.dec_unit_type, self.dec_units,) for i in range(self.num_of_layers)])\n\n  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n    decoder_initial_state = self.cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n    #print(decoder_initial_state, len(decoder_initial_state))\n    #print(batch_sz)\n    #print(len(encoder_state))\n    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n    return decoder_initial_state\n\n  def call(self, x, initial_state):\n    x = self.embedding(x)\n    #print(\"calles\")\n    output = self.decoder(x, initial_state=initial_state)\n    return output","metadata":{"id":"vPiwKS7P7A3X","execution":{"iopub.status.busy":"2022-06-23T14:17:15.076943Z","iopub.execute_input":"2022-06-23T14:17:15.077332Z","iopub.status.idle":"2022-06-23T14:17:15.096018Z","shell.execute_reply.started":"2022-06-23T14:17:15.077299Z","shell.execute_reply":"2022-06-23T14:17:15.095073Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\n# #encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\n# decoder = Decoder( num_decoder_tokens,  1, 16, 3, \"lstm\", 64, 0.0, 0.0)\n# #sample_hidden = encoder.initialize_hidden_state()\n# #decoder.build(input_shape =(None, ))\n# # decoder.summary()\n# #sample_x = tf.random.uniform((2  ,max_decoder_seq_length))\n# decoder.attention_mechanism.setup_memory(out)\n# initial_state = decoder.build_initial_state(64, tuple(state), tf.float32)\n# # sample_output = decoder(dec_input_data[:8192], initial_state)\n# # out1 , state1 = sample_output[0], sample_output[1:]","metadata":{"id":"Fe8whQdYCdfZ","outputId":"62bd1aa0-6726-41c8-a941-25bf2702c4fa","execution":{"iopub.status.busy":"2022-06-23T14:17:16.305656Z","iopub.execute_input":"2022-06-23T14:17:16.306237Z","iopub.status.idle":"2022-06-23T14:17:16.311119Z","shell.execute_reply.started":"2022-06-23T14:17:16.306199Z","shell.execute_reply":"2022-06-23T14:17:16.309803Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# out1","metadata":{"id":"s1cQ1JBzWhth","execution":{"iopub.status.busy":"2022-06-23T14:17:17.423518Z","iopub.execute_input":"2022-06-23T14:17:17.423875Z","iopub.status.idle":"2022-06-23T14:17:17.428118Z","shell.execute_reply.started":"2022-06-23T14:17:17.423845Z","shell.execute_reply":"2022-06-23T14:17:17.427087Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"input_data = tf.data.Dataset.from_tensor_slices((enc_input_data, dec_input_data))\ntarget_data =  tf.data.Dataset.from_tensor_slices(dec_target_data)\ntrain_dataset  = tf.data.Dataset.zip((input_data, target_data)).batch(batch_size, drop_remainder=True)\n\ninput_data = tf.data.Dataset.from_tensor_slices((val_enc_input_data, val_dec_input_data))\ntarget_data =  tf.data.Dataset.from_tensor_slices(val_dec_target_data)\nval_dataset  = tf.data.Dataset.zip((input_data, target_data)).batch(batch_size, drop_remainder=True)","metadata":{"id":"Y-R5t0ex2M6v","execution":{"iopub.status.busy":"2022-06-23T14:17:18.377412Z","iopub.execute_input":"2022-06-23T14:17:18.378626Z","iopub.status.idle":"2022-06-23T14:17:18.407482Z","shell.execute_reply.started":"2022-06-23T14:17:18.378583Z","shell.execute_reply":"2022-06-23T14:17:18.406602Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq():\n  def __init__(self, num_encoder_tokens, num_decoder_token, encoder_embedding_dim, decoder_embedding_dim,num_of_unit, num_of_layers, unit_type, batch_size, recurrent_dropout, dropout, attention_type = None):\n    super().__init__()\n    self.batch_size = batch_size\n    self.attention_type = attention_type\n    self.encoder = Encoder(  num_encoder_tokens, encoder_embedding_dim, num_of_unit, num_of_layers, unit_type, self.batch_size,  recurrent_dropout, dropout)\n    #self.encoder.summary()\n    self.dec = Decoder( num_decoder_tokens,  decoder_embedding_dim, num_of_unit, num_of_layers, unit_type, self.batch_size, recurrent_dropout, dropout, attention_type)\n    #sample_x = tf.random.uniform((batch_size  ,max_decoder_seq_length))\n\n  def call(self, enc_inp, dec_inp):\n    #print(\"fsdfa\",dec_inp.shape)\n    x = self.encoder(enc_inp)\n    enc_out, enc_state = x[0], x[1:]\n    #print(enc_out.shape)\n    if self.attention_type:\n        self.dec.attention_mechanism.setup_memory(enc_out)\n        enc_state = self.dec.build_initial_state(self.batch_size, tuple(enc_state), tf.float32)\n    #print(\"fucck\")\n    x = self.dec(dec_inp,enc_state)\n    return x\n\n  @tf.function\n  def validation_step(self, val_enc_input_data, val_dec_input_data, targ):\n    #dec_input_data = val_dec_input_data[ : , :-1 ]\n    out = self.call(val_enc_input_data, val_dec_input_data)\n    logits = out[0].rnn_output\n    #print(logits.item())\n    loss = 0\n    for (i, (ta, pre)) in enumerate(zip(tf.unstack(targ),tf.unstack(logits))):\n        stop = tf.where( ta == 1)[0][0]\n        self.metric.update_state(ta[:stop], pre[:stop])\n        loss += self.loss_function(ta[:stop], pre[:stop])\n    #loss += self.loss_function(real, logits)\n    #print(\"Validation Loss = \", loss.numpy())\n    #self.metric.update_state(real, logits)\n    return loss/i, self.metric.result()\n\n  @tf.function\n  def train_step(self, enc_input_data, dec_input_data, targ):\n    loss = 0\n\n    with tf.GradientTape() as tape:\n      \n      out = self.call(enc_input_data, dec_input_data)\n      logits = out[0].rnn_output\n      loss = 0\n      for (i, (ta, pre)) in enumerate(zip(tf.unstack(targ),tf.unstack(logits))):\n        stop = tf.where( ta == 1)[0][0]\n        self.metric.update_state(ta[:stop], pre[:stop])\n        loss += self.loss_function(ta[:stop], pre[:stop])\n \n      \n    variables = self.encoder.variables + self.dec.variables\n    gradients = tape.gradient(loss, variables)\n    self.optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss/i, self.metric.result()\n\n  def fit(self, train_dataset, val_dataset, epochs, loss, optimizer, checkpoint, metric):\n    self.metric = metric\n\n    \n    self.loss_function = loss\n    self.optimizer = optimizer\n    steps_per_epoch = len(input)//batch_size\n    step_per_val_epoch  = len(validation_input)//batch_size\n    print(steps_per_epoch)\n    for epoch in range(epochs):\n      start = time.time()\n\n      #enc_hidden = encoder.initialize_hidden_state()\n      total_loss = 0\n      total_acc = 0\n      # print(enc_hidden[0].shape, enc_hidden[1].shape)\n\n      self.metric.reset_states()\n      for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n        #print(\"targ\", targ)\n        batch_loss , acc = self.train_step(inp[0],inp[1] ,targ )\n        total_loss += batch_loss\n        total_acc += acc\n        if batch % 100 == 0:\n          #break\n          print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                      batch,\n                                                      batch_loss.numpy()))\n      # saving (checkpoint) the model every 2 epochs\n      #val_enc_inp, val_dec_inp , val_targ = val_dataset.take(-1)\n      #val_enc_inp, val_dec_inp = val_inp.take(-1)\n          #andb.log({\"Epoch {epoch + 1} Batch {batch}\": batch_loss.numpy()})\n      total_val_loss = 0\n      total_val_acc = 0\n\n      self.metric.reset_states()\n      for (batch, (inp, targ)) in enumerate(val_dataset.take(steps_per_epoch)):\n        #print(batch)\n       \n        val_batch_loss, val_acc = self.validation_step(inp[0],inp[1] ,targ)\n        total_val_loss +=val_batch_loss\n        total_val_acc += val_acc\n\n      print(f\"Validatiion loss:  {total_val_loss.numpy()/  step_per_val_epoch}\")\n      print((f\"Validatiion Acc:  {(total_val_acc.numpy()/  step_per_val_epoch)*100}\"))\n      # print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n      #                                                 batch,\n      #                                                 val_batch_loss.numpy()))\n      \n        \n      if val_acc>=95:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n        break\n      if (epoch + 1) % 2 == 0:\n        checkpoint.save(file_prefix = checkpoint_prefix)\n\n      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                          total_loss / steps_per_epoch))\n      print(\"Accuracy \",(total_acc.numpy()/steps_per_epoch) *100)\n      print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n         \n#       wandb.log({\"Epoch\": epoch + 1,\n#                 \"Train loss\": total_loss / steps_per_epoch,\n#                  \"Train Accuracy\": (total_acc.numpy()/steps_per_epoch) *100,\n#                  \"Val Accuracy\": (total_val_acc.numpy()/  step_per_val_epoch)*100,\n#                  \"Val Loss\": total_val_loss.numpy()/  step_per_val_epoch\n#                 })\n\n        \n            \n\n    ","metadata":{"id":"cVoQMjKYYLX2","execution":{"iopub.status.busy":"2022-06-23T14:17:19.608208Z","iopub.execute_input":"2022-06-23T14:17:19.608586Z","iopub.status.idle":"2022-06-23T14:17:19.632692Z","shell.execute_reply.started":"2022-06-23T14:17:19.608555Z","shell.execute_reply":"2022-06-23T14:17:19.631509Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ns2s = Seq2Seq(num_encoder_tokens, num_decoder_tokens,  encoder_embedding_dim =1024,\n              decoder_embedding_dim= 128,\n              num_of_unit =64,\n              num_of_layers = 2, \n              unit_type =\"gru\",\n             batch_size = batch_size, \n              recurrent_dropout = 0.3,\n              dropout = 0.4 , \n              attention_type = None)\n\nsample_out = s2s.call(enc_input_data[:batch_size], dec_input_data[:batch_size])","metadata":{"id":"BRKTJEGid7MN","execution":{"iopub.status.busy":"2022-06-23T14:17:22.655914Z","iopub.execute_input":"2022-06-23T14:17:22.656513Z","iopub.status.idle":"2022-06-23T14:17:24.490713Z","shell.execute_reply.started":"2022-06-23T14:17:22.656474Z","shell.execute_reply":"2022-06-23T14:17:24.489737Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"#s2s.call(enc_input_data[:batch_size], dec_input_data[:batch_size])","metadata":{"id":"f_J78RufnWC0","execution":{"iopub.status.busy":"2022-06-23T14:17:26.039695Z","iopub.execute_input":"2022-06-23T14:17:26.040737Z","iopub.status.idle":"2022-06-23T14:17:26.045957Z","shell.execute_reply.started":"2022-06-23T14:17:26.040699Z","shell.execute_reply":"2022-06-23T14:17:26.044316Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def create_batch(appende, batch_size = 64):\n  temp  = []\n  for i in range(batch_size):\n    temp.append(appende)\n  return np.array(temp)\ndef idx_to_word(word):\n  return  \"\".join([reverse_target_token[char] for char in word])\ndef word_to_index(word):\n  return  [reverse_input_token[char] for char in word]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T12:55:27.856423Z","iopub.execute_input":"2022-06-23T12:55:27.856821Z","iopub.status.idle":"2022-06-23T12:55:27.864297Z","shell.execute_reply.started":"2022-06-23T12:55:27.856790Z","shell.execute_reply":"2022-06-23T12:55:27.863155Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"class BeamSearch():\n\n  def __init__(self, beam_size):\n    self.beam_size = beam_size\n\n  def beam_search_decoder(aelf, data, k):\n    sequences = [[list(), 0.0]]\n    # walk over each step in sequence\n    for row in data:\n      all_candidates = list()\n      # expand each current candidate\n      for i in range(len(sequences)):\n        seq, score = sequences[i]\n        for j in range(len(row)):\n          candidate = [seq + [j], score - log(row[j])]\n          all_candidates.append(candidate)\n      # order all candidates by score\n      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n      # select k best\n      sequences = ordered[:k]\n    return sequences\n  \n  def on_epoch_end(self):\n    acc = 0\n    for (i, (inp, targ)) in enumerate(val_dataset.take(batch_size)):\n        prediction = s2s.call(inp[0], inp[1])[0].rnn_output\n    #prediction = self.model.predict([val_enc_input_data , val_dec_input_data])\n        print(prediction.shape)\n        for i, pred in enumerate(prediction):\n          beam_search_prediction = self.beam_search_decoder(pred, self.beam_size)\n          correct_prediction = 0\n          for k in range(self.beam_size):\n            translated_word = \"\\t\"+\"\".join([reverse_target_token[x] for x in beam_search_prediction[k][0][:len(validation_target[i])-1]])\n            print(translated_word, validation_target[i])\n            #print(validation_target[i])\n\n            def idx2char(idx_list):\n              return \"\".join([reverse_target_token[x] for x in idx_list])\n\n            if \"\\t\"+ idx2char(beam_search_prediction[k][0][:len(validation_target[i])-1]) == validation_target[i]:\n              correct_prediction+=1\n              break\n        mul = 10.0**4\n        acc += ((correct_prediction/prediction.shape[0])*mul)/mul \n        \n    logs[\"character_accuracy\"] = ((correct_prediction/prediction.shape[0])*mul)/mul\n    print(\"- character_accuracy\",logs[\"character_accuracy\"])\n    #print(f\"Accuracy by Beam Search {correct_prediction/len(validation_target)}\")\n      # print(len(beam_search_prediction))\n      # print(beam_search_prediction)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:09:43.108789Z","iopub.execute_input":"2022-06-23T13:09:43.109248Z","iopub.status.idle":"2022-06-23T13:09:43.126564Z","shell.execute_reply.started":"2022-06-23T13:09:43.109213Z","shell.execute_reply":"2022-06-23T13:09:43.125624Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.RMSprop()\n\n\ndef loss_function(real, pred):\n  # real shape = (BATCH_SIZE, max_length_output)\n  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n  #print(pred,\"fucck\", real)\n  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n  loss = cross_entropy(y_true=real, y_pred=pred)\n  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n  mask = tf.cast(mask, dtype=loss.dtype)  \n  loss = mask* loss\n  loss = tf.reduce_mean(loss)\n  return loss  ","metadata":{"id":"CD30sG2dRTLD","execution":{"iopub.status.busy":"2022-06-23T14:17:30.781872Z","iopub.execute_input":"2022-06-23T14:17:30.782221Z","iopub.status.idle":"2022-06-23T14:17:30.789585Z","shell.execute_reply.started":"2022-06-23T14:17:30.782192Z","shell.execute_reply":"2022-06-23T14:17:30.788531Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=s2s.encoder,\n                                 decoder=s2s.dec,\n                                 )","metadata":{"id":"25sIIGGxRVC2","execution":{"iopub.status.busy":"2022-06-23T14:17:32.633456Z","iopub.execute_input":"2022-06-23T14:17:32.634118Z","iopub.status.idle":"2022-06-23T14:17:32.639957Z","shell.execute_reply.started":"2022-06-23T14:17:32.634082Z","shell.execute_reply":"2022-06-23T14:17:32.639026Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"tf.config.run_functions_eagerly(False)\ns2s.fit(train_dataset, val_dataset, 30, loss_function, optimizer, checkpoint , metric =  tf.keras.metrics.SparseCategoricalAccuracy())\n#beam_search = BeamSearch(3)\n#beam_search.on_epoch_end()","metadata":{"id":"fHckm-8X2qG2","outputId":"9c76069e-108a-4b88-835a-5ac1222b9bc0","scrolled":true,"execution":{"iopub.status.busy":"2022-06-23T16:57:14.993350Z","iopub.execute_input":"2022-06-23T16:57:14.994025Z","iopub.status.idle":"2022-06-23T17:45:06.381366Z","shell.execute_reply.started":"2022-06-23T16:57:14.993989Z","shell.execute_reply":"2022-06-23T17:45:06.380297Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"690\nEpoch 1 Batch 0 Loss 3.5763\nEpoch 1 Batch 100 Loss 3.4973\nEpoch 1 Batch 200 Loss 3.4369\nEpoch 1 Batch 300 Loss 3.5817\nEpoch 1 Batch 400 Loss 3.7672\nEpoch 1 Batch 500 Loss 3.6298\nEpoch 1 Batch 600 Loss 3.7385\nValidatiion loss:  3.685123988560268\nValidatiion Acc:  70.04863739013672\nEpoch 1 Loss 3.5447\nAccuracy  68.08103699615036\nTime taken for 1 epoch 97.48323965072632 sec\n\nEpoch 2 Batch 0 Loss 3.6035\nEpoch 2 Batch 100 Loss 3.4749\nEpoch 2 Batch 200 Loss 3.4495\nEpoch 2 Batch 300 Loss 3.5760\nEpoch 2 Batch 400 Loss 3.7596\nEpoch 2 Batch 500 Loss 3.6399\nEpoch 2 Batch 600 Loss 3.7253\nValidatiion loss:  3.680878121512277\nValidatiion Acc:  70.39741516113281\nEpoch 2 Loss 3.5444\nAccuracy  70.13961570850317\nTime taken for 1 epoch 95.45968866348267 sec\n\nEpoch 3 Batch 0 Loss 3.6002\nEpoch 3 Batch 100 Loss 3.4918\nEpoch 3 Batch 200 Loss 3.4336\nEpoch 3 Batch 300 Loss 3.5812\nEpoch 3 Batch 400 Loss 3.7651\nEpoch 3 Batch 500 Loss 3.6222\nEpoch 3 Batch 600 Loss 3.7305\nValidatiion loss:  3.673712158203125\nValidatiion Acc:  70.571779523577\nEpoch 3 Loss 3.5432\nAccuracy  70.43487106544384\nTime taken for 1 epoch 95.77675032615662 sec\n\nEpoch 4 Batch 0 Loss 3.5113\nEpoch 4 Batch 100 Loss 3.5037\nEpoch 4 Batch 200 Loss 3.4333\nEpoch 4 Batch 300 Loss 3.5784\nEpoch 4 Batch 400 Loss 3.7619\nEpoch 4 Batch 500 Loss 3.6191\nEpoch 4 Batch 600 Loss 3.7301\nValidatiion loss:  3.6797764369419643\nValidatiion Acc:  70.68340846470424\nEpoch 4 Loss 3.5421\nAccuracy  70.60280620187953\nTime taken for 1 epoch 95.21680307388306 sec\n\nEpoch 5 Batch 0 Loss 3.6011\nEpoch 5 Batch 100 Loss 3.4795\nEpoch 5 Batch 200 Loss 3.4241\nEpoch 5 Batch 300 Loss 3.5752\nEpoch 5 Batch 400 Loss 3.7658\nEpoch 5 Batch 500 Loss 3.6244\nEpoch 5 Batch 600 Loss 3.7232\nValidatiion loss:  3.6785975864955356\nValidatiion Acc:  70.74407849993024\nEpoch 5 Loss 3.5425\nAccuracy  70.68650839985281\nTime taken for 1 epoch 96.57148885726929 sec\n\nEpoch 6 Batch 0 Loss 3.5766\nEpoch 6 Batch 100 Loss 3.4747\nEpoch 6 Batch 200 Loss 3.4370\nEpoch 6 Batch 300 Loss 3.5805\nEpoch 6 Batch 400 Loss 3.7645\nEpoch 6 Batch 500 Loss 3.6282\nEpoch 6 Batch 600 Loss 3.7326\nValidatiion loss:  3.6717982700892855\nValidatiion Acc:  70.80140795026507\nEpoch 6 Loss 3.5415\nAccuracy  70.74818044468977\nTime taken for 1 epoch 95.48325753211975 sec\n\nEpoch 7 Batch 0 Loss 3.6396\nEpoch 7 Batch 100 Loss 3.4659\nEpoch 7 Batch 200 Loss 3.4365\nEpoch 7 Batch 300 Loss 3.5850\nEpoch 7 Batch 400 Loss 3.7643\nEpoch 7 Batch 500 Loss 3.6261\nEpoch 7 Batch 600 Loss 3.7208\nValidatiion loss:  3.673998587472098\nValidatiion Acc:  70.84750039236886\nEpoch 7 Loss 3.5415\nAccuracy  70.80478391785553\nTime taken for 1 epoch 96.0996949672699 sec\n\nEpoch 8 Batch 0 Loss 3.6239\nEpoch 8 Batch 100 Loss 3.4944\nEpoch 8 Batch 200 Loss 3.4256\nEpoch 8 Batch 300 Loss 3.5933\nEpoch 8 Batch 400 Loss 3.7660\nEpoch 8 Batch 500 Loss 3.6339\nEpoch 8 Batch 600 Loss 3.7068\nValidatiion loss:  3.678741019112723\nValidatiion Acc:  70.88820321219308\nEpoch 8 Loss 3.5407\nAccuracy  70.85450545601223\nTime taken for 1 epoch 94.9937174320221 sec\n\nEpoch 9 Batch 0 Loss 3.6209\nEpoch 9 Batch 100 Loss 3.4596\nEpoch 9 Batch 200 Loss 3.4518\nEpoch 9 Batch 300 Loss 3.5864\nEpoch 9 Batch 400 Loss 3.7641\nEpoch 9 Batch 500 Loss 3.6273\nEpoch 9 Batch 600 Loss 3.7133\nValidatiion loss:  3.6905517578125\nValidatiion Acc:  70.91632298060826\nEpoch 9 Loss 3.5405\nAccuracy  70.89263031448144\nTime taken for 1 epoch 95.6899163722992 sec\n\nEpoch 10 Batch 0 Loss 3.6472\nEpoch 10 Batch 100 Loss 3.4764\nEpoch 10 Batch 200 Loss 3.4352\nEpoch 10 Batch 300 Loss 3.5777\nEpoch 10 Batch 400 Loss 3.7626\nEpoch 10 Batch 500 Loss 3.6192\nEpoch 10 Batch 600 Loss 3.7227\nValidatiion loss:  3.679398454938616\nValidatiion Acc:  70.94112396240234\nEpoch 10 Loss 3.5399\nAccuracy  70.91198907382247\nTime taken for 1 epoch 94.676522731781 sec\n\nEpoch 11 Batch 0 Loss 3.6066\nEpoch 11 Batch 100 Loss 3.4765\nEpoch 11 Batch 200 Loss 3.4340\nEpoch 11 Batch 300 Loss 3.5706\nEpoch 11 Batch 400 Loss 3.7698\nEpoch 11 Batch 500 Loss 3.6189\nEpoch 11 Batch 600 Loss 3.7290\nValidatiion loss:  3.683835710797991\nValidatiion Acc:  70.97473689488002\nEpoch 11 Loss 3.5389\nAccuracy  70.94800866168478\nTime taken for 1 epoch 95.6745388507843 sec\n\nEpoch 12 Batch 0 Loss 3.5698\nEpoch 12 Batch 100 Loss 3.4926\nEpoch 12 Batch 200 Loss 3.4254\nEpoch 12 Batch 300 Loss 3.5838\nEpoch 12 Batch 400 Loss 3.7670\nEpoch 12 Batch 500 Loss 3.6269\nEpoch 12 Batch 600 Loss 3.7162\nValidatiion loss:  3.675771222795759\nValidatiion Acc:  71.00943429129464\nEpoch 12 Loss 3.5382\nAccuracy  70.98263063292572\nTime taken for 1 epoch 96.2171540260315 sec\n\nEpoch 13 Batch 0 Loss 3.5370\nEpoch 13 Batch 100 Loss 3.4671\nEpoch 13 Batch 200 Loss 3.4211\nEpoch 13 Batch 300 Loss 3.5759\nEpoch 13 Batch 400 Loss 3.7706\nEpoch 13 Batch 500 Loss 3.6166\nEpoch 13 Batch 600 Loss 3.7045\nValidatiion loss:  3.681839861188616\nValidatiion Acc:  71.04588099888393\nEpoch 13 Loss 3.5374\nAccuracy  71.02054319519927\nTime taken for 1 epoch 97.39634156227112 sec\n\nEpoch 14 Batch 0 Loss 3.4638\nEpoch 14 Batch 100 Loss 3.4575\nEpoch 14 Batch 200 Loss 3.4302\nEpoch 14 Batch 300 Loss 3.5719\nEpoch 14 Batch 400 Loss 3.7626\nEpoch 14 Batch 500 Loss 3.6180\nEpoch 14 Batch 600 Loss 3.7154\nValidatiion loss:  3.6771902901785714\nValidatiion Acc:  71.0759026663644\nEpoch 14 Loss 3.5373\nAccuracy  71.05160035948822\nTime taken for 1 epoch 97.99719738960266 sec\n\nEpoch 15 Batch 0 Loss 3.4863\nEpoch 15 Batch 100 Loss 3.5306\nEpoch 15 Batch 200 Loss 3.4246\nEpoch 15 Batch 300 Loss 3.5717\nEpoch 15 Batch 400 Loss 3.7581\nEpoch 15 Batch 500 Loss 3.6190\nEpoch 15 Batch 600 Loss 3.7061\nValidatiion loss:  3.681192452566964\nValidatiion Acc:  71.08870370047433\nEpoch 15 Loss 3.5400\nAccuracy  71.06771717900816\nTime taken for 1 epoch 96.37709522247314 sec\n\nEpoch 16 Batch 0 Loss 3.4754\nEpoch 16 Batch 100 Loss 3.4952\nEpoch 16 Batch 200 Loss 3.4257\nEpoch 16 Batch 300 Loss 3.5754\nEpoch 16 Batch 400 Loss 3.7776\nEpoch 16 Batch 500 Loss 3.6266\nEpoch 16 Batch 600 Loss 3.7154\nValidatiion loss:  3.6837881905691963\nValidatiion Acc:  71.11782618931362\nEpoch 16 Loss 3.5358\nAccuracy  71.09677079795064\nTime taken for 1 epoch 96.48132586479187 sec\n\nEpoch 17 Batch 0 Loss 3.4652\nEpoch 17 Batch 100 Loss 3.4765\nEpoch 17 Batch 200 Loss 3.4657\nEpoch 17 Batch 300 Loss 3.5702\nEpoch 17 Batch 400 Loss 3.7646\nEpoch 17 Batch 500 Loss 3.6211\nEpoch 17 Batch 600 Loss 3.7277\nValidatiion loss:  3.6702540806361608\nValidatiion Acc:  71.14669799804688\nEpoch 17 Loss 3.5359\nAccuracy  71.12526713937953\nTime taken for 1 epoch 95.58844590187073 sec\n\nEpoch 18 Batch 0 Loss 3.4546\nEpoch 18 Batch 100 Loss 3.4636\nEpoch 18 Batch 200 Loss 3.4317\nEpoch 18 Batch 300 Loss 3.5853\nEpoch 18 Batch 400 Loss 3.7614\nEpoch 18 Batch 500 Loss 3.6315\nEpoch 18 Batch 600 Loss 3.7174\nValidatiion loss:  3.68535897391183\nValidatiion Acc:  71.17701939174107\nEpoch 18 Loss 3.5349\nAccuracy  71.15897800611413\nTime taken for 1 epoch 95.64829158782959 sec\n\nEpoch 19 Batch 0 Loss 3.5402\nEpoch 19 Batch 100 Loss 3.4901\nEpoch 19 Batch 200 Loss 3.4306\nEpoch 19 Batch 300 Loss 3.5791\nEpoch 19 Batch 400 Loss 3.7688\nEpoch 19 Batch 500 Loss 3.6178\nEpoch 19 Batch 600 Loss 3.7015\nValidatiion loss:  3.6753919328962055\nValidatiion Acc:  71.20368412562779\nEpoch 19 Loss 3.5353\nAccuracy  71.18279056272645\nTime taken for 1 epoch 94.41810250282288 sec\n\nEpoch 20 Batch 0 Loss 3.4865\nEpoch 20 Batch 100 Loss 3.4973\nEpoch 20 Batch 200 Loss 3.4193\nEpoch 20 Batch 300 Loss 3.5781\nEpoch 20 Batch 400 Loss 3.7591\nEpoch 20 Batch 500 Loss 3.6165\nEpoch 20 Batch 600 Loss 3.7072\nValidatiion loss:  3.675970458984375\nValidatiion Acc:  71.23194558279855\nEpoch 20 Loss 3.5344\nAccuracy  71.21240588201992\nTime taken for 1 epoch 95.8449239730835 sec\n\nEpoch 21 Batch 0 Loss 3.4703\nEpoch 21 Batch 100 Loss 3.4693\nEpoch 21 Batch 200 Loss 3.4287\nEpoch 21 Batch 300 Loss 3.5818\nEpoch 21 Batch 400 Loss 3.7601\nEpoch 21 Batch 500 Loss 3.6217\nEpoch 21 Batch 600 Loss 3.7076\nValidatiion loss:  3.6718593052455355\nValidatiion Acc:  71.25962938581193\nEpoch 21 Loss 3.5341\nAccuracy  71.2402786033741\nTime taken for 1 epoch 94.50703763961792 sec\n\nEpoch 22 Batch 0 Loss 3.4528\nEpoch 22 Batch 100 Loss 3.5120\nEpoch 22 Batch 200 Loss 3.4332\nEpoch 22 Batch 300 Loss 3.5753\nEpoch 22 Batch 400 Loss 3.7661\nEpoch 22 Batch 500 Loss 3.6125\nEpoch 22 Batch 600 Loss 3.7090\nValidatiion loss:  3.6657980782645088\nValidatiion Acc:  71.28908429827008\nEpoch 22 Loss 3.5333\nAccuracy  71.26828843268795\nTime taken for 1 epoch 95.65891456604004 sec\n\nEpoch 23 Batch 0 Loss 3.4929\nEpoch 23 Batch 100 Loss 3.4746\nEpoch 23 Batch 200 Loss 3.4198\nEpoch 23 Batch 300 Loss 3.5839\nEpoch 23 Batch 400 Loss 3.7628\nEpoch 23 Batch 500 Loss 3.6203\nEpoch 23 Batch 600 Loss 3.7060\nValidatiion loss:  3.660209437779018\nValidatiion Acc:  71.31873539515904\nEpoch 23 Loss 3.5332\nAccuracy  71.29886350769927\nTime taken for 1 epoch 94.90589022636414 sec\n\nEpoch 24 Batch 0 Loss 3.5028\nEpoch 24 Batch 100 Loss 3.4913\nEpoch 24 Batch 200 Loss 3.4381\nEpoch 24 Batch 300 Loss 3.5878\nEpoch 24 Batch 400 Loss 3.7649\nEpoch 24 Batch 500 Loss 3.6199\nEpoch 24 Batch 600 Loss 3.7106\nValidatiion loss:  3.671570260184152\nValidatiion Acc:  71.35091509137835\nEpoch 24 Loss 3.5324\nAccuracy  71.32987644361413\nTime taken for 1 epoch 95.32777810096741 sec\n\nEpoch 25 Batch 0 Loss 3.4403\nEpoch 25 Batch 100 Loss 3.4664\nEpoch 25 Batch 200 Loss 3.4242\nEpoch 25 Batch 300 Loss 3.5759\nEpoch 25 Batch 400 Loss 3.7697\nEpoch 25 Batch 500 Loss 3.6228\nEpoch 25 Batch 600 Loss 3.6928\nValidatiion loss:  3.664365495954241\nValidatiion Acc:  71.37062617710659\nEpoch 25 Loss 3.5348\nAccuracy  71.35644442793252\nTime taken for 1 epoch 95.27790999412537 sec\n\nEpoch 26 Batch 0 Loss 3.4563\nEpoch 26 Batch 100 Loss 3.4526\nEpoch 26 Batch 200 Loss 3.4239\nEpoch 26 Batch 300 Loss 3.5818\nEpoch 26 Batch 400 Loss 3.7799\nEpoch 26 Batch 500 Loss 3.6308\nEpoch 26 Batch 600 Loss 3.6967\nValidatiion loss:  3.673008946010045\nValidatiion Acc:  71.37474060058594\nEpoch 26 Loss 3.5401\nAccuracy  71.37144226958786\nTime taken for 1 epoch 96.07237148284912 sec\n\nEpoch 27 Batch 0 Loss 3.4348\nEpoch 27 Batch 100 Loss 3.4586\nEpoch 27 Batch 200 Loss 3.4183\nEpoch 27 Batch 300 Loss 3.5779\nEpoch 27 Batch 400 Loss 3.7621\nEpoch 27 Batch 500 Loss 3.6180\nEpoch 27 Batch 600 Loss 3.7076\nValidatiion loss:  3.674753679547991\nValidatiion Acc:  71.39757973807198\nEpoch 27 Loss 3.5325\nAccuracy  71.38250820878623\nTime taken for 1 epoch 95.38557291030884 sec\n\nEpoch 28 Batch 0 Loss 3.4931\nEpoch 28 Batch 100 Loss 3.4554\nEpoch 28 Batch 200 Loss 3.4182\nEpoch 28 Batch 300 Loss 3.5881\nEpoch 28 Batch 400 Loss 3.7674\nEpoch 28 Batch 500 Loss 3.6194\nEpoch 28 Batch 600 Loss 3.7066\nValidatiion loss:  3.671427263532366\nValidatiion Acc:  71.4239501953125\nEpoch 28 Loss 3.5307\nAccuracy  71.40739551488903\nTime taken for 1 epoch 95.08099174499512 sec\n\nEpoch 29 Batch 0 Loss 3.4592\nEpoch 29 Batch 100 Loss 3.4585\nEpoch 29 Batch 200 Loss 3.4257\nEpoch 29 Batch 300 Loss 3.5875\nEpoch 29 Batch 400 Loss 3.7688\nEpoch 29 Batch 500 Loss 3.6177\nEpoch 29 Batch 600 Loss 3.7068\nValidatiion loss:  3.6643689836774556\nValidatiion Acc:  71.4540263584682\nEpoch 29 Loss 3.5295\nAccuracy  71.43583435943161\nTime taken for 1 epoch 95.99103951454163 sec\n\nEpoch 30 Batch 0 Loss 3.4315\nEpoch 30 Batch 100 Loss 3.4506\nEpoch 30 Batch 200 Loss 3.4236\nEpoch 30 Batch 300 Loss 3.5784\nEpoch 30 Batch 400 Loss 3.7659\nEpoch 30 Batch 500 Loss 3.6287\nEpoch 30 Batch 600 Loss 3.7037\nValidatiion loss:  3.666281999860491\nValidatiion Acc:  71.48065839494978\nEpoch 30 Loss 3.5300\nAccuracy  71.46485701851223\nTime taken for 1 epoch 94.07491493225098 sec\n\n","output_type":"stream"}]},{"cell_type":"code","source":"s2s.dec.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:38:39.954290Z","iopub.execute_input":"2022-06-23T13:38:39.954947Z","iopub.status.idle":"2022-06-23T13:38:39.961865Z","shell.execute_reply.started":"2022-06-23T13:38:39.954915Z","shell.execute_reply":"2022-06-23T13:38:39.960603Z"},"trusted":true},"execution_count":151,"outputs":[{"name":"stdout","text":"Model: \"decoder_29\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_59 (Embedding)     multiple                  66560     \n_________________________________________________________________\ndense_29 (Dense)             multiple                  1105      \n_________________________________________________________________\nstacked_rnn_cells_59 (Stacke multiple                  66624     \n_________________________________________________________________\nbasic_decoder_27 (BasicDecod multiple                  67729     \n=================================================================\nTotal params: 134,289\nTrainable params: 134,289\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"sweep_config = {\n    \n    'method':'bayes',\n    'metric': {\n        'name':'Val Accuracy',\n        'goal':'maximize'\n    },\n    'parameters':{\n    \n    \"num_of_layer\" : {'values': [1,2,3]},\n    \"unit_size\": {\"values\":[16,32,64]},\n    \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n    \"dropout\": {\"values\": [0.0, 0.2, 0.4]},\n    'recurrent_dropout':{'values':[0.0,0.3]},\n    \"epochs\":{\"value\":10},\n    \"encoder_embedding_dim\":{\"values\": [64, 128, 1024]},\n    \"decoder_embedding_dim\":{\"values\": [64, 128, 1024]},\n    \"optimizer\":{\"values\": [\"adam\",\"rmsprop\"]}             \n                   }\n}\npprint.pprint(sweep_config)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:39:05.967105Z","iopub.execute_input":"2022-06-23T13:39:05.967502Z","iopub.status.idle":"2022-06-23T13:39:05.977473Z","shell.execute_reply.started":"2022-06-23T13:39:05.967471Z","shell.execute_reply":"2022-06-23T13:39:05.976540Z"},"trusted":true},"execution_count":152,"outputs":[{"name":"stdout","text":"{'method': 'bayes',\n 'metric': {'goal': 'maximize', 'name': 'Val Accuracy'},\n 'parameters': {'decoder_embedding_dim': {'values': [64, 128, 1024]},\n                'dropout': {'values': [0.0, 0.2, 0.4]},\n                'encoder_embedding_dim': {'values': [64, 128, 1024]},\n                'epochs': {'value': 10},\n                'num_of_layer': {'values': [1, 2, 3]},\n                'optimizer': {'values': ['adam', 'rmsprop']},\n                'recurrent_dropout': {'values': [0.0, 0.3]},\n                'unit_size': {'values': [16, 32, 64]},\n                'unit_type': {'values': ['lstm', 'rnn', 'gru']}}}\n","output_type":"stream"}]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"Sweep_without_Attention\")\n","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:39:18.168254Z","iopub.execute_input":"2022-06-23T13:39:18.168666Z","iopub.status.idle":"2022-06-23T13:39:19.138083Z","shell.execute_reply.started":"2022-06-23T13:39:18.168636Z","shell.execute_reply":"2022-06-23T13:39:19.137026Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Create sweep with ID: gk7yunpp\nSweep URL: https://wandb.ai/aslan/Sweep_without_Attention/sweeps/gk7yunpp\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:39:19.859928Z","iopub.execute_input":"2022-06-23T13:39:19.860889Z","iopub.status.idle":"2022-06-23T13:39:19.866133Z","shell.execute_reply.started":"2022-06-23T13:39:19.860833Z","shell.execute_reply":"2022-06-23T13:39:19.865235Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"\ndef train(config = None):\n  with wandb.init(config=config):\n    config = wandb.config\n    #print(config)\n    s2s = Seq2Seq(num_encoder_tokens,num_decoder_tokens,config.encoder_embedding_dim, config.decoder_embedding_dim, config.unit_size, config.num_of_layer,config.unit_type , batch_size, config.dropout,config.recurrent_dropout, attention_type = None)\n    if config.optimizer == \"adm\":\n        optimizer = tf.keras.optimizers.Adam()\n    else:\n        optimizer = tf.keras.optimizers.RMSprop()\n    checkpoint_dir = './training_checkpoints'\n    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=s2s.encoder,\n                                 decoder=s2s.dec,\n                                 )\n\n    # seq2seq.compile(optimizer=config.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\",])\n    s2s.fit(\n        train_dataset,\n        val_dataset,\n        config.epochs,\n        loss_function,\n        optimizer,\n        checkpoint,\n        metric = tf.keras.metrics.SparseCategoricalAccuracy()\n        )\nwandb.agent(sweep_id, train)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T13:39:23.437761Z","iopub.execute_input":"2022-06-23T13:39:23.438208Z","iopub.status.idle":"2022-06-23T13:42:13.703689Z","shell.execute_reply.started":"2022-06-23T13:39:23.438160Z","shell.execute_reply":"2022-06-23T13:42:13.702243Z"},"trusted":true},"execution_count":155,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6gxy0b9z with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_embedding_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.4\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_layer: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tunit_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tunit_type: lstm\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.19"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220623_133925-6gxy0b9z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/aslan/Sweep_without_Attention/runs/6gxy0b9z\" target=\"_blank\">summer-sweep-1</a></strong> to <a href=\"https://wandb.ai/aslan/Sweep_without_Attention\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/aslan/Sweep_without_Attention/sweeps/gk7yunpp\" target=\"_blank\">https://wandb.ai/aslan/Sweep_without_Attention/sweeps/gk7yunpp</a>"},"metadata":{}},{"name":"stdout","text":"690\nEpoch 1 Batch 0 Loss 4.2406\nEpoch 1 Batch 100 Loss 4.2131\nEpoch 1 Batch 200 Loss 4.1920\nEpoch 1 Batch 300 Loss 4.1047\nEpoch 1 Batch 400 Loss 4.1948\nEpoch 1 Batch 500 Loss 4.0740\nEpoch 1 Batch 600 Loss 4.1345\nValidatiion loss:  4.123540823800223\nValidatiion Acc:  13.466736929757253\nEpoch 1 Loss 4.1287\nAccuracy  14.915398860323256\nTime taken for 1 epoch 92.08108830451965 sec\n\nEpoch 2 Batch 0 Loss 4.1785\nEpoch 2 Batch 100 Loss 4.2118\nEpoch 2 Batch 200 Loss 4.1484\nEpoch 2 Batch 300 Loss 4.0263\nEpoch 2 Batch 400 Loss 4.1537\nEpoch 2 Batch 500 Loss 3.9949\nEpoch 2 Batch 600 Loss 4.1288\nValidatiion loss:  4.083060128348214\nValidatiion Acc:  16.441647665841238\nEpoch 2 Loss 4.0667\nAccuracy  18.529208639393683\nTime taken for 1 epoch 68.99501252174377 sec\n\nEpoch 3 Batch 0 Loss 4.1539\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Train Accuracy</td><td>▁█</td></tr><tr><td>Train loss</td><td>█▁</td></tr><tr><td>Val Accuracy</td><td>▁█</td></tr><tr><td>Val Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2</td></tr><tr><td>Train Accuracy</td><td>18.52921</td></tr><tr><td>Train loss</td><td>4.06675</td></tr><tr><td>Val Accuracy</td><td>16.44165</td></tr><tr><td>Val Loss</td><td>4.08306</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">summer-sweep-1</strong>: <a href=\"https://wandb.ai/aslan/Sweep_without_Attention/runs/6gxy0b9z\" target=\"_blank\">https://wandb.ai/aslan/Sweep_without_Attention/runs/6gxy0b9z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20220623_133925-6gxy0b9z/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# !cat ./wandb/run-20220622_082243-tv6u3ywu/logs/debug.log","metadata":{"execution":{"iopub.status.busy":"2022-06-22T08:40:53.524413Z","iopub.execute_input":"2022-06-22T08:40:53.525141Z","iopub.status.idle":"2022-06-22T08:40:54.230409Z","shell.execute_reply.started":"2022-06-22T08:40:53.525094Z","shell.execute_reply":"2022-06-22T08:40:54.229187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualisation","metadata":{}},{"cell_type":"code","source":"# def idx_to_word(word, translate_dict):\n#   return  \"\".join([translate_dict[char] for char in word])\n\n\n# idx_to_word(next(iter(train_dataset))[1][0].numpy(), reverse_target_token)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:13:03.197348Z","iopub.execute_input":"2022-06-22T15:13:03.197726Z","iopub.status.idle":"2022-06-22T15:13:03.219604Z","shell.execute_reply.started":"2022-06-22T15:13:03.197695Z","shell.execute_reply":"2022-06-22T15:13:03.218442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_out = s2s.call(val_enc_input_data[:batch_size], dec_input_data[:batch_size])","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:05:35.714418Z","iopub.execute_input":"2022-06-22T15:05:35.714834Z","iopub.status.idle":"2022-06-22T15:05:35.900777Z","shell.execute_reply.started":"2022-06-22T15:05:35.714778Z","shell.execute_reply":"2022-06-22T15:05:35.899881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# next(iter(train_dataset))[1][0].numpy()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:12:39.86428Z","iopub.execute_input":"2022-06-22T15:12:39.864671Z","iopub.status.idle":"2022-06-22T15:12:39.887317Z","shell.execute_reply.started":"2022-06-22T15:12:39.864642Z","shell.execute_reply":"2022-06-22T15:12:39.886166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# argmax(sample_out[0].rnn_output[], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:15:16.070342Z","iopub.execute_input":"2022-06-22T15:15:16.070774Z","iopub.status.idle":"2022-06-22T15:15:16.080627Z","shell.execute_reply.started":"2022-06-22T15:15:16.070737Z","shell.execute_reply":"2022-06-22T15:15:16.079502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reverse_target_token","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:15:41.730527Z","iopub.execute_input":"2022-06-22T15:15:41.730948Z","iopub.status.idle":"2022-06-22T15:15:41.740387Z","shell.execute_reply.started":"2022-06-22T15:15:41.730913Z","shell.execute_reply":"2022-06-22T15:15:41.739138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idx_to_word(argmax(sample_out[0].rnn_output[0], axis = 1), reverse_target_token)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T15:08:36.085839Z","iopub.execute_input":"2022-06-22T15:08:36.086393Z","iopub.status.idle":"2022-06-22T15:08:36.095769Z","shell.execute_reply.started":"2022-06-22T15:08:36.086345Z","shell.execute_reply":"2022-06-22T15:08:36.094676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_enc_input_data[0]","metadata":{"id":"U-wA0D4dX2t4","outputId":"722a2932-202c-4ab3-867e-66cf6832e304","execution":{"iopub.status.busy":"2022-06-22T08:40:54.231991Z","iopub.execute_input":"2022-06-22T08:40:54.232765Z","iopub.status.idle":"2022-06-22T08:40:54.238258Z","shell.execute_reply.started":"2022-06-22T08:40:54.232718Z","shell.execute_reply":"2022-06-22T08:40:54.236892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_dec_input_data.shape","metadata":{"id":"SQOKJ5t2XxOr","outputId":"f0ba6934-b2c2-4f0b-8f50-ebb4d17d46a9","execution":{"iopub.status.busy":"2022-06-22T08:40:54.240229Z","iopub.execute_input":"2022-06-22T08:40:54.240604Z","iopub.status.idle":"2022-06-22T08:40:54.248328Z","shell.execute_reply.started":"2022-06-22T08:40:54.240569Z","shell.execute_reply":"2022-06-22T08:40:54.247316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_dec_target_data[110]","metadata":{"id":"0HDvOkuGWkWB","outputId":"dac5d6d6-afba-4c40-8124-560f0cfa7cdd","execution":{"iopub.status.busy":"2022-06-22T08:40:54.251633Z","iopub.execute_input":"2022-06-22T08:40:54.252469Z","iopub.status.idle":"2022-06-22T08:40:54.25944Z","shell.execute_reply.started":"2022-06-22T08:40:54.252439Z","shell.execute_reply":"2022-06-22T08:40:54.257999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_out = s2s.call(val_enc_input_data[:batch_size], val_dec_input_data[:batch_size])","metadata":{"id":"QImBhPbTV9OG","outputId":"caa04bfb-856f-40fd-b42f-828c44ce851e","execution":{"iopub.status.busy":"2022-06-22T08:40:54.26149Z","iopub.execute_input":"2022-06-22T08:40:54.262325Z","iopub.status.idle":"2022-06-22T08:40:54.269448Z","shell.execute_reply.started":"2022-06-22T08:40:54.262287Z","shell.execute_reply":"2022-06-22T08:40:54.268335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.dot(1,np.equal(val_dec_target_data[0][:3], np.argmax(sample_out[0].rnn_output[0][:3], axis =1)))","metadata":{"id":"-e_OlFyHbqkM","outputId":"1351f30a-f6d4-4687-8253-5313265f860a","execution":{"iopub.status.busy":"2022-06-22T08:40:54.270615Z","iopub.execute_input":"2022-06-22T08:40:54.27228Z","iopub.status.idle":"2022-06-22T08:40:54.27955Z","shell.execute_reply.started":"2022-06-22T08:40:54.272243Z","shell.execute_reply":"2022-06-22T08:40:54.278601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_out[0].rnn_output[0].shape","metadata":{"id":"VS5o8Whhmf6n","outputId":"54830daa-2431-41e3-8a40-627459ad5fb9","execution":{"iopub.status.busy":"2022-06-22T08:40:54.281174Z","iopub.execute_input":"2022-06-22T08:40:54.281934Z","iopub.status.idle":"2022-06-22T08:40:54.291187Z","shell.execute_reply.started":"2022-06-22T08:40:54.281897Z","shell.execute_reply":"2022-06-22T08:40:54.290105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_dec_target_data[0][:3]","metadata":{"id":"MxvkjOlObMrK","outputId":"b54a4b1c-cefc-4924-8082-30f5d7cecd75","execution":{"iopub.status.busy":"2022-06-22T08:40:54.29303Z","iopub.execute_input":"2022-06-22T08:40:54.293853Z","iopub.status.idle":"2022-06-22T08:40:54.301355Z","shell.execute_reply.started":"2022-06-22T08:40:54.293815Z","shell.execute_reply":"2022-06-22T08:40:54.300392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation_target[0]","metadata":{"id":"f8e2y3aAbo69","outputId":"54e8f2b6-1444-41c6-82ba-4b8e08e97819","execution":{"iopub.status.busy":"2022-06-22T08:40:54.303165Z","iopub.execute_input":"2022-06-22T08:40:54.303438Z","iopub.status.idle":"2022-06-22T08:40:54.311303Z","shell.execute_reply.started":"2022-06-22T08:40:54.303402Z","shell.execute_reply":"2022-06-22T08:40:54.310315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target_token","metadata":{"id":"FgavI_web2HG","execution":{"iopub.status.busy":"2022-06-22T08:40:54.312716Z","iopub.execute_input":"2022-06-22T08:40:54.313646Z","iopub.status.idle":"2022-06-22T08:40:54.322091Z","shell.execute_reply.started":"2022-06-22T08:40:54.313606Z","shell.execute_reply":"2022-06-22T08:40:54.321112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #m = tf.keras.metrics.SparseCategoricalAccuracy()\n# m.update_state(val_dec_target_data[0], sample_out[0].rnn_output[0][:3])\n# m.result().numpy()","metadata":{"id":"ZdmMny5tcgsk","outputId":"13531154-fde4-463f-d351-779da7e42345","execution":{"iopub.status.busy":"2022-06-22T08:40:54.323631Z","iopub.execute_input":"2022-06-22T08:40:54.324459Z","iopub.status.idle":"2022-06-22T08:40:54.33095Z","shell.execute_reply.started":"2022-06-22T08:40:54.324383Z","shell.execute_reply":"2022-06-22T08:40:54.330032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.where(val_dec_target_data[0] == 1)","metadata":{"id":"NlSvaPXRg6eq","outputId":"71fe8e85-c003-4937-b33e-e8b302f81563","execution":{"iopub.status.busy":"2022-06-22T08:40:54.332304Z","iopub.execute_input":"2022-06-22T08:40:54.332862Z","iopub.status.idle":"2022-06-22T08:40:54.340248Z","shell.execute_reply.started":"2022-06-22T08:40:54.332825Z","shell.execute_reply":"2022-06-22T08:40:54.339065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zip(val_dec_target_data, sample_out[0].rnn_output)","metadata":{"id":"V4t5mn2ff9fn","outputId":"7c43a97a-dbe0-48c2-bbd7-064332ff864a","execution":{"iopub.status.busy":"2022-06-22T08:40:54.341869Z","iopub.execute_input":"2022-06-22T08:40:54.342275Z","iopub.status.idle":"2022-06-22T08:40:54.350187Z","shell.execute_reply.started":"2022-06-22T08:40:54.342238Z","shell.execute_reply":"2022-06-22T08:40:54.34927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss_function(val_dec_input_data[:64], sample_out[0].rnn_output)","metadata":{"id":"GNf_vGipbGDC","outputId":"16d4ebbd-4c0c-4512-b94a-75d5ce403a9f","execution":{"iopub.status.busy":"2022-06-22T08:40:54.351586Z","iopub.execute_input":"2022-06-22T08:40:54.352637Z","iopub.status.idle":"2022-06-22T08:40:54.36082Z","shell.execute_reply.started":"2022-06-22T08:40:54.352599Z","shell.execute_reply":"2022-06-22T08:40:54.359829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# m = tf.keras.metrics.SparseCategoricalAccuracy()\n# step_per_val_epoch  = len(validation_input)//batch_size\n# m.update_state(val_dec_input_data[:64], sample_out[0].rnn_output)/step_per_val_epoch","metadata":{"id":"jnWZHZ3LcjuK","outputId":"98642f71-b68d-4383-ead7-3ed727c7df59","execution":{"iopub.status.busy":"2022-06-22T08:40:54.362369Z","iopub.execute_input":"2022-06-22T08:40:54.362889Z","iopub.status.idle":"2022-06-22T08:40:54.370603Z","shell.execute_reply.started":"2022-06-22T08:40:54.362853Z","shell.execute_reply":"2022-06-22T08:40:54.369671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  dec_input_data[0]","metadata":{"id":"whKn8yCHhJqx","execution":{"iopub.status.busy":"2022-06-22T08:40:54.371991Z","iopub.execute_input":"2022-06-22T08:40:54.372577Z","iopub.status.idle":"2022-06-22T08:40:54.383628Z","shell.execute_reply.started":"2022-06-22T08:40:54.372541Z","shell.execute_reply":"2022-06-22T08:40:54.38241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# t = list(val_dataset)[:2]\n# x, y , z = t[0][0], t[0][1], t[1]","metadata":{"id":"IjsQSX3ICgBx","execution":{"iopub.status.busy":"2022-06-22T08:40:54.386945Z","iopub.execute_input":"2022-06-22T08:40:54.388167Z","iopub.status.idle":"2022-06-22T08:40:54.394437Z","shell.execute_reply.started":"2022-06-22T08:40:54.388108Z","shell.execute_reply":"2022-06-22T08:40:54.393189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for val in t:\n#   enc_inp\n#   print(len(val[0]))","metadata":{"id":"Mh3Use5VC0lz","outputId":"a02d59e5-698d-49ca-f0d6-04ef44cd57fe","execution":{"iopub.status.busy":"2022-06-22T08:40:54.397056Z","iopub.execute_input":"2022-06-22T08:40:54.39754Z","iopub.status.idle":"2022-06-22T08:40:54.403271Z","shell.execute_reply.started":"2022-06-22T08:40:54.397476Z","shell.execute_reply":"2022-06-22T08:40:54.402205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n# loss = cross_entropy(y_true=dec_target_data[:64], y_pred=sample_out[0].rnn_output)\n# loss","metadata":{"id":"Xd7eP7r0gNsA","outputId":"bedc2e89-acdb-4272-8b64-67579ed11384","execution":{"iopub.status.busy":"2022-06-22T08:40:54.404666Z","iopub.execute_input":"2022-06-22T08:40:54.406668Z","iopub.status.idle":"2022-06-22T08:40:54.412572Z","shell.execute_reply.started":"2022-06-22T08:40:54.406631Z","shell.execute_reply":"2022-06-22T08:40:54.411703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for x, y in val_dataset:\n#   print(x.shape,y.shape)","metadata":{"id":"n8CBRHqXBFFh","outputId":"e3cf1a01-25a0-4f30-86d2-a12aac96a9ea","execution":{"iopub.status.busy":"2022-06-22T08:40:54.415765Z","iopub.execute_input":"2022-06-22T08:40:54.41658Z","iopub.status.idle":"2022-06-22T08:40:54.422092Z","shell.execute_reply.started":"2022-06-22T08:40:54.416546Z","shell.execute_reply":"2022-06-22T08:40:54.42111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x,y=val_dataset.take(1)","metadata":{"id":"N7kEoVlWRqvB","outputId":"62b45a85-7db9-40c9-a303-ce8f9d43d6df","execution":{"iopub.status.busy":"2022-06-22T08:40:54.423636Z","iopub.execute_input":"2022-06-22T08:40:54.424253Z","iopub.status.idle":"2022-06-22T08:40:54.431587Z","shell.execute_reply.started":"2022-06-22T08:40:54.424216Z","shell.execute_reply":"2022-06-22T08:40:54.430626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(val_dataset.range(1))\n","metadata":{"id":"0_7v0t5_TsLX","outputId":"e4b2e112-2e42-4550-f5a1-9e7769a97b97","execution":{"iopub.status.busy":"2022-06-22T08:40:54.434832Z","iopub.execute_input":"2022-06-22T08:40:54.435272Z","iopub.status.idle":"2022-06-22T08:40:54.44195Z","shell.execute_reply.started":"2022-06-22T08:40:54.435243Z","shell.execute_reply":"2022-06-22T08:40:54.440996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# s2s.predict([val_enc_input_data,val_dec_input_data])","metadata":{"id":"rspjiEnida96","outputId":"7bcbea59-7118-4ab0-a7f0-1ae9ff60d9b8","execution":{"iopub.status.busy":"2022-06-22T08:40:54.443111Z","iopub.execute_input":"2022-06-22T08:40:54.444292Z","iopub.status.idle":"2022-06-22T08:40:54.451733Z","shell.execute_reply.started":"2022-06-22T08:40:54.444255Z","shell.execute_reply":"2022-06-22T08:40:54.450799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# s2s.compile(\n#     optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n# )\n# input_data = tf.data.Dataset.from_tensor_slices((enc_input_data, dec_input_data))\n# target_data =  tf.data.Dataset.from_tensor_slices(dec_target_data)\n# train_dataset  = tf.data.Dataset.zip((input_data, target_data)).batch(batch_size)\n# #s2s.summary()\n# s2s.fit(\n#     train_dataset,\n#     batch_size=64,\n#     epochs=1,\n# )","metadata":{"id":"qK-34q8xNQi_","outputId":"38b5c0d0-2dde-4c04-f009-fd524fbe8e12","execution":{"iopub.status.busy":"2022-06-22T08:40:54.452756Z","iopub.execute_input":"2022-06-22T08:40:54.453013Z","iopub.status.idle":"2022-06-22T08:40:54.461777Z","shell.execute_reply.started":"2022-06-22T08:40:54.452988Z","shell.execute_reply":"2022-06-22T08:40:54.460656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# class Seq2seq(tf.keras.Model):\n#   def __init__(self, num_encoder_tokens, num_decoder_tokens,embedding_dim,num_of_layers,unit_type, dropout , recurrent_dropout):\n#     super().__init__()\n#     self.encoder_inputs = Input(shape = (None,), name = \"Input_layer_1\")\n#     self.decoder_inputs = keras.Input(shape=(None,), name = \"Input_layer_2\")\n#     self.num_encoder_tokens = num_encoder_tokens\n#     self.embedding_dim = embedding_dim\n#     self.dropout = dropout\n#     self.recurrent_dropout = recurrent_dropout\n#     self.num_decoder_tokens = num_decoder_tokens\n#     self.num_of_encoder_layer  =num_of_layers\n#     self.num_of_decoder_layer =num_of_layers\n#     self.type_encoder_unit =unit_type \n#     self.type_decoder_unit =unit_type\n#     self.train_step()\n#     self.build_model()\n\n#   def get_embedding_layer(self, num_encoder_tokens, embedding_dim,  name):\n#     return Embedding(num_encoder_tokens, embedding_dim, mask_zero = True, name =name )\n\n#   def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n#     #print(cell_type)\n#     if cell_type == \"lstm\":\n#       return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n#     elif cell_type == \"rnn\":\n#       return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n#     elif cell_type ==\"gru\":\n#       return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n#     else:\n#       print(f\"Invalid cell type: {cell_type}\")\n#   def get_encoder(self,latent_dim, cell_type = \"lstm\", num_of_layer = 1, name = None ):\n#     return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim) for i in range(num_of_layer)],), return_sequences=True, return_state=True, name = name)\n\n#   def get_decoder(self,latent_dim ,cell_type = \"lstm\", num_of_layer = 1, name = None ):\n#     return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim,) for i in range(num_of_layer)]), return_sequences=True, return_state=True)\n\n#   def get_dense_layer(self, num_decoder_token, activation = \"softmax\"):\n#     return Dense(num_decoder_tokens, activation= activation)\n\n#   def train_step(self):\n#     self.embedding_layer = self.get_embedding_layer( self.num_encoder_tokens, self.embedding_dim ,name = \"encoder_embedding\")\n#     self.embedding_results = self.embedding_layer(self.encoder_inputs)\n#     print(self.embedding_results.shape)\n#     self.encoder = self.get_encoder( self.embedding_dim,self.type_encoder_unit, self.num_of_encoder_layer , name =\"encoder\" )\n#     encoder_results = self.encoder(self.embedding_results)\n\n#     self.encoder_outputs, self.encoder_states = encoder_results[0], encoder_results[1:]\n\n#     self.embedding_layer2 = self.get_embedding_layer( self.num_decoder_tokens, self.embedding_dim, name = \"decoder_embedding\")\n#     self.embedding_results2 = self.embedding_layer2(self.decoder_inputs,)\n\n#     self.decoder = self.get_decoder( self.embedding_dim, self.type_decoder_unit, self.num_of_decoder_layer,)\n#     self.decoder_results = self.decoder(self.embedding_results2, initial_state=self.encoder_states)\n\n#     self.decoder_output = self.decoder_results[0]\n#     self.decoder_dense = self.get_dense_layer(self.num_decoder_tokens)\n#     self.dense_output = self.decoder_dense(self.decoder_output)\n\n#   def build_model(self):\n    \n#     self.model = keras.Model([self.encoder_inputs, self.decoder_inputs], self.dense_output, name = \"Seq2Seq_model\")\n#     return self.model\n\n","metadata":{"id":"x9fpzXhN7DMU","execution":{"iopub.status.busy":"2022-06-22T08:40:54.46466Z","iopub.execute_input":"2022-06-22T08:40:54.464946Z","iopub.status.idle":"2022-06-22T08:40:54.47427Z","shell.execute_reply.started":"2022-06-22T08:40:54.464922Z","shell.execute_reply":"2022-06-22T08:40:54.473317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, 1024,1,\"rnn\", 0.0, 0.0).build_model()\n# seq2seq.summary()","metadata":{"id":"qJ07iEXsJr6K","outputId":"f82e8d28-64d7-4a30-9437-d3dbaba684b5","execution":{"iopub.status.busy":"2022-06-22T08:40:54.475595Z","iopub.execute_input":"2022-06-22T08:40:54.476619Z","iopub.status.idle":"2022-06-22T08:40:54.487298Z","shell.execute_reply.started":"2022-06-22T08:40:54.476582Z","shell.execute_reply":"2022-06-22T08:40:54.486306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n","metadata":{"id":"HXZ2_xdsAQee"}},{"cell_type":"code","source":"# s2s.compile(\n#     optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n# )\n\n# s2s.fit(\n#     [enc_input_data, dec_input_data],\n#     dec_target_data,\n#     batch_size=64,\n#     epochs=1,\n# )","metadata":{"id":"Haec2CJvrP32","outputId":"3f520910-41d6-464c-bbb4-6ebef57a7b5d","execution":{"iopub.status.busy":"2022-06-22T08:40:54.488811Z","iopub.execute_input":"2022-06-22T08:40:54.489835Z","iopub.status.idle":"2022-06-22T08:40:54.497421Z","shell.execute_reply.started":"2022-06-22T08:40:54.489759Z","shell.execute_reply":"2022-06-22T08:40:54.496393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class BeamSearch(keras.callbacks.Callback):\n\n#   def __init__(self, beam_size):\n#     self.beam_size = beam_size\n\n#   def beam_search_decoder(aelf, data, k):\n#     sequences = [[list(), 0.0]]\n#     # walk over each step in sequence\n#     for row in data:\n#       all_candidates = list()\n#       # expand each current candidate\n#       for i in range(len(sequences)):\n#         seq, score = sequences[i]\n#         for j in range(len(row)):\n#           candidate = [seq + [j], score - log(row[j])]\n#           all_candidates.append(candidate)\n#       # order all candidates by score\n#       ordered = sorted(all_candidates, key=lambda tup:tup[1])\n#       # select k best\n#       sequences = ordered[:k]\n#     return sequences\n  \n#   def on_epoch_end(self, epoch, logs = None):\n#     prediction = self.model.predict([val_enc_input_data , val_dec_input_data])\n#     print(prediction.shape)\n#     for i, pred in enumerate(prediction):\n#       beam_search_prediction = self.beam_search_decoder(pred, self.beam_size)\n#       correct_prediction = 0\n#       for k in range(self.beam_size):\n#         #translated_word = \"\\t\"+\"\".join([reverse_target_token[x] for x in beam_search_prediction[k][0][:len(validation_target[i])-1]])\n#         #print(translated_word, validation_target[i])\n#         #print(validation_target[i])\n        \n#         def idx2char(idx_list):\n#           return \"\".join([reverse_target_token[x] for x in idx_list])\n\n#         if \"\\t\"+ idx2char(beam_search_prediction[k][0][:len(validation_target[i])-1]) == validation_target[i]:\n#           correct_prediction+=1\n#           break\n#     mul = 10.0**2\n#     logs[\"character_accuracy\"] = ((correct_prediction/prediction.shape[0])*mul)/mul\n#     print(\"- character_accuracy\",logs[\"character_accuracy\"])\n#     #print(f\"Accuracy by Beam Search {correct_prediction/len(validation_target)}\")\n#       # print(len(beam_search_prediction))\n#       # print(beam_search_prediction)\n","metadata":{"id":"KPyDujbe-Brq","execution":{"iopub.status.busy":"2022-06-22T08:40:54.498965Z","iopub.execute_input":"2022-06-22T08:40:54.499402Z","iopub.status.idle":"2022-06-22T08:40:54.508679Z","shell.execute_reply.started":"2022-06-22T08:40:54.499367Z","shell.execute_reply":"2022-06-22T08:40:54.507682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def beam_search_decoder(data, k):\n#     decodedWords = [[list(), 0.0]]\n#     # walk over each step in sequence\n#     for word in data:\n#       candidates = list()\n#       # expand each current candidate\n#       for sequence in decodedWords:\n#         seq, score = sequence\n#         for j in range(len(word)):\n#           candidate = [seq + [j], score - log(word[j])]\n#           candidates.append(candidate)\n#       # order all candidates by score\n#       ordered = sorted(candidates, key=lambda a:a[1])\n#       # select k best\n#       decodedWords = ordered[:k]\n#     return decodedWords\n  \n# def translate(seq):\n#   sentence = [] \n#   for x in seq:\n#     char = reverse_target_token[x]\n#     sentence.append(char)\n#   return \"\".join(sentence)\n# class WordAccuracyCallback(keras.callbacks.Callback):\n#   def __init__(self,beam_size):\n#     self.beam_size=beam_size\n#   def on_epoch_end(self, epoch, logs=None):\n#     pred=self.model.predict([val_enc_input_data , val_dec_input_data])\n#     count=0\n#     for i in range(pred.shape[0]):\n#       pSequences=beam_search_decoder(pred[i],self.beam_size)\n#       for j in range(self.beam_size):\n#         if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n#           count=count+1\n#           break\n#     factor = 10.0 ** 4\n#     logs[\"WordAccuracy\"]=math.trunc((count/pred.shape[0])*factor)/factor\n#     print(\"- wordAccuracy:\",logs[\"WordAccuracy\"])","metadata":{"id":"FE8vtOJOrP33","execution":{"iopub.status.busy":"2022-06-22T08:40:54.511515Z","iopub.execute_input":"2022-06-22T08:40:54.512413Z","iopub.status.idle":"2022-06-22T08:40:54.523285Z","shell.execute_reply.started":"2022-06-22T08:40:54.512372Z","shell.execute_reply":"2022-06-22T08:40:54.522209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_config = {\n    \n#     'method':'bayes',\n#     'metric': {\n#         'name':'val_accuracy',\n#         'goal':'maximize'\n#     },\n#     'parameters':{\n    \n#     \"num_of_layer\" : {'values': [1,2,3]},\n#     \"unit_size\": {\"values\":[16,32,64]},\n#     \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n#     \"dropout\": {\"values\": [0.0, 0.2, 0.4]},\n#     'recurrent_dropout':{'values':[0.0,0.3]},\n#     \"beam_size\" : {\"values\":[1,2,3,4]},\n#     \"epochs\":{\"value\":20},  \n#     \"optimizer\":{\"values\": [\"adam\",\"rmsprop\"]}             \n#                    }\n# }\n\n\n\n# pprint.pprint(sweep_config)","metadata":{"id":"mIW2Ofow5Deo","outputId":"0c99b3cb-eaac-4b1e-acfa-1aff5a6bad9e","execution":{"iopub.status.busy":"2022-06-22T08:40:54.524966Z","iopub.execute_input":"2022-06-22T08:40:54.525496Z","iopub.status.idle":"2022-06-22T08:40:54.536165Z","shell.execute_reply.started":"2022-06-22T08:40:54.525411Z","shell.execute_reply":"2022-06-22T08:40:54.535173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep_config, project=\"seq2seq\")","metadata":{"id":"x8YmtLZN_74p","execution":{"iopub.status.busy":"2022-06-22T08:40:54.537711Z","iopub.execute_input":"2022-06-22T08:40:54.53812Z","iopub.status.idle":"2022-06-22T08:40:54.545489Z","shell.execute_reply.started":"2022-06-22T08:40:54.538084Z","shell.execute_reply":"2022-06-22T08:40:54.544525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(config = None):\n#   with wandb.init(config=config):\n#     config = wandb.config\n#     #print(config)\n#     seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, config.unit_size, config.num_of_layer,config.unit_type , config.dropout,config.recurrent_dropout).build_model()\n#     seq2seq.compile(optimizer=config.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\",])\n#     seq2seq.fit(\n#         [encoder_input_data, decoder_input_data],\n#         decoder_target_data,\n#         batch_size=batch_size,\n#         epochs=config.epochs,\n#         validation_data =  ([validation_encoder_input_data , validation_decoder_input_data] ,validation_decoder_target_data),\n#         callbacks = [BeamSearch(config.beam_size), WandbCallback()],verbose = 1, \n#         )\n\n\n    \n    \n# wandb.agent(sweep_id, train)","metadata":{"id":"qYv2feSRAzW_","execution":{"iopub.status.busy":"2022-06-22T08:40:54.547019Z","iopub.execute_input":"2022-06-22T08:40:54.549119Z","iopub.status.idle":"2022-06-22T08:40:54.559107Z","shell.execute_reply.started":"2022-06-22T08:40:54.54908Z","shell.execute_reply":"2022-06-22T08:40:54.558099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seq2seq.compile(\n#     optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n# )\n# seq2seq.metrics_names\n\n","metadata":{"id":"DIrCXZTAGyTL","outputId":"0b540428-89cc-4ac6-ed17-8fc3bffafd6c","execution":{"iopub.status.busy":"2022-06-22T08:40:54.560462Z","iopub.execute_input":"2022-06-22T08:40:54.561904Z","iopub.status.idle":"2022-06-22T08:40:54.570362Z","shell.execute_reply.started":"2022-06-22T08:40:54.561875Z","shell.execute_reply":"2022-06-22T08:40:54.569403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pred=seq2seq.predict([val_enc_input_data , val_dec_input_data])\n# count=0\n# for i in range(pred.shape[0]//400):\n#       pSequences=beam_search_decoder(pred[i],3)\n#       for j in range(3):\n#         print({\"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])}, \"original =\", {validation_target[i]} )\n#         if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n#           count=count+1\n#           print(\"yes\")\n#           break\n# factor = 10.0 ** 4\n","metadata":{"scrolled":true,"id":"xLTgqB68rP35","outputId":"416e74fe-2665-4117-c579-56eb7c7beade","execution":{"iopub.status.busy":"2022-06-22T08:40:54.571737Z","iopub.execute_input":"2022-06-22T08:40:54.572805Z","iopub.status.idle":"2022-06-22T08:40:54.58519Z","shell.execute_reply.started":"2022-06-22T08:40:54.572766Z","shell.execute_reply":"2022-06-22T08:40:54.584271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = seq2seq.predict([val_enc_input_data , val_dec_input_data])\n# x.shape","metadata":{"id":"qnzLTsbxrP36","outputId":"357821dd-b5f9-4924-add4-0ebd058f94a7","execution":{"iopub.status.busy":"2022-06-22T08:40:54.586627Z","iopub.execute_input":"2022-06-22T08:40:54.587428Z","iopub.status.idle":"2022-06-22T08:40:54.595067Z","shell.execute_reply.started":"2022-06-22T08:40:54.587383Z","shell.execute_reply":"2022-06-22T08:40:54.594095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# histotry = seq2seq.fit(\n#     [enc_input_data, dec_input_data],\n#     dec_target_data,\n#     batch_size=8192,\n#     epochs=1,\n#     callbacks = [WordAccuracyCallback(3), ],\n# )\n# # Save model\n# seq2seq.save(\"s2s\")\n","metadata":{"id":"ox_fyYUrAQef","outputId":"f4da868c-355c-4132-8481-e4ffcb605de4","execution":{"iopub.status.busy":"2022-06-22T08:40:54.596529Z","iopub.execute_input":"2022-06-22T08:40:54.597146Z","iopub.status.idle":"2022-06-22T08:40:54.604963Z","shell.execute_reply.started":"2022-06-22T08:40:54.59711Z","shell.execute_reply":"2022-06-22T08:40:54.604015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for key in histotry.history.keys():\n#       print(key , histotry.history[key])\n#       #wandb.log({key : histotry.history[key]})","metadata":{"id":"2BnI7lHtQtnT","execution":{"iopub.status.busy":"2022-06-22T08:40:54.607382Z","iopub.execute_input":"2022-06-22T08:40:54.61045Z","iopub.status.idle":"2022-06-22T08:40:54.614888Z","shell.execute_reply.started":"2022-06-22T08:40:54.610409Z","shell.execute_reply":"2022-06-22T08:40:54.613941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seq2seq.metrics_names","metadata":{"id":"Npqd4if4HVZT","execution":{"iopub.status.busy":"2022-06-22T08:40:54.616603Z","iopub.execute_input":"2022-06-22T08:40:54.617516Z","iopub.status.idle":"2022-06-22T08:40:54.624584Z","shell.execute_reply.started":"2022-06-22T08:40:54.617387Z","shell.execute_reply":"2022-06-22T08:40:54.623466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run inference (sampling)\n\n1. encode input and retrieve initial decoder state\n2. run one step of decoder with this initial state\nand a \"start of sequence\" token as target.\nOutput will be the next target token.\n3. Repeat with the current target token and current states\n","metadata":{"id":"BC5CbwHlAQef"}},{"cell_type":"code","source":"# # Define sampling models\n# # Restore the model and construct the encoder and decoder.\n# model = keras.models.load_model(\"s2s\")\n\n# encoder_inputs = model.input[0]  # input_1\n# temp = model.layers[2].output\n# encoder_outputs, state = temp[0], temp[1:]  # lstm_1\n# encoder_states = state\n# encoder_model = keras.Model(encoder_inputs, encoder_states)\n\n# decoder_inputs = model.input[1]  # input_2\n# decoder_state_input_h = keras.Input(shape=(latent_dim,))\n# decoder_state_input_c = keras.Input(shape=(latent_dim,))\n# decoder_states_inputs = state\n# decoder_lstm = model.layers[3]\n# temp = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n# decoder_outputs, state_dec = temp[0], temp[1:]\n# decoder_states = state_dec\n# decoder_dense = model.layers[4]\n# decoder_outputs = decoder_dense(decoder_outputs)\n# decoder_model = keras.Model(\n#     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n# )\n\n# # Reverse-lookup token index to decode sequences back to\n# # something readable.\n# # reverse_input_char_index = dict((i, char) for char, i in num_encoder_tokens.items())\n# # reverse_target_char_index = dict((i, char) for char, i in num_decoder_tokens.items())\n# # print(reverse_input_char_index)\n# # print(input_token_index)\n\n# reverse_input_token = dict((i, char) for char, i in input_token.items())\n# reverse_target_token = dict((i, char) for char, i in target_token.items())\n# def decode_sequence(input_seq):\n#     # Encode the input as state vectors.\n#     states_value = encoder_model.predict(input_seq)\n\n#     # Generate empty target sequence of length 1.\n#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n#     # Populate the first character of target sequence with the start character.\n#     target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n\n#     # Sampling loop for a batch of sequences\n#     # (to simplify, here we assume a batch of size 1).\n#     stop_condition = False\n#     decoded_sentence = \"\"\n#     while not stop_condition:\n#         temp = decoder_model.predict([target_seq] + states_value)\n#         output_tokens, state = temp[0],temp[1:]\n\n#         # Sample a token\n#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n#         #print(reverse_target_char_index)\n#         sampled_char = reverse_target_token[sampled_token_index]\n#         decoded_sentence += sampled_char\n\n#         # Exit condition: either hit max length\n#         # or find stop character.\n#         if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n#             stop_condition = True\n\n#         # Update the target sequence (of length 1).\n#         target_seq = np.zeros((1, 1, num_decoder_tokens))\n#         target_seq[0, 0, sampled_token_index] = 1.0\n\n#         # Update states\n#         states_value = state\n#     return decoded_sentence\n\n","metadata":{"id":"meuadqgkAQeg","execution":{"iopub.status.busy":"2022-06-22T08:40:54.626328Z","iopub.execute_input":"2022-06-22T08:40:54.626998Z","iopub.status.idle":"2022-06-22T08:40:54.637456Z","shell.execute_reply.started":"2022-06-22T08:40:54.626963Z","shell.execute_reply":"2022-06-22T08:40:54.636537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can now generate decoded sentences as such:\n","metadata":{"id":"nSNoQ5AvAQeh"}},{"cell_type":"code","source":"# for seq_index in range(20):\n#     # Take one sequence (part of the training set)\n#     # for trying out decoding.\n#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n#     decoded_sentence = decode_sequence(input_seq)\n#     print(\"-\")\n#     print(\"Input sentence:\", input_texts[seq_index])\n#     print(\"Decoded sentence:\", decoded_sentence)\n","metadata":{"id":"wjp__2oJAQeh","execution":{"iopub.status.busy":"2022-06-22T08:40:54.640429Z","iopub.execute_input":"2022-06-22T08:40:54.640681Z","iopub.status.idle":"2022-06-22T08:40:54.650442Z","shell.execute_reply.started":"2022-06-22T08:40:54.64065Z","shell.execute_reply":"2022-06-22T08:40:54.649491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! git log","metadata":{"id":"ygwmClalB8vI","outputId":"09f6fff2-5b5d-42a4-e7b8-2e20df103971","execution":{"iopub.status.busy":"2022-06-22T08:40:54.651773Z","iopub.execute_input":"2022-06-22T08:40:54.65276Z","iopub.status.idle":"2022-06-22T08:40:54.65989Z","shell.execute_reply.started":"2022-06-22T08:40:54.652699Z","shell.execute_reply":"2022-06-22T08:40:54.658996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"lzaKcIYKrP39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"oX52e6mtrP39"},"execution_count":null,"outputs":[]}]}