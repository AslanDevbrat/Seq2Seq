{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "XtD33RcAAQeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "UBUz4pd87YeP",
        "execution": {
          "iopub.status.busy": "2022-06-19T11:46:04.715027Z",
          "iopub.execute_input": "2022-06-19T11:46:04.715895Z",
          "iopub.status.idle": "2022-06-19T11:46:36.340025Z",
          "shell.execute_reply.started": "2022-06-19T11:46:04.715742Z",
          "shell.execute_reply": "2022-06-19T11:46:36.338600Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNNCell, GRUCell, Dense, LSTMCell\n",
        "from tensorflow.keras import Input\n",
        "import pandas as pd\n",
        "from numpy import argmax\n",
        "from math import log\n",
        "import pprint\n",
        "import math\n",
        "import wandb\n",
        "import os\n",
        "import io\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# wandb_api = user_secrets.get_secret(\"wandb_api\")\n",
        "\n",
        "# #wandb.login(key=wandb_api)\n",
        "# ! wandb login $wandb_api\n",
        "\n",
        "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "EnvOeSibAQeV",
        "outputId": "9d21a4b6-32f8-446e-e263-1deb828afd83",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:55:00.466081Z",
          "iopub.execute_input": "2022-06-19T14:55:00.466557Z",
          "iopub.status.idle": "2022-06-19T14:55:04.846701Z",
          "shell.execute_reply.started": "2022-06-19T14:55:00.466520Z",
          "shell.execute_reply": "2022-06-19T14:55:04.844596Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data\n"
      ],
      "metadata": {
        "id": "NbOpYTqYAQeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n"
      ],
      "metadata": {
        "id": "1rKnaKIZAQeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf 'dakshina_dataset_v1.0.tar'\n",
        "train_file_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "val_file_path= \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "test_file_path  = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\""
      ],
      "metadata": {
        "id": "roFBlOuMAYkx",
        "outputId": "9b6d91b5-f474-4f64-fcd2-9bef44e7f70e",
        "execution": {
          "iopub.status.busy": "2022-06-19T11:47:33.470862Z",
          "iopub.execute_input": "2022-06-19T11:47:33.471940Z",
          "iopub.status.idle": "2022-06-19T11:47:49.677198Z",
          "shell.execute_reply.started": "2022-06-19T11:47:33.471901Z",
          "shell.execute_reply": "2022-06-19T11:47:49.676010Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-20 10:36:57--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 108.177.98.128, 74.125.197.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   121MB/s    in 16s     \n",
            "\n",
            "2022-06-20 10:37:13 (121 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 100000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = train_file_path\n"
      ],
      "metadata": {
        "id": "akMCxfVHAQeZ",
        "execution": {
          "iopub.status.busy": "2022-06-19T09:33:02.211259Z",
          "iopub.execute_input": "2022-06-19T09:33:02.211737Z",
          "iopub.status.idle": "2022-06-19T09:33:02.219013Z",
          "shell.execute_reply.started": "2022-06-19T09:33:02.211682Z",
          "shell.execute_reply": "2022-06-19T09:33:02.217846Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n"
      ],
      "metadata": {
        "id": "U-3Djb36AQea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processData(filename,input_chars=set(),target_chars=set()):\n",
        "  input=[]\n",
        "  target=[]\n",
        "  with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:len(lines) -1]:\n",
        "      t_text,i_text, attestation = line.split(\"\\t\")\n",
        "       # We use \"\\t\" as the \"start sequence\" character and \"\\n\" as \"end sequence\" character for the target text.\n",
        "      input.append(i_text)\n",
        "      target.append(\"\\t\"+t_text+\"\\n\")\n",
        "      for char in i_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "      for char in t_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "  target_chars.add(\"\\t\")\n",
        "  target_chars.add(\"\\n\")\n",
        "\n",
        "  input_chars = sorted(list(input_chars))\n",
        "  target_chars = sorted(list(target_chars))\n",
        "  num_encoder_tokens = len(input_chars)\n",
        "  num_decoder_tokens = len(target_chars)\n",
        "  max_encoder_seq_length = max([len(txt) for txt in input])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target])\n",
        "  return input,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length     "
      ],
      "metadata": {
        "id": "5CZtxlJmaYmb",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:21:54.385136Z",
          "iopub.execute_input": "2022-06-19T15:21:54.385606Z",
          "iopub.status.idle": "2022-06-19T15:21:54.397778Z",
          "shell.execute_reply.started": "2022-06-19T15:21:54.385570Z",
          "shell.execute_reply": "2022-06-19T15:21:54.396551Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length=processData(train_file_path)\n",
        "print(\"Number of samples:\", len(input))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "pg8AcuMoab8U",
        "outputId": "d23f2f4c-71b5-4e1c-a69c-5d2e862e87a0",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:21:56.055100Z",
          "iopub.execute_input": "2022-06-19T15:21:56.055975Z",
          "iopub.status.idle": "2022-06-19T15:21:56.181503Z",
          "shell.execute_reply.started": "2022-06-19T15:21:56.055922Z",
          "shell.execute_reply": "2022-06-19T15:21:56.179927Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 44204\n",
            "Number of unique input tokens: 26\n",
            "Number of unique output tokens: 65\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "# Vectorize the data.\n",
        "validation_input,validation_target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, validation_max_encoder_seq_length, validation_max_decoder_seq_length=processData(val_file_path,set(input_chars),set(target_chars))\n",
        "\n",
        "print(\"Number of validation samples:\", len(validation_input))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"validation Max sequence length for inputs:\", validation_max_encoder_seq_length)\n",
        "print(\"validation Max sequence length for outputs:\", validation_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "lmCnQKWNbsuh",
        "outputId": "297588b2-3d49-4b22-81c1-9d513c8cf6a9",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:32:00.457765Z",
          "iopub.execute_input": "2022-06-19T15:32:00.458307Z",
          "iopub.status.idle": "2022-06-19T15:32:00.484257Z",
          "shell.execute_reply.started": "2022-06-19T15:32:00.458272Z",
          "shell.execute_reply": "2022-06-19T15:32:00.482872Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation samples: 4502\n",
            "Number of unique input tokens: 26\n",
            "Number of unique output tokens: 65\n",
            "validation Max sequence length for inputs: 16\n",
            "validation Max sequence length for outputs: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "test_input,test_target,test_input_chars,test_target_chars,test_num_encoder_tokens,test_num_decoder_tokens, test_max_encoder_seq_length, test_max_decoder_seq_length=processData(test_file_path)\n",
        "print(\"Number of validation samples:\", len(test_input))\n",
        "print(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "Q9tUFlovbz5i",
        "outputId": "9a4ce782-d90f-4fce-a15b-cd3cc944ae19",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:05:18.181757Z",
          "iopub.execute_input": "2022-06-19T15:05:18.182569Z",
          "iopub.status.idle": "2022-06-19T15:05:18.200082Z",
          "shell.execute_reply.started": "2022-06-19T15:05:18.182519Z",
          "shell.execute_reply": "2022-06-19T15:05:18.199204Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation samples: 4358\n",
            "Test Max sequence length for inputs: 18\n",
            "Test Max sequence length for outputs: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_encoder_input_data=np.zeros(\n",
        "    (len(validation_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "test_encoder_input_data=np.zeros(\n",
        "    (len(test_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_decoder_input_data =np.zeros(\n",
        "    (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_decoder_target_data = np.zeros(\n",
        "    (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input, target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token[char]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "# for validation data\n",
        "for i, (validation_input_text, validation_target_text) in enumerate(zip(validation_input, validation_target)):\n",
        "    for t, char in enumerate(validation_input_text):\n",
        "        validation_encoder_input_data[i, t, input_token[char]] = 1.0\n",
        "    for t, char in enumerate(validation_target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        validation_decoder_input_data[i, t, target_token[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            validation_decoder_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "\n",
        "# for test data\n",
        "for i, (test_input_text, test_target_text) in enumerate(zip(test_input, test_target)):\n",
        "    for t, char in enumerate(test_input_text):\n",
        "        test_encoder_input_data[i, t, input_token[char]] = 1.0"
      ],
      "metadata": {
        "id": "oBF7Cdqrb5Hc",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:30:35.223556Z",
          "iopub.execute_input": "2022-06-19T14:30:35.224039Z",
          "iopub.status.idle": "2022-06-19T14:30:36.884489Z",
          "shell.execute_reply.started": "2022-06-19T14:30:35.224003Z",
          "shell.execute_reply": "2022-06-19T14:30:36.883493Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2q7AdDj4P_8p",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:30:37.301677Z",
          "iopub.execute_input": "2022-06-19T14:30:37.302743Z",
          "iopub.status.idle": "2022-06-19T14:30:38.242257Z",
          "shell.execute_reply.started": "2022-06-19T14:30:37.302700Z",
          "shell.execute_reply": "2022-06-19T14:30:38.241015Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "\n",
        "enc_input_data = np.zeros(\n",
        "    (len(input), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_input_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_target_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length,num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "#Decoder Target Sequences are Padded to a maximum length of max_decoder SeqLen characters with a vocabulary of sizeofTeluguVocab different characters. \n",
        "for i, (input_text, target_text) in enumerate(zip(input, target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        enc_input_data[i, t] = input_token[char]\n",
        "    #enc_input_data[i, t + 1 :] = input_token[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # dec_target_data is ahead of dec_input_data by one timestep\n",
        "        dec_input_data[i, t] = target_token[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will not include the start character.\n",
        "            dec_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "    #dec_input_data[i, t + 1: ] = target_token[\" \"]\n",
        "    #dec_target_data[i, t:, target_token[\" \"]] = 1.0\n",
        "    \n",
        "val_enc_input_data = np.zeros(\n",
        "    (len(validation_input), validation_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_input_data = np.zeros(\n",
        "    (len(validation_input), validation_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_target_data = np.zeros(\n",
        "    (len(validation_input), validation_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(validation_input,validation_target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        # Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object. \n",
        "        # This enumerated object can then be used directly for loops or converted into a list of tuples using the list() method.\n",
        "        val_enc_input_data[i, t] = input_token[char]\n",
        "    #val_enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        val_dec_input_data[i, t] = target_token[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_dec_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "    #val_dec_input_data[i, t + 1: ] = target_token_index[\" \"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:33:00.750838Z",
          "iopub.execute_input": "2022-06-19T15:33:00.751958Z",
          "iopub.status.idle": "2022-06-19T15:33:01.589162Z",
          "shell.execute_reply.started": "2022-06-19T15:33:00.751901Z",
          "shell.execute_reply": "2022-06-19T15:33:01.588102Z"
        },
        "trusted": true,
        "id": "6_L_o5BGrP3z"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTDataset:\n",
        "    def __init__(self, problem_type='en-hi'):\n",
        "        self.problem_type = 'en-'\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "    \n",
        "\n",
        "    def unicode_to_ascii(self, s):\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    ## Step 1 and Step 2 \n",
        "    def preprocess_sentence(self, w):\n",
        "        # w = self.unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "        # # creating a space between a word and the punctuation following it\n",
        "        # # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "        # # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        # w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "        # w = w.strip()\n",
        "\n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        #print(w)\n",
        "        w = '\\t' + w + '\\n'\n",
        "        \n",
        "        return w\n",
        "    \n",
        "    def create_dataset(self, path, num_examples):\n",
        "        # path : path to spa-eng.txt file\n",
        "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
        "        #lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "        #word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "        data =  pd.read_csv(path,delimiter=\"\\t\", header= None, nrows = num_examples )\n",
        "        data = data.dropna()\n",
        "        print(data.info())\n",
        "        return data[0].apply(self.preprocess_sentence).values.astype(str), data[1].apply(self.preprocess_sentence).values.astype(str)\n",
        "\n",
        "    # Step 3 and Step 4\n",
        "    def tokenize(self, lang):\n",
        "        # lang = list of sentences in a language\n",
        "        \n",
        "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True)\n",
        "        lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "        ## and pads the sequences to match the longest sequences in the given input\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        return tensor, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self, path, num_examples=None):\n",
        "        # creating cleaned input, output pairs\n",
        "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
        "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
        "        #file_path = download_dakshina()\n",
        "        input_tensor_train, target_tensor_train, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(train_file_path, num_examples)\n",
        "        input_tensor_val, target_tensor_val, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(val_file_path, num_examples)\n",
        "        input_tensor_test, target_tensor_test, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(test_file_path, num_examples)\n",
        "        x = input_tensor_train\n",
        "        y  =target_tensor_train\n",
        "        #input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.4)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train))\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, target_tensor_test))\n",
        "        test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        return train_dataset, val_dataset, test_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer\n"
      ],
      "metadata": {
        "id": "EOIwk0-PrP3z"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64\n",
        "# Let's limit the #training examples for faster training\n",
        "num_examples = 300000\n",
        "\n",
        "dataset_creator = NMTDataset('en-spa')\n",
        "train_dataset, val_dataset,test_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "s3Wn4CM1zgLH",
        "outputId": "ef9d53f4-fe3b-48f2-8a02-83a8c2615fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 44202 entries, 0 to 44203\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       44202 non-null  object\n",
            " 1   1       44202 non-null  object\n",
            " 2   2       44202 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4502 entries, 0 to 4501\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       4502 non-null   object\n",
            " 1   1       4502 non-null   object\n",
            " 2   2       4502 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 140.7+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4358 entries, 0 to 4357\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       4358 non-null   object\n",
            " 1   1       4358 non-null   object\n",
            " 2   2       4358 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 136.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "id": "9gaZrS4g1T5X",
        "outputId": "97ad97b4-743b-44fc-81d2-ccd5ff884d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 22]), TensorShape([64, 21]))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n"
      ],
      "metadata": {
        "id": "eGM50wwWAQed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, num_of_layers, enc_unit_type, batch_sz, recurrent_dropout, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.enc_unit_type = enc_unit_type\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.dropout = dropout\n",
        "    self.embedding = Embedding( vocab_size, embedding_dim)\n",
        "\n",
        "    self.encoder_layer = self.get_encoder_layer(self.enc_units,\n",
        "                                                self.num_of_layers, self.enc_unit_type)\n",
        "    \n",
        "\n",
        "  def get_encoder_layer(self, enc_units, num_of_layers, enc_unit_type):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(enc_unit_type, \n",
        "                                                                                 enc_units) for i in range(num_of_layers)],),\n",
        "                                  return_sequences=True, return_state=True, name = \"Encoder\")\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "      #print(cell_type)\n",
        "      if cell_type == \"lstm\":\n",
        "        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "      elif cell_type == \"rnn\":\n",
        "        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      elif cell_type ==\"gru\":\n",
        "        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      else:\n",
        "        print(f\"Invalid cell type: {cell_type}\")\n",
        "\n",
        "    \n",
        "  def call(self, x):\n",
        "      x = self.embedding(x)\n",
        "      output = self.encoder_layer(x,)\n",
        "\n",
        "      #print(output)\n",
        "      return output\n",
        "    \n",
        "  def initialize_hidden_state(self):\n",
        "      print(\"Called\")\n",
        "      return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "metadata": {
        "id": "-_IMrNwXsYR4"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_input_data[0].shape"
      ],
      "metadata": {
        "id": "3iT2EuNf1TDo",
        "outputId": "7a08f4c6-f141-4984-9b5f-544a2c3aceec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\n",
        "encoder = Encoder( num_encoder_tokens, 1, 16, 3, \"lstm\", 64, 0.0, 0.0)\n",
        "#sample_hidden = encoder.initialize_hidden_state()\n",
        "# encoder.build(input_shape =(None,26))\n",
        "# encoder.summary()\n",
        "sample_output = encoder(enc_input_data[:64])\n",
        "out , state = sample_output[0], sample_output[1:]"
      ],
      "metadata": {
        "id": "oG6E1qaLv92s"
      },
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, state"
      ],
      "metadata": {
        "id": "yHmvx-1zRHJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, num_of_layers, \n",
        "               dec_unit_type, batch_sz, recurrent_dropout, dropout, \n",
        "               attention_type = \"luong\"):\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.dec_unit_type = dec_unit_type\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.attention_type = attention_type\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.dropout = dropout\n",
        "    print(\"decoder embedding dim\", embedding_dim)\n",
        "    self.embedding = Embedding( vocab_size, embedding_dim)\n",
        "\n",
        "    self.fc  = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.decoder_cells = self.get_stacked_rnn_cell()\n",
        "\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, None\n",
        "                                                              , self.batch_sz*[max_encoder_seq_length], \n",
        "                                                              self.attention_type)\n",
        "\n",
        "    self.cell = self.build_cell()\n",
        "\n",
        "    #print(self.cell)\n",
        "\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.cell, sampler = self.sampler, output_layer = self.fc)\n",
        "\n",
        "\n",
        "\n",
        "  def build_cell(self):\n",
        "    cell = tfa.seq2seq.AttentionWrapper(self.decoder_cells, self.attention_mechanism,\n",
        "                                        attention_layer_size = self.dec_units)\n",
        "    return cell\n",
        "  \n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "      #print(cell_type)\n",
        "      if cell_type == \"lstm\":\n",
        "        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "      elif cell_type == \"rnn\":\n",
        "        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      elif cell_type ==\"gru\":\n",
        "        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      else:\n",
        "        print(f\"Invalid cell type: {cell_type}\")\n",
        "\n",
        "  def get_stacked_rnn_cell(self,):\n",
        "    return tf.keras.layers.StackedRNNCells( [self.get_cell(self.dec_unit_type, self.dec_units,) for i in range(self.num_of_layers)])\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    #print(batch_sz)\n",
        "    #print(len(encoder_state))\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "  def call(self, x, inital_state):\n",
        "    x = self.embedding(x)\n",
        "    print(\"calles\")\n",
        "    output = self.decoder(x, initial_state=initial_state)\n",
        "    return output"
      ],
      "metadata": {
        "id": "vPiwKS7P7A3X"
      },
      "execution_count": 431,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\n",
        "decoder = Decoder( num_decoder_tokens,  1, 16, 3, \"lstm\", 64, 0.0, 0.0)\n",
        "#sample_hidden = encoder.initialize_hidden_state()\n",
        "#decoder.build(input_shape =(None, ))\n",
        "# decoder.summary()\n",
        "#sample_x = tf.random.uniform((2  ,max_decoder_seq_length))\n",
        "decoder.attention_mechanism.setup_memory(out)\n",
        "initial_state = decoder.build_initial_state(64, tuple(state), tf.float32)\n",
        "# sample_output = decoder(dec_input_data[:8192], initial_state)\n",
        "# out1 , state1 = sample_output[0], sample_output[1:]"
      ],
      "metadata": {
        "id": "Fe8whQdYCdfZ",
        "outputId": "62bd1aa0-6726-41c8-a941-25bf2702c4fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder embedding dim 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1"
      ],
      "metadata": {
        "id": "s1cQ1JBzWhth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(tf.keras.Model):\n",
        "  def __init__(self, num_encoder_tokens, num_decoder_token, encoder_embedding_dim, decoder_embedding_dim,num_of_unit, num_of_layers, unit_type, batch_size, recurrent_dropout, dropout):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = Encoder(  num_encoder_tokens, 1, 16, 3, \"lstm\", self.batch_size, 0.0, 0.0)\n",
        "    #self.encoder.summary()\n",
        "    self.dec = Decoder( num_decoder_tokens,  1, 16, 3, \"lstm\", self.batch_size, 0.0, 0.0)\n",
        "    #sample_x = tf.random.uniform((batch_size  ,max_decoder_seq_length))\n",
        "\n",
        "  def call(self, enc_inp, dec_inp):\n",
        "    print(\"fsdfa\",dec_inp.shape)\n",
        "    x = self.encoder(enc_inp)\n",
        "    enc_out, enc_state = x[0], x[1:]\n",
        "    print(enc_out.shape)\n",
        "    self.dec.attention_mechanism.setup_memory(enc_out)\n",
        "    dec_initial_state = self.dec.build_initial_state(self.batch_size, tuple(enc_state), tf.float32)\n",
        "    print(\"fucck\")\n",
        "    x = self.dec(dec_inp,dec_initial_state)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "cVoQMjKYYLX2"
      },
      "execution_count": 506,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "s2s = seq2seq(num_encoder_tokens, num_decoder_tokens,  encoder_embedding_dim =256,\n",
        "              decoder_embedding_dim= 1024,\n",
        "              num_of_unit =16,\n",
        "              num_of_layers = 5, \n",
        "              unit_type =\"gru\",\n",
        "              batch_size = batch_size, \n",
        "              recurrent_dropout = 0,\n",
        "              dropout = 0 )\n",
        "s2s(enc_input_data[:batch_size], dec_input_data[:batch_size])"
      ],
      "metadata": {
        "id": "BRKTJEGid7MN",
        "outputId": "2f8b153a-cd65-486e-9f75-0e8ee46d3e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 521,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder embedding dim 1\n",
            "fsdfa (64, 21)\n",
            "(64, 20, 16)\n",
            "fucck\n",
            "calles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BasicDecoderOutput(rnn_output=<tf.Tensor: shape=(64, 21, 65), dtype=float32, numpy=\n",
              "array([[[-6.13270022e-05,  2.31395989e-05, -8.10927886e-05, ...,\n",
              "          6.83790888e-04,  1.83056138e-04, -6.49537425e-04],\n",
              "        [-1.82759468e-05,  2.38674220e-05, -1.15368108e-04, ...,\n",
              "          6.18242717e-04,  1.94367414e-04, -6.05694368e-04],\n",
              "        [ 4.25923172e-05,  2.15497639e-05, -1.44162739e-04, ...,\n",
              "          5.58540865e-04,  2.10503626e-04, -5.71234617e-04],\n",
              "        ...,\n",
              "        [ 1.10182038e-03, -3.12013930e-04, -2.64843169e-04, ...,\n",
              "          1.68447310e-04, -2.89943346e-05, -3.45949084e-04],\n",
              "        [ 1.11428415e-03, -3.25001456e-04, -2.67812895e-04, ...,\n",
              "          1.60258642e-04, -6.42661762e-05, -3.38802114e-04],\n",
              "        [ 1.12430984e-03, -3.36707250e-04, -2.70811433e-04, ...,\n",
              "          1.52624969e-04, -9.67412416e-05, -3.32208932e-04]],\n",
              "\n",
              "       [[ 1.73842302e-04,  5.57638887e-05, -1.88074308e-04, ...,\n",
              "          1.46988503e-04, -8.58596377e-06, -4.62570955e-04],\n",
              "        [ 1.89216866e-04,  3.98667944e-05, -2.30048739e-04, ...,\n",
              "          1.40276577e-04,  2.89248219e-05, -4.13629168e-04],\n",
              "        [ 2.23182316e-04,  2.48397482e-05, -2.58310407e-04, ...,\n",
              "          1.37811585e-04,  6.74877665e-05, -3.77573248e-04],\n",
              "        ...,\n",
              "        [ 8.97129416e-04, -1.78459639e-04, -1.90923965e-04, ...,\n",
              "          1.08897650e-04,  2.34673076e-04, -2.69469980e-04],\n",
              "        [ 9.54287476e-04, -1.99144299e-04, -1.96605746e-04, ...,\n",
              "          9.63614657e-05,  2.21462207e-04, -2.57943204e-04],\n",
              "        [ 1.00240379e-03, -2.19357345e-04, -2.01896342e-04, ...,\n",
              "          8.52169687e-05,  2.00470124e-04, -2.46946467e-04]],\n",
              "\n",
              "       [[ 1.50706153e-04,  4.88359910e-05, -9.09759037e-05, ...,\n",
              "          2.57054024e-04,  6.09981435e-05, -4.59001865e-04],\n",
              "        [ 1.73271823e-04,  3.81028440e-05, -1.32298315e-04, ...,\n",
              "          2.31160084e-04,  9.07665890e-05, -4.09200788e-04],\n",
              "        [ 2.14992368e-04,  2.70551918e-05, -1.63057484e-04, ...,\n",
              "          2.09274847e-04,  1.23000107e-04, -3.71077389e-04],\n",
              "        ...,\n",
              "        [ 1.14921539e-03, -2.92324607e-04, -2.11736886e-04, ...,\n",
              "          1.53001911e-05,  8.33160448e-05, -1.65344405e-04],\n",
              "        [ 1.16731832e-03, -3.07882146e-04, -2.13666717e-04, ...,\n",
              "          7.82024108e-06,  4.78748334e-05, -1.57697956e-04],\n",
              "        [ 1.18175452e-03, -3.22160282e-04, -2.15704844e-04, ...,\n",
              "          8.29762484e-07,  1.34943793e-05, -1.50589200e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 6.51780138e-05,  3.21544503e-05, -6.63986066e-05, ...,\n",
              "          3.39442224e-04,  9.92331770e-05, -4.26911516e-04],\n",
              "        [ 1.01545018e-04,  2.61391033e-05, -1.00971527e-04, ...,\n",
              "          3.02680390e-04,  1.25145714e-04, -3.87031236e-04],\n",
              "        [ 1.54692156e-04,  1.88926206e-05, -1.28275802e-04, ...,\n",
              "          2.69757351e-04,  1.55169444e-04, -3.55472846e-04],\n",
              "        ...,\n",
              "        [ 5.33928338e-04, -9.87907479e-05, -1.14510614e-04, ...,\n",
              "          1.37200972e-04,  1.05847299e-04, -2.74605671e-04],\n",
              "        [ 6.35990058e-04, -1.16242489e-04, -1.25195176e-04, ...,\n",
              "          1.16169722e-04,  1.56486160e-04, -2.61211040e-04],\n",
              "        [ 7.30065396e-04, -1.35000941e-04, -1.37334704e-04, ...,\n",
              "          9.64201972e-05,  1.92719046e-04, -2.47441698e-04]],\n",
              "\n",
              "       [[ 1.07711363e-04,  4.40188996e-05, -1.29460677e-04, ...,\n",
              "          2.98752857e-04,  5.92579963e-05, -4.80701739e-04],\n",
              "        [ 1.35015100e-04,  3.41318591e-05, -1.67653125e-04, ...,\n",
              "          2.71711615e-04,  8.79793370e-05, -4.35852475e-04],\n",
              "        [ 1.80092800e-04,  2.38443881e-05, -1.95745131e-04, ...,\n",
              "          2.49064062e-04,  1.19638593e-04, -4.01668105e-04],\n",
              "        ...,\n",
              "        [ 8.26241681e-04, -1.44197184e-04, -1.76083093e-04, ...,\n",
              "          1.25964216e-04,  2.47316930e-04, -2.94481841e-04],\n",
              "        [ 8.97116261e-04, -1.65767109e-04, -1.85173005e-04, ...,\n",
              "          1.10584660e-04,  2.45656702e-04, -2.81370478e-04],\n",
              "        [ 9.57358803e-04, -1.87206562e-04, -1.93517146e-04, ...,\n",
              "          9.71114569e-05,  2.33323153e-04, -2.68831820e-04]],\n",
              "\n",
              "       [[ 1.70998537e-05,  2.67623600e-05, -1.40940792e-05, ...,\n",
              "          5.18459012e-04,  1.69589039e-04, -5.13974694e-04],\n",
              "        [ 5.66986928e-05,  2.49669247e-05, -5.01441100e-05, ...,\n",
              "          4.61505464e-04,  1.85590892e-04, -4.69314866e-04],\n",
              "        [ 1.14487011e-04,  2.09118116e-05, -8.06504395e-05, ...,\n",
              "          4.09110216e-04,  2.06627345e-04, -4.33552952e-04],\n",
              "        ...,\n",
              "        [ 8.30991019e-04, -1.42048943e-04, -1.64269091e-04, ...,\n",
              "          1.19365744e-04,  2.45124305e-04, -2.86980299e-04],\n",
              "        [ 9.02340340e-04, -1.64056328e-04, -1.75556648e-04, ...,\n",
              "          1.02999889e-04,  2.42673545e-04, -2.73041544e-04],\n",
              "        [ 9.62920429e-04, -1.85883488e-04, -1.85707089e-04, ...,\n",
              "          8.87858259e-05,  2.29850935e-04, -2.59799563e-04]]],\n",
              "      dtype=float32)>, sample_id=<tf.Tensor: shape=(64, 21), dtype=int32, numpy=\n",
              "array([[58, 58, 58, ..., 29, 29, 29],\n",
              "       [58, 58, 58, ..., 29, 29, 29],\n",
              "       [58, 58, 58, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [58, 58, 58, ..., 58, 29, 29],\n",
              "       [58, 58, 58, ..., 29, 29, 29],\n",
              "       [58, 58, 58, ..., 29, 29, 29]], dtype=int32)>),\n",
              " AttentionWrapperState(cell_state=([<tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[-0.00291504, -0.0081221 , -0.00164781, ...,  0.00141712,\n",
              "         0.00211466, -0.00092172],\n",
              "       [-0.00310391, -0.00786205, -0.00145529, ...,  0.00159932,\n",
              "         0.00197792, -0.0011042 ],\n",
              "       [-0.00298255, -0.00816006, -0.00189051, ...,  0.00143091,\n",
              "         0.00173924, -0.00102786],\n",
              "       ...,\n",
              "       [-0.00323616, -0.00704328, -0.00100456, ...,  0.00185847,\n",
              "         0.00188986, -0.00090966],\n",
              "       [-0.00313015, -0.00775118, -0.00132819, ...,  0.00165994,\n",
              "         0.00202746, -0.00110689],\n",
              "       [-0.00312549, -0.00776829, -0.00133625, ...,  0.00164807,\n",
              "         0.00201552, -0.00111779]], dtype=float32)>, <tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[-0.00581885, -0.01622809, -0.0033043 , ...,  0.00283683,\n",
              "         0.00423867, -0.00184205],\n",
              "       [-0.00619796, -0.01571179, -0.00291793, ...,  0.00320193,\n",
              "         0.00396431, -0.00220659],\n",
              "       [-0.00595516, -0.01630646, -0.00379045, ...,  0.002865  ,\n",
              "         0.00348598, -0.00205439],\n",
              "       ...,\n",
              "       [-0.00646295, -0.01407715, -0.00201418, ...,  0.00372102,\n",
              "         0.00378749, -0.00181773],\n",
              "       [-0.00625054, -0.01549055, -0.00266312, ...,  0.00332323,\n",
              "         0.00406357, -0.0022119 ],\n",
              "       [-0.00624131, -0.0155248 , -0.00267927, ...,  0.00329949,\n",
              "         0.0040396 , -0.0022337 ]], dtype=float32)>], [<tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[-1.3062507e-03, -8.7658362e-04,  2.6522966e-03, ...,\n",
              "        -2.6701218e-05, -1.7695555e-03, -1.6980852e-03],\n",
              "       [-1.0228573e-03, -7.6517486e-04,  2.5816760e-03, ...,\n",
              "        -1.9820945e-04, -1.6136887e-03, -1.2357922e-03],\n",
              "       [-1.2623913e-03, -9.4600272e-04,  2.6544728e-03, ...,\n",
              "        -6.4542808e-05, -1.9617148e-03, -1.4990839e-03],\n",
              "       ...,\n",
              "       [-6.4876647e-04, -5.9322978e-04,  2.1942519e-03, ...,\n",
              "        -2.5654558e-04, -1.3346911e-03, -8.9720846e-04],\n",
              "       [-9.4563287e-04, -7.1983656e-04,  2.5407069e-03, ...,\n",
              "        -2.3978928e-04, -1.5305798e-03, -1.1574472e-03],\n",
              "       [-9.5502503e-04, -7.2503026e-04,  2.5491323e-03, ...,\n",
              "        -2.4590254e-04, -1.5404740e-03, -1.1632731e-03]], dtype=float32)>, <tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[-2.6115035e-03, -1.7506736e-03,  5.3062551e-03, ...,\n",
              "        -5.3364140e-05, -3.5409932e-03, -3.3871296e-03],\n",
              "       [-2.0450938e-03, -1.5281182e-03,  5.1647425e-03, ...,\n",
              "        -3.9610240e-04, -3.2290965e-03, -2.4653214e-03],\n",
              "       [-2.5238926e-03, -1.8891850e-03,  5.3100386e-03, ...,\n",
              "        -1.2898626e-04, -3.9255512e-03, -2.9900884e-03],\n",
              "       ...,\n",
              "       [-1.2972041e-03, -1.1846670e-03,  4.3894621e-03, ...,\n",
              "        -5.1262899e-04, -2.6708310e-03, -1.7902816e-03],\n",
              "       [-1.8907228e-03, -1.4375814e-03,  5.0828420e-03, ...,\n",
              "        -4.7918764e-04, -3.0627847e-03, -2.3091433e-03],\n",
              "       [-1.9095158e-03, -1.4479540e-03,  5.0997040e-03, ...,\n",
              "        -4.9140310e-04, -3.0825764e-03, -2.3207460e-03]], dtype=float32)>], [<tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[ 3.8705394e-04,  1.5430991e-03,  1.5828375e-03, ...,\n",
              "         9.7917800e-04, -6.5641594e-04,  7.6856202e-04],\n",
              "       [-5.4642802e-05,  1.2059517e-03,  1.1889709e-03, ...,\n",
              "         6.2710606e-04, -5.9324084e-04,  7.6016743e-04],\n",
              "       [ 1.9518103e-04,  1.5515207e-03,  1.4231825e-03, ...,\n",
              "         8.7403192e-04, -5.9740851e-04,  8.8648940e-04],\n",
              "       ...,\n",
              "       [-1.2076740e-04,  7.8960496e-04,  8.4606378e-04, ...,\n",
              "         3.9330375e-04, -4.1885339e-04,  5.3006876e-04],\n",
              "       [-1.1408374e-04,  1.0977921e-03,  1.1094469e-03, ...,\n",
              "         5.5813539e-04, -5.6934892e-04,  7.0880016e-04],\n",
              "       [-1.1351175e-04,  1.0941492e-03,  1.1058705e-03, ...,\n",
              "         5.6878879e-04, -5.5310572e-04,  7.0198416e-04]], dtype=float32)>, <tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[ 0.0007747 ,  0.0030901 ,  0.00316662, ...,  0.00195664,\n",
              "        -0.0013137 ,  0.00153719],\n",
              "       [-0.00010935,  0.00241445,  0.00237835, ...,  0.00125354,\n",
              "        -0.0011873 ,  0.00152038],\n",
              "       [ 0.00039068,  0.0031068 ,  0.00284724, ...,  0.00174663,\n",
              "        -0.00119564,  0.00177301],\n",
              "       ...,\n",
              "       [-0.00024163,  0.00158055,  0.0016923 , ...,  0.00078639,\n",
              "        -0.00083821,  0.00106019],\n",
              "       [-0.0002283 ,  0.0021978 ,  0.00221921, ...,  0.00111576,\n",
              "        -0.00113947,  0.00141765],\n",
              "       [-0.00022715,  0.00219052,  0.00221205, ...,  0.00113704,\n",
              "        -0.00110696,  0.00140401]], dtype=float32)>]), attention=<tf.Tensor: shape=(64, 16), dtype=float32, numpy=\n",
              "array([[-0.001819  , -0.0015186 , -0.00072812, ...,  0.00111627,\n",
              "        -0.00010958, -0.00138402],\n",
              "       [-0.00122402, -0.00110271, -0.00046592, ...,  0.00064088,\n",
              "        -0.00022268, -0.00115134],\n",
              "       [-0.0014367 , -0.00145531, -0.00039294, ...,  0.00042962,\n",
              "        -0.00028262, -0.00110669],\n",
              "       ...,\n",
              "       [-0.00089336, -0.00068305, -0.00046236, ...,  0.00057756,\n",
              "        -0.00016316, -0.00091519],\n",
              "       [-0.00115643, -0.00098176, -0.0004902 , ...,  0.00066208,\n",
              "        -0.00021494, -0.00114037],\n",
              "       [-0.00114585, -0.00098042, -0.00047555, ...,  0.0006349 ,\n",
              "        -0.00022978, -0.00113482]], dtype=float32)>, alignments=<tf.Tensor: shape=(64, 20), dtype=float32, numpy=\n",
              "array([[0.0500003 , 0.05000029, 0.05000028, ..., 0.04999978, 0.04999978,\n",
              "        0.04999978],\n",
              "       [0.05000019, 0.05000018, 0.05000017, ..., 0.04999979, 0.04999977,\n",
              "        0.04999975],\n",
              "       [0.0500002 , 0.05000019, 0.05000021, ..., 0.04999971, 0.04999969,\n",
              "        0.04999968],\n",
              "       ...,\n",
              "       [0.05000011, 0.05000011, 0.05000011, ..., 0.04999986, 0.04999984,\n",
              "        0.04999983],\n",
              "       [0.05000018, 0.05000018, 0.05000017, ..., 0.04999979, 0.04999978,\n",
              "        0.04999976],\n",
              "       [0.05000017, 0.05000017, 0.05000016, ..., 0.04999979, 0.04999977,\n",
              "        0.04999975]], dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(64, 20), dtype=float32, numpy=\n",
              "array([[0.0500003 , 0.05000029, 0.05000028, ..., 0.04999978, 0.04999978,\n",
              "        0.04999978],\n",
              "       [0.05000019, 0.05000018, 0.05000017, ..., 0.04999979, 0.04999977,\n",
              "        0.04999975],\n",
              "       [0.0500002 , 0.05000019, 0.05000021, ..., 0.04999971, 0.04999969,\n",
              "        0.04999968],\n",
              "       ...,\n",
              "       [0.05000011, 0.05000011, 0.05000011, ..., 0.04999986, 0.04999984,\n",
              "        0.04999983],\n",
              "       [0.05000018, 0.05000018, 0.05000017, ..., 0.04999979, 0.04999978,\n",
              "        0.04999976],\n",
              "       [0.05000017, 0.05000017, 0.05000016, ..., 0.04999979, 0.04999977,\n",
              "        0.04999975]], dtype=float32)>),\n",
              " <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
              " array([21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
              "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
              "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
              "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21], dtype=int32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 521
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Seq2seq(tf.keras.Model):\n",
        "  def __init__(self, num_encoder_tokens, num_decoder_tokens,embedding_dim,num_of_layers,unit_type, dropout , recurrent_dropout):\n",
        "    super().__init__()\n",
        "    self.encoder_inputs = Input(shape = (None,), name = \"Input_layer_1\")\n",
        "    self.decoder_inputs = keras.Input(shape=(None,), name = \"Input_layer_2\")\n",
        "    self.num_encoder_tokens = num_encoder_tokens\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.dropout = dropout\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.num_decoder_tokens = num_decoder_tokens\n",
        "    self.num_of_encoder_layer  =num_of_layers\n",
        "    self.num_of_decoder_layer =num_of_layers\n",
        "    self.type_encoder_unit =unit_type \n",
        "    self.type_decoder_unit =unit_type\n",
        "    self.train_step()\n",
        "    self.build_model()\n",
        "\n",
        "  def get_embedding_layer(self, num_encoder_tokens, embedding_dim,  name):\n",
        "    return Embedding(num_encoder_tokens, embedding_dim, mask_zero = True, name =name )\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "    #print(cell_type)\n",
        "    if cell_type == \"lstm\":\n",
        "      return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "    elif cell_type == \"rnn\":\n",
        "      return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "    elif cell_type ==\"gru\":\n",
        "      return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "    else:\n",
        "      print(f\"Invalid cell type: {cell_type}\")\n",
        "  def get_encoder(self,latent_dim, cell_type = \"lstm\", num_of_layer = 1, name = None ):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim) for i in range(num_of_layer)],), return_sequences=True, return_state=True, name = name)\n",
        "\n",
        "  def get_decoder(self,latent_dim ,cell_type = \"lstm\", num_of_layer = 1, name = None ):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim,) for i in range(num_of_layer)]), return_sequences=True, return_state=True)\n",
        "\n",
        "  def get_dense_layer(self, num_decoder_token, activation = \"softmax\"):\n",
        "    return Dense(num_decoder_tokens, activation= activation)\n",
        "\n",
        "  def train_step(self):\n",
        "    self.embedding_layer = self.get_embedding_layer( self.num_encoder_tokens, self.embedding_dim ,name = \"encoder_embedding\")\n",
        "    self.embedding_results = self.embedding_layer(self.encoder_inputs)\n",
        "    print(self.embedding_results.shape)\n",
        "    self.encoder = self.get_encoder( self.embedding_dim,self.type_encoder_unit, self.num_of_encoder_layer , name =\"encoder\" )\n",
        "    encoder_results = self.encoder(self.embedding_results)\n",
        "\n",
        "    self.encoder_outputs, self.encoder_states = encoder_results[0], encoder_results[1:]\n",
        "\n",
        "    self.embedding_layer2 = self.get_embedding_layer( self.num_decoder_tokens, self.embedding_dim, name = \"decoder_embedding\")\n",
        "    self.embedding_results2 = self.embedding_layer2(self.decoder_inputs,)\n",
        "\n",
        "    self.decoder = self.get_decoder( self.embedding_dim, self.type_decoder_unit, self.num_of_decoder_layer,)\n",
        "    self.decoder_results = self.decoder(self.embedding_results2, initial_state=self.encoder_states)\n",
        "\n",
        "    self.decoder_output = self.decoder_results[0]\n",
        "    self.decoder_dense = self.get_dense_layer(self.num_decoder_tokens)\n",
        "    self.dense_output = self.decoder_dense(self.decoder_output)\n",
        "\n",
        "  def build_model(self):\n",
        "    \n",
        "    self.model = keras.Model([self.encoder_inputs, self.decoder_inputs], self.dense_output, name = \"Seq2Seq_model\")\n",
        "    return self.model\n",
        "\n"
      ],
      "metadata": {
        "id": "x9fpzXhN7DMU",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:57:09.043044Z",
          "iopub.execute_input": "2022-06-19T14:57:09.043462Z",
          "iopub.status.idle": "2022-06-19T14:57:09.068515Z",
          "shell.execute_reply.started": "2022-06-19T14:57:09.043428Z",
          "shell.execute_reply": "2022-06-19T14:57:09.067563Z"
        },
        "trusted": true
      },
      "execution_count": 412,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, 1024,1,\"rnn\", 0.0, 0.0).build_model()\n",
        "seq2seq.summary()"
      ],
      "metadata": {
        "id": "qJ07iEXsJr6K",
        "outputId": "f82e8d28-64d7-4a30-9437-d3dbaba684b5",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:59:22.231672Z",
          "iopub.execute_input": "2022-06-19T14:59:22.232141Z",
          "iopub.status.idle": "2022-06-19T14:59:22.718772Z",
          "shell.execute_reply.started": "2022-06-19T14:59:22.232106Z",
          "shell.execute_reply": "2022-06-19T14:59:22.717672Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 1024)\n",
            "Model: \"Seq2Seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input_layer_1 (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Input_layer_2 (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 1024)   26624       ['Input_layer_1[0][0]']          \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 1024)   66560       ['Input_layer_2[0][0]']          \n",
            "                                                                                                  \n",
            " encoder (RNN)                  [(None, None, 1024)  2098176     ['encoder_embedding[0][0]']      \n",
            "                                , (None, 1024)]                                                   \n",
            "                                                                                                  \n",
            " rnn (RNN)                      [(None, None, 1024)  2098176     ['decoder_embedding[0][0]',      \n",
            "                                , (None, 1024)]                   'encoder[0][1]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 65)     66625       ['rnn[0][0]']                    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,356,161\n",
            "Trainable params: 4,356,161\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n"
      ],
      "metadata": {
        "id": "HXZ2_xdsAQee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T14:21:19.177217Z",
          "iopub.execute_input": "2022-06-19T14:21:19.177828Z",
          "iopub.status.idle": "2022-06-19T14:21:19.197353Z",
          "shell.execute_reply.started": "2022-06-19T14:21:19.177762Z",
          "shell.execute_reply": "2022-06-19T14:21:19.196282Z"
        },
        "trusted": true,
        "id": "Haec2CJvrP32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearch(keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self, beam_size):\n",
        "    self.beam_size = beam_size\n",
        "\n",
        "  def beam_search_decoder(aelf, data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    prediction = self.model.predict([val_enc_input_data , val_dec_input_data])\n",
        "    print(prediction.shape)\n",
        "    for i, pred in enumerate(prediction):\n",
        "      beam_search_prediction = self.beam_search_decoder(pred, self.beam_size)\n",
        "      correct_prediction = 0\n",
        "      for k in range(self.beam_size):\n",
        "        #translated_word = \"\\t\"+\"\".join([reverse_target_token[x] for x in beam_search_prediction[k][0][:len(validation_target[i])-1]])\n",
        "        #print(translated_word, validation_target[i])\n",
        "        #print(validation_target[i])\n",
        "        \n",
        "        def idx2char(idx_list):\n",
        "          return \"\".join([reverse_target_token[x] for x in idx_list])\n",
        "\n",
        "        if \"\\t\"+ idx2char(beam_search_prediction[k][0][:len(validation_target[i])-1]) == validation_target[i]:\n",
        "          correct_prediction+=1\n",
        "          break\n",
        "    mul = 10.0**2\n",
        "    logs[\"character_accuracy\"] = ((correct_prediction/prediction.shape[0])*mul)/mul\n",
        "    print(\"- character_accuracy\",logs[\"character_accuracy\"])\n",
        "    #print(f\"Accuracy by Beam Search {correct_prediction/len(validation_target)}\")\n",
        "      # print(len(beam_search_prediction))\n",
        "      # print(beam_search_prediction)\n"
      ],
      "metadata": {
        "id": "KPyDujbe-Brq",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:10:18.730086Z",
          "iopub.execute_input": "2022-06-19T15:10:18.730979Z",
          "iopub.status.idle": "2022-06-19T15:10:18.745337Z",
          "shell.execute_reply.started": "2022-06-19T15:10:18.730935Z",
          "shell.execute_reply": "2022-06-19T15:10:18.744072Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decoder(data, k):\n",
        "    decodedWords = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for word in data:\n",
        "      candidates = list()\n",
        "      # expand each current candidate\n",
        "      for sequence in decodedWords:\n",
        "        seq, score = sequence\n",
        "        for j in range(len(word)):\n",
        "          candidate = [seq + [j], score - log(word[j])]\n",
        "          candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(candidates, key=lambda a:a[1])\n",
        "      # select k best\n",
        "      decodedWords = ordered[:k]\n",
        "    return decodedWords\n",
        "  \n",
        "def translate(seq):\n",
        "  sentence = [] \n",
        "  for x in seq:\n",
        "    char = reverse_target_token[x]\n",
        "    sentence.append(char)\n",
        "  return \"\".join(sentence)\n",
        "class WordAccuracyCallback(keras.callbacks.Callback):\n",
        "  def __init__(self,beam_size):\n",
        "    self.beam_size=beam_size\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    pred=self.model.predict([val_enc_input_data , val_dec_input_data])\n",
        "    count=0\n",
        "    for i in range(pred.shape[0]):\n",
        "      pSequences=beam_search_decoder(pred[i],self.beam_size)\n",
        "      for j in range(self.beam_size):\n",
        "        if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n",
        "          count=count+1\n",
        "          break\n",
        "    factor = 10.0 ** 4\n",
        "    logs[\"WordAccuracy\"]=math.trunc((count/pred.shape[0])*factor)/factor\n",
        "    print(\"- wordAccuracy:\",logs[\"WordAccuracy\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:20.414711Z",
          "iopub.execute_input": "2022-06-19T15:22:20.415244Z",
          "iopub.status.idle": "2022-06-19T15:22:20.431301Z",
          "shell.execute_reply.started": "2022-06-19T15:22:20.415207Z",
          "shell.execute_reply": "2022-06-19T15:22:20.429988Z"
        },
        "trusted": true,
        "id": "FE8vtOJOrP33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    'method':'bayes',\n",
        "    'metric': {\n",
        "        'name':'val_accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters':{\n",
        "    \n",
        "    \"num_of_layer\" : {'values': [1,2,3]},\n",
        "    \"unit_size\": {\"values\":[16,32,64]},\n",
        "    \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n",
        "    \"dropout\": {\"values\": [0.0, 0.2, 0.4]},\n",
        "    'recurrent_dropout':{'values':[0.0,0.3]},\n",
        "    \"beam_size\" : {\"values\":[1,2,3,4]},\n",
        "    \"epochs\":{\"value\":20},  \n",
        "    \"optimizer\":{\"values\": [\"adam\",\"rmsprop\"]}             \n",
        "                   }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "id": "mIW2Ofow5Deo",
        "outputId": "0c99b3cb-eaac-4b1e-acfa-1aff5a6bad9e",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:21:23.223786Z",
          "iopub.execute_input": "2022-06-19T14:21:23.224923Z",
          "iopub.status.idle": "2022-06-19T14:21:23.234457Z",
          "shell.execute_reply.started": "2022-06-19T14:21:23.224875Z",
          "shell.execute_reply": "2022-06-19T14:21:23.233323Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'method': 'bayes',\n 'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n 'parameters': {'beam_size': {'values': [1, 2, 3, 4]},\n                'dropout': {'values': [0.0, 0.2, 0.4]},\n                'epochs': {'value': 20},\n                'num_of_layer': {'values': [1, 2, 3]},\n                'optimizer': {'values': ['adam', 'rmsprop']},\n                'recurrent_dropout': {'values': [0.0, 0.3]},\n                'unit_size': {'values': [16, 32, 64]},\n                'unit_type': {'values': ['lstm', 'rnn', 'gru']}}}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"seq2seq\")"
      ],
      "metadata": {
        "id": "x8YmtLZN_74p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config = None):\n",
        "  with wandb.init(config=config):\n",
        "    config = wandb.config\n",
        "    #print(config)\n",
        "    seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, config.unit_size, config.num_of_layer,config.unit_type , config.dropout,config.recurrent_dropout).build_model()\n",
        "    seq2seq.compile(optimizer=config.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\",])\n",
        "    seq2seq.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=config.epochs,\n",
        "        validation_data =  ([validation_encoder_input_data , validation_decoder_input_data] ,validation_decoder_target_data),\n",
        "        callbacks = [BeamSearch(config.beam_size), WandbCallback()],verbose = 1, \n",
        "        )\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "qYv2feSRAzW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")\n",
        "seq2seq.metrics_names\n",
        "\n"
      ],
      "metadata": {
        "id": "DIrCXZTAGyTL",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:24.717670Z",
          "iopub.execute_input": "2022-06-19T15:22:24.718289Z",
          "iopub.status.idle": "2022-06-19T15:22:24.738490Z",
          "shell.execute_reply.started": "2022-06-19T15:22:24.718252Z",
          "shell.execute_reply": "2022-06-19T15:22:24.737389Z"
        },
        "trusted": true,
        "outputId": "0b540428-89cc-4ac6-ed17-8fc3bffafd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 290,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=seq2seq.predict([val_enc_input_data , val_dec_input_data])\n",
        "count=0\n",
        "for i in range(pred.shape[0]//400):\n",
        "      pSequences=beam_search_decoder(pred[i],3)\n",
        "      for j in range(3):\n",
        "        print({\"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])}, \"original =\", {validation_target[i]} )\n",
        "        if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n",
        "          count=count+1\n",
        "          print(\"yes\")\n",
        "          break\n",
        "factor = 10.0 ** 4\n"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-06-19T15:41:09.089308Z",
          "iopub.execute_input": "2022-06-19T15:41:09.089822Z",
          "iopub.status.idle": "2022-06-19T15:41:22.154791Z",
          "shell.execute_reply.started": "2022-06-19T15:41:09.089769Z",
          "shell.execute_reply": "2022-06-19T15:41:22.153760Z"
        },
        "trusted": true,
        "id": "xLTgqB68rP35",
        "outputId": "416e74fe-2665-4117-c579-56eb7c7beade"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'\\tकं\\nा'} original = {'\\tअंक\\n'}\n{'\\tवं\\nा'} original = {'\\tअंक\\n'}\n{'\\t्ं\\nा'} original = {'\\tअंक\\n'}\n{'\\tकं\\nा'} original = {'\\tअंक\\n'}\n{'\\tवं\\nा'} original = {'\\tअंक\\n'}\n{'\\t्ं\\nा'} original = {'\\tअंक\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tकन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tवन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\t्न\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tकन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tवन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\t्न\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tकर\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n{'\\tवर\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n{'\\t्र\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = seq2seq.predict([val_enc_input_data , val_dec_input_data])\n",
        "x.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:33:14.072698Z",
          "iopub.execute_input": "2022-06-19T15:33:14.073169Z",
          "iopub.status.idle": "2022-06-19T15:33:26.484545Z",
          "shell.execute_reply.started": "2022-06-19T15:33:14.073134Z",
          "shell.execute_reply": "2022-06-19T15:33:26.483378Z"
        },
        "trusted": true,
        "id": "qnzLTsbxrP36",
        "outputId": "357821dd-b5f9-4924-add4-0ebd058f94a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 301,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(4502, 17, 65)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "histotry = seq2seq.fit(\n",
        "    [enc_input_data, dec_input_data],\n",
        "    dec_target_data,\n",
        "    batch_size=8192,\n",
        "    epochs=1,\n",
        "    callbacks = [WordAccuracyCallback(3), ],\n",
        ")\n",
        "# Save model\n",
        "seq2seq.save(\"s2s\")\n"
      ],
      "metadata": {
        "id": "ox_fyYUrAQef",
        "outputId": "f4da868c-355c-4132-8481-e4ffcb605de4",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:25.394757Z",
          "iopub.execute_input": "2022-06-19T15:22:25.395410Z",
          "iopub.status.idle": "2022-06-19T15:28:55.450107Z",
          "shell.execute_reply.started": "2022-06-19T15:22:25.395375Z",
          "shell.execute_reply": "2022-06-19T15:28:55.448703Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "6/6 [==============================] - 214s 34s/step - loss: 1.1271 - acc: 0.1550\n- wordAccuracy: 0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for key in histotry.history.keys():\n",
        "#       print(key , histotry.history[key])\n",
        "#       #wandb.log({key : histotry.history[key]})"
      ],
      "metadata": {
        "id": "2BnI7lHtQtnT",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:17:37.411280Z",
          "iopub.status.idle": "2022-06-19T14:17:37.411991Z",
          "shell.execute_reply.started": "2022-06-19T14:17:37.411629Z",
          "shell.execute_reply": "2022-06-19T14:17:37.411665Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq.metrics_names"
      ],
      "metadata": {
        "id": "Npqd4if4HVZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ],
      "metadata": {
        "id": "BC5CbwHlAQef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define sampling models\n",
        "# # Restore the model and construct the encoder and decoder.\n",
        "# model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "# encoder_inputs = model.input[0]  # input_1\n",
        "# temp = model.layers[2].output\n",
        "# encoder_outputs, state = temp[0], temp[1:]  # lstm_1\n",
        "# encoder_states = state\n",
        "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# decoder_inputs = model.input[1]  # input_2\n",
        "# decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "# decoder_states_inputs = state\n",
        "# decoder_lstm = model.layers[3]\n",
        "# temp = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "# decoder_outputs, state_dec = temp[0], temp[1:]\n",
        "# decoder_states = state_dec\n",
        "# decoder_dense = model.layers[4]\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# decoder_model = keras.Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# )\n",
        "\n",
        "# # Reverse-lookup token index to decode sequences back to\n",
        "# # something readable.\n",
        "# # reverse_input_char_index = dict((i, char) for char, i in num_encoder_tokens.items())\n",
        "# # reverse_target_char_index = dict((i, char) for char, i in num_decoder_tokens.items())\n",
        "# # print(reverse_input_char_index)\n",
        "# # print(input_token_index)\n",
        "\n",
        "# reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "# reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "# def decode_sequence(input_seq):\n",
        "#     # Encode the input as state vectors.\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "#     # Generate empty target sequence of length 1.\n",
        "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#     # Populate the first character of target sequence with the start character.\n",
        "#     target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "#     # Sampling loop for a batch of sequences\n",
        "#     # (to simplify, here we assume a batch of size 1).\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = \"\"\n",
        "#     while not stop_condition:\n",
        "#         temp = decoder_model.predict([target_seq] + states_value)\n",
        "#         output_tokens, state = temp[0],temp[1:]\n",
        "\n",
        "#         # Sample a token\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#         #print(reverse_target_char_index)\n",
        "#         sampled_char = reverse_target_token[sampled_token_index]\n",
        "#         decoded_sentence += sampled_char\n",
        "\n",
        "#         # Exit condition: either hit max length\n",
        "#         # or find stop character.\n",
        "#         if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "#             stop_condition = True\n",
        "\n",
        "#         # Update the target sequence (of length 1).\n",
        "#         target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#         target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "#         # Update states\n",
        "#         states_value = state\n",
        "#     return decoded_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "meuadqgkAQeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now generate decoded sentences as such:\n"
      ],
      "metadata": {
        "id": "nSNoQ5AvAQeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for seq_index in range(20):\n",
        "#     # Take one sequence (part of the training set)\n",
        "#     # for trying out decoding.\n",
        "#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "#     decoded_sentence = decode_sequence(input_seq)\n",
        "#     print(\"-\")\n",
        "#     print(\"Input sentence:\", input_texts[seq_index])\n",
        "#     print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "metadata": {
        "id": "wjp__2oJAQeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git log"
      ],
      "metadata": {
        "id": "ygwmClalB8vI",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:44:04.633143Z",
          "iopub.execute_input": "2022-06-19T15:44:04.633593Z",
          "iopub.status.idle": "2022-06-19T15:44:05.689112Z",
          "shell.execute_reply.started": "2022-06-19T15:44:04.633552Z",
          "shell.execute_reply": "2022-06-19T15:44:05.687915Z"
        },
        "trusted": true,
        "outputId": "09f6fff2-5b5d-42a4-e7b8-2e20df103971"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lzaKcIYKrP39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oX52e6mtrP39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}