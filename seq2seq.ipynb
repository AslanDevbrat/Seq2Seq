{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n"
      ],
      "metadata": {
        "id": "XtD33RcAAQeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb --upgrade\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "UBUz4pd87YeP",
        "execution": {
          "iopub.status.busy": "2022-06-19T11:46:04.715027Z",
          "iopub.execute_input": "2022-06-19T11:46:04.715895Z",
          "iopub.status.idle": "2022-06-19T11:46:36.340025Z",
          "shell.execute_reply.started": "2022-06-19T11:46:04.715742Z",
          "shell.execute_reply": "2022-06-19T11:46:36.338600Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNNCell, GRUCell, Dense, LSTMCell\n",
        "from tensorflow.keras import Input\n",
        "import pandas as pd\n",
        "from numpy import argmax\n",
        "from math import log\n",
        "import pprint\n",
        "import math\n",
        "import wandb\n",
        "import os\n",
        "import io\n",
        "from wandb.keras import WandbCallback\n",
        "import time\n",
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# wandb_api = user_secrets.get_secret(\"wandb_api\")\n",
        "\n",
        "# #wandb.login(key=wandb_api)\n",
        "# ! wandb login $wandb_api\n",
        "\n",
        "# os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "EnvOeSibAQeV",
        "outputId": "9a2390d6-dc09-4e65-e3ce-50cabcb8eb7f",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:55:00.466081Z",
          "iopub.execute_input": "2022-06-19T14:55:00.466557Z",
          "iopub.status.idle": "2022-06-19T14:55:04.846701Z",
          "shell.execute_reply.started": "2022-06-19T14:55:00.466520Z",
          "shell.execute_reply": "2022-06-19T14:55:04.844596Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maslan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data\n"
      ],
      "metadata": {
        "id": "NbOpYTqYAQeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n"
      ],
      "metadata": {
        "id": "1rKnaKIZAQeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf 'dakshina_dataset_v1.0.tar'\n",
        "train_file_path = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "val_file_path= \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n",
        "test_file_path  = \"/content/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\""
      ],
      "metadata": {
        "id": "roFBlOuMAYkx",
        "outputId": "30552fad-bc94-4a74-c1ad-e9b31d170ac3",
        "execution": {
          "iopub.status.busy": "2022-06-19T11:47:33.470862Z",
          "iopub.execute_input": "2022-06-19T11:47:33.471940Z",
          "iopub.status.idle": "2022-06-19T11:47:49.677198Z",
          "shell.execute_reply.started": "2022-06-19T11:47:33.471901Z",
          "shell.execute_reply": "2022-06-19T11:47:49.676010Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-20 17:43:06--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.211.128, 173.194.212.128, 173.194.214.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.211.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G  89.6MB/s    in 17s     \n",
            "\n",
            "2022-06-20 17:43:23 (111 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 100000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path = train_file_path\n"
      ],
      "metadata": {
        "id": "akMCxfVHAQeZ",
        "execution": {
          "iopub.status.busy": "2022-06-19T09:33:02.211259Z",
          "iopub.execute_input": "2022-06-19T09:33:02.211737Z",
          "iopub.status.idle": "2022-06-19T09:33:02.219013Z",
          "shell.execute_reply.started": "2022-06-19T09:33:02.211682Z",
          "shell.execute_reply": "2022-06-19T09:33:02.217846Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n"
      ],
      "metadata": {
        "id": "U-3Djb36AQea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processData(filename,input_chars=set(),target_chars=set()):\n",
        "  input=[]\n",
        "  target=[]\n",
        "  with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[:len(lines) -1]:\n",
        "      t_text,i_text, attestation = line.split(\"\\t\")\n",
        "       # We use \"\\t\" as the \"start sequence\" character and \"\\n\" as \"end sequence\" character for the target text.\n",
        "      input.append(i_text)\n",
        "      target.append(\"\\t\"+t_text+\"\\n\")\n",
        "      for char in i_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "      for char in t_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "  target_chars.add(\"\\t\")\n",
        "  target_chars.add(\"\\n\")\n",
        "\n",
        "  input_chars = sorted(list(input_chars))\n",
        "  target_chars = sorted(list(target_chars))\n",
        "  num_encoder_tokens = len(input_chars)\n",
        "  num_decoder_tokens = len(target_chars)\n",
        "  max_encoder_seq_length = max([len(txt) for txt in input])\n",
        "  max_decoder_seq_length = max([len(txt) for txt in target])\n",
        "  return input,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length     "
      ],
      "metadata": {
        "id": "5CZtxlJmaYmb",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:21:54.385136Z",
          "iopub.execute_input": "2022-06-19T15:21:54.385606Z",
          "iopub.status.idle": "2022-06-19T15:21:54.397778Z",
          "shell.execute_reply.started": "2022-06-19T15:21:54.385570Z",
          "shell.execute_reply": "2022-06-19T15:21:54.396551Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "input,target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length=processData(train_file_path)\n",
        "print(\"Number of samples:\", len(input))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "pg8AcuMoab8U",
        "outputId": "62f38cb8-caea-4d05-bf80-7467fa20a38d",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:21:56.055100Z",
          "iopub.execute_input": "2022-06-19T15:21:56.055975Z",
          "iopub.status.idle": "2022-06-19T15:21:56.181503Z",
          "shell.execute_reply.started": "2022-06-19T15:21:56.055922Z",
          "shell.execute_reply": "2022-06-19T15:21:56.179927Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 44204\n",
            "Number of unique input tokens: 26\n",
            "Number of unique output tokens: 65\n",
            "Max sequence length for inputs: 20\n",
            "Max sequence length for outputs: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "# Vectorize the data.\n",
        "validation_input,validation_target,input_chars,target_chars,num_encoder_tokens,num_decoder_tokens, validation_max_encoder_seq_length, validation_max_decoder_seq_length=processData(val_file_path,set(input_chars),set(target_chars))\n",
        "\n",
        "print(\"Number of validation samples:\", len(validation_input))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"validation Max sequence length for inputs:\", validation_max_encoder_seq_length)\n",
        "print(\"validation Max sequence length for outputs:\", validation_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "lmCnQKWNbsuh",
        "outputId": "7a6d2817-a008-4c29-c182-2931e9489e01",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:32:00.457765Z",
          "iopub.execute_input": "2022-06-19T15:32:00.458307Z",
          "iopub.status.idle": "2022-06-19T15:32:00.484257Z",
          "shell.execute_reply.started": "2022-06-19T15:32:00.458272Z",
          "shell.execute_reply": "2022-06-19T15:32:00.482872Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation samples: 4502\n",
            "Number of unique input tokens: 26\n",
            "Number of unique output tokens: 65\n",
            "validation Max sequence length for inputs: 16\n",
            "validation Max sequence length for outputs: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data.\n",
        "test_input,test_target,test_input_chars,test_target_chars,test_num_encoder_tokens,test_num_decoder_tokens, test_max_encoder_seq_length, test_max_decoder_seq_length=processData(test_file_path)\n",
        "print(\"Number of validation samples:\", len(test_input))\n",
        "print(\"Test Max sequence length for inputs:\", test_max_encoder_seq_length)\n",
        "print(\"Test Max sequence length for outputs:\", test_max_decoder_seq_length)"
      ],
      "metadata": {
        "id": "Q9tUFlovbz5i",
        "outputId": "1cf26bf3-ae60-4a5b-ecaa-837749b34cb6",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:05:18.181757Z",
          "iopub.execute_input": "2022-06-19T15:05:18.182569Z",
          "iopub.status.idle": "2022-06-19T15:05:18.200082Z",
          "shell.execute_reply.started": "2022-06-19T15:05:18.182519Z",
          "shell.execute_reply": "2022-06-19T15:05:18.199204Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of validation samples: 4358\n",
            "Test Max sequence length for inputs: 18\n",
            "Test Max sequence length for outputs: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_encoder_input_data=np.zeros(\n",
        "    (len(validation_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "test_encoder_input_data=np.zeros(\n",
        "    (len(test_input), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_decoder_input_data =np.zeros(\n",
        "    (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "validation_decoder_target_data = np.zeros(\n",
        "    (len(validation_input), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input, target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token[char]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "# for validation data\n",
        "for i, (validation_input_text, validation_target_text) in enumerate(zip(validation_input, validation_target)):\n",
        "    for t, char in enumerate(validation_input_text):\n",
        "        validation_encoder_input_data[i, t, input_token[char]] = 1.0\n",
        "    for t, char in enumerate(validation_target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        validation_decoder_input_data[i, t, target_token[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            validation_decoder_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "\n",
        "# for test data\n",
        "for i, (test_input_text, test_target_text) in enumerate(zip(test_input, test_target)):\n",
        "    for t, char in enumerate(test_input_text):\n",
        "        test_encoder_input_data[i, t, input_token[char]] = 1.0"
      ],
      "metadata": {
        "id": "oBF7Cdqrb5Hc",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:30:35.223556Z",
          "iopub.execute_input": "2022-06-19T14:30:35.224039Z",
          "iopub.status.idle": "2022-06-19T14:30:36.884489Z",
          "shell.execute_reply.started": "2022-06-19T14:30:35.224003Z",
          "shell.execute_reply": "2022-06-19T14:30:36.883493Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2q7AdDj4P_8p",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:30:37.301677Z",
          "iopub.execute_input": "2022-06-19T14:30:37.302743Z",
          "iopub.status.idle": "2022-06-19T14:30:38.242257Z",
          "shell.execute_reply.started": "2022-06-19T14:30:37.302700Z",
          "shell.execute_reply": "2022-06-19T14:30:38.241015Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_token = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "\n",
        "reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "\n",
        "enc_input_data = np.zeros(\n",
        "    (len(input), max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_input_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_target_data = np.zeros(\n",
        "    (len(input), max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "#Decoder Target Sequences are Padded to a maximum length of max_decoder SeqLen characters with a vocabulary of sizeofTeluguVocab different characters. \n",
        "for i, (input_text, target_text) in enumerate(zip(input, target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        enc_input_data[i, t] = input_token[char]\n",
        "    #enc_input_data[i, t + 1 :] = input_token[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # dec_target_data is ahead of dec_input_data by one timestep\n",
        "        dec_input_data[i, t] = target_token[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will not include the start character.\n",
        "            dec_target_data[i, t - 1] = target_token[char]\n",
        "    #dec_input_data[i, t + 1: ] = target_token[\" \"]\n",
        "    #dec_target_data[i, t:, target_token[\" \"]] = 1.0\n",
        "    \n",
        "val_enc_input_data = np.zeros(\n",
        "    (len(validation_input), validation_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_input_data = np.zeros(\n",
        "    (len(validation_input), validation_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_target_data = np.zeros(\n",
        "    (len(validation_input), validation_max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(validation_input,validation_target)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        # Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object. \n",
        "        # This enumerated object can then be used directly for loops or converted into a list of tuples using the list() method.\n",
        "        val_enc_input_data[i, t] = input_token[char]\n",
        "    #val_enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        val_dec_input_data[i, t] = target_token[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_dec_target_data[i, t - 1, target_token[char]] = 1.0\n",
        "    #val_dec_input_data[i, t + 1: ] = target_token_index[\" \"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:33:00.750838Z",
          "iopub.execute_input": "2022-06-19T15:33:00.751958Z",
          "iopub.status.idle": "2022-06-19T15:33:01.589162Z",
          "shell.execute_reply.started": "2022-06-19T15:33:00.751901Z",
          "shell.execute_reply": "2022-06-19T15:33:01.588102Z"
        },
        "trusted": true,
        "id": "6_L_o5BGrP3z"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTDataset:\n",
        "    def __init__(self, problem_type='en-hi'):\n",
        "        self.problem_type = 'en-'\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "    \n",
        "\n",
        "    def unicode_to_ascii(self, s):\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    ## Step 1 and Step 2 \n",
        "    def preprocess_sentence(self, w):\n",
        "        # w = self.unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "        # # creating a space between a word and the punctuation following it\n",
        "        # # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "        # # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        # w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "        # w = w.strip()\n",
        "\n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        #print(w)\n",
        "        w = '\\t' + w + '\\n'\n",
        "        \n",
        "        return w\n",
        "    \n",
        "    def create_dataset(self, path, num_examples):\n",
        "        # path : path to spa-eng.txt file\n",
        "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
        "        #lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "        #word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "        data =  pd.read_csv(path,delimiter=\"\\t\", header= None, nrows = num_examples )\n",
        "        data = data.dropna()\n",
        "        print(data.info())\n",
        "        return data[0].apply(self.preprocess_sentence).values.astype(str), data[1].apply(self.preprocess_sentence).values.astype(str)\n",
        "\n",
        "    # Step 3 and Step 4\n",
        "    def tokenize(self, lang):\n",
        "        # lang = list of sentences in a language\n",
        "        \n",
        "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True)\n",
        "        lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "        ## and pads the sequences to match the longest sequences in the given input\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        return tensor, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self, path, num_examples=None):\n",
        "        # creating cleaned input, output pairs\n",
        "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
        "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
        "        #file_path = download_dakshina()\n",
        "        input_tensor_train, target_tensor_train, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(train_file_path, num_examples)\n",
        "        input_tensor_val, target_tensor_val, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(val_file_path, num_examples)\n",
        "        input_tensor_test, target_tensor_test, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(test_file_path, num_examples)\n",
        "        x = input_tensor_train\n",
        "        y  =target_tensor_train\n",
        "        #input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.4)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train,target_tensor_train))\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, target_tensor_test))\n",
        "        test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "        return train_dataset, val_dataset, test_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer\n"
      ],
      "metadata": {
        "id": "EOIwk0-PrP3z"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64\n",
        "# Let's limit the #training examples for faster training\n",
        "num_examples = 300000\n",
        "\n",
        "dataset_creator = NMTDataset('en-spa')\n",
        "train_dataset, val_dataset,test_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Wn4CM1zgLH",
        "outputId": "977b8f65-a1c2-4b15-e040-29a04fd3a88f"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 44202 entries, 0 to 44203\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       44202 non-null  object\n",
            " 1   1       44202 non-null  object\n",
            " 2   2       44202 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4502 entries, 0 to 4501\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       4502 non-null   object\n",
            " 1   1       4502 non-null   object\n",
            " 2   2       4502 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 140.7+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 4358 entries, 0 to 4357\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   0       4358 non-null   object\n",
            " 1   1       4358 non-null   object\n",
            " 2   2       4358 non-null   int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 136.2+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataset))[1][0]"
      ],
      "metadata": {
        "id": "9Rox45Q1hWgm",
        "outputId": "05609fc2-54dc-4c20-e204-01bda96fe41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(21,), dtype=int32, numpy=\n",
              "array([ 1,  7, 15, 28,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gaZrS4g1T5X",
        "outputId": "97ad97b4-743b-44fc-81d2-ccd5ff884d00"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 22]), TensorShape([64, 21]))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n"
      ],
      "metadata": {
        "id": "eGM50wwWAQed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, num_of_layers, enc_unit_type, batch_sz, recurrent_dropout, dropout):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.enc_unit_type = enc_unit_type\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.dropout = dropout\n",
        "    self.embedding = Embedding( vocab_size, embedding_dim)\n",
        "\n",
        "    self.encoder_layer = self.get_encoder_layer(self.enc_units,\n",
        "                                                self.num_of_layers, self.enc_unit_type)\n",
        "    \n",
        "\n",
        "  def get_encoder_layer(self, enc_units, num_of_layers, enc_unit_type):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(enc_unit_type, \n",
        "                                                                                 enc_units) for i in range(num_of_layers)],),\n",
        "                                  return_sequences=True, return_state=True, name = \"Encoder\")\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "      #print(cell_type)\n",
        "      if cell_type == \"lstm\":\n",
        "        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "      elif cell_type == \"rnn\":\n",
        "        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      elif cell_type ==\"gru\":\n",
        "        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      else:\n",
        "        print(f\"Invalid cell type: {cell_type}\")\n",
        "\n",
        "    \n",
        "  def call(self, x):\n",
        "      x = self.embedding(x)\n",
        "      output = self.encoder_layer(x,)\n",
        "\n",
        "      #print(output)\n",
        "      return output\n",
        "    \n",
        "  def initialize_hidden_state(self):\n",
        "      print(\"Called\")\n",
        "      return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "metadata": {
        "id": "-_IMrNwXsYR4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_input_data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iT2EuNf1TDo",
        "outputId": "7a08f4c6-f141-4984-9b5f-544a2c3aceec"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\n",
        "encoder = Encoder( num_encoder_tokens, 1, 16, 3, \"lstm\", 64, 0.0, 0.0)\n",
        "#sample_hidden = encoder.initialize_hidden_state()\n",
        "# encoder.build(input_shape =(None,26))\n",
        "# encoder.summary()\n",
        "sample_output = encoder(enc_input_data[:64])\n",
        "out , state = sample_output[0], sample_output[1:]"
      ],
      "metadata": {
        "id": "oG6E1qaLv92s"
      },
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, state"
      ],
      "metadata": {
        "id": "yHmvx-1zRHJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, num_of_layers, \n",
        "               dec_unit_type, batch_sz, recurrent_dropout, dropout, \n",
        "               attention_type = \"luong\"):\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.dec_unit_type = dec_unit_type\n",
        "    self.num_of_layers = num_of_layers\n",
        "    self.attention_type = attention_type\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.dropout = dropout\n",
        "    print(\"decoder embedding dim\", embedding_dim)\n",
        "    self.embedding = Embedding( vocab_size, embedding_dim)\n",
        "\n",
        "    self.fc  = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.decoder_cells = self.get_stacked_rnn_cell()\n",
        "\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, None\n",
        "                                                              , self.batch_sz*[max_encoder_seq_length], \n",
        "                                                              self.attention_type)\n",
        "\n",
        "    self.cell = self.build_cell()\n",
        "\n",
        "    #print(self.cell)\n",
        "\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.cell, sampler = self.sampler, output_layer = self.fc)\n",
        "\n",
        "\n",
        "\n",
        "  def build_cell(self):\n",
        "    cell = tfa.seq2seq.AttentionWrapper(self.decoder_cells, self.attention_mechanism,\n",
        "                                        attention_layer_size = self.dec_units)\n",
        "    return cell\n",
        "  \n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "      #print(cell_type)\n",
        "      if cell_type == \"lstm\":\n",
        "        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "      elif cell_type == \"rnn\":\n",
        "        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      elif cell_type ==\"gru\":\n",
        "        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "      else:\n",
        "        print(f\"Invalid cell type: {cell_type}\")\n",
        "\n",
        "  def get_stacked_rnn_cell(self,):\n",
        "    return tf.keras.layers.StackedRNNCells( [self.get_cell(self.dec_unit_type, self.dec_units,) for i in range(self.num_of_layers)])\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    #print(batch_sz)\n",
        "    #print(len(encoder_state))\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "  def call(self, x, initial_state):\n",
        "    x = self.embedding(x)\n",
        "    print(\"calles\")\n",
        "    output = self.decoder(x, initial_state=initial_state)\n",
        "    return output"
      ],
      "metadata": {
        "id": "vPiwKS7P7A3X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#encoder = Encoder( num_encoder_tokens, 1024, 1024, 3, \"lstm\", batch_size, 0.0, 0.0).build(input_shape =(None,22))\n",
        "decoder = Decoder( num_decoder_tokens,  1, 16, 3, \"lstm\", 64, 0.0, 0.0)\n",
        "#sample_hidden = encoder.initialize_hidden_state()\n",
        "#decoder.build(input_shape =(None, ))\n",
        "# decoder.summary()\n",
        "#sample_x = tf.random.uniform((2  ,max_decoder_seq_length))\n",
        "decoder.attention_mechanism.setup_memory(out)\n",
        "initial_state = decoder.build_initial_state(64, tuple(state), tf.float32)\n",
        "# sample_output = decoder(dec_input_data[:8192], initial_state)\n",
        "# out1 , state1 = sample_output[0], sample_output[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe8whQdYCdfZ",
        "outputId": "62bd1aa0-6726-41c8-a941-25bf2702c4fa"
      },
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder embedding dim 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out1"
      ],
      "metadata": {
        "id": "s1cQ1JBzWhth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class seq2seq(tf.keras.Model):\n",
        "  def __init__(self, num_encoder_tokens, num_decoder_token, encoder_embedding_dim, decoder_embedding_dim,num_of_unit, num_of_layers, unit_type, batch_size, recurrent_dropout, dropout):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder = Encoder(  num_encoder_tokens, 1, 16, 3, \"lstm\", self.batch_size, 0.0, 0.0)\n",
        "    #self.encoder.summary()\n",
        "    self.dec = Decoder( num_decoder_tokens,  1, 16, 3, \"lstm\", self.batch_size, 0.0, 0.0)\n",
        "    #sample_x = tf.random.uniform((batch_size  ,max_decoder_seq_length))\n",
        "\n",
        "  def call(self, enc_inp, dec_inp):\n",
        "    print(\"fsdfa\",dec_inp.shape)\n",
        "    x = self.encoder(enc_inp)\n",
        "    enc_out, enc_state = x[0], x[1:]\n",
        "    print(enc_out.shape)\n",
        "    self.dec.attention_mechanism.setup_memory(enc_out)\n",
        "    dec_initial_state = self.dec.build_initial_state(self.batch_size, tuple(enc_state), tf.float32)\n",
        "    print(\"fucck\")\n",
        "    x = self.dec(dec_inp,dec_initial_state)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "cVoQMjKYYLX2"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "s2s = seq2seq(num_encoder_tokens, num_decoder_tokens,  encoder_embedding_dim =256,\n",
        "              decoder_embedding_dim= 1024,\n",
        "              num_of_unit =16,\n",
        "              num_of_layers = 5, \n",
        "              unit_type =\"gru\",\n",
        "             batch_size = batch_size, \n",
        "              recurrent_dropout = 0,\n",
        "              dropout = 0 )\n",
        "sample_out = s2s(enc_input_data[:batch_size], dec_input_data[:batch_size])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRKTJEGid7MN",
        "outputId": "6e4caa8e-34f7-48cc-eb1e-c321f3c2d2f7"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder embedding dim 1\n",
            "fsdfa (64, 21)\n",
            "(64, 20, 16)\n",
            "fucck\n",
            "calles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "loss = cross_entropy(y_true=dec_target_data[:64], y_pred=sample_out[0].rnn_output)\n",
        "loss"
      ],
      "metadata": {
        "id": "Xd7eP7r0gNsA",
        "outputId": "bedc2e89-acdb-4272-8b64-67579ed11384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 21), dtype=float32, numpy=\n",
              "array([[4.1747932, 4.1745915, 4.174487 , ..., 4.174175 , 4.174177 ,\n",
              "        4.17418  ],\n",
              "       [4.174734 , 4.174542 , 4.1742854, ..., 4.1741757, 4.174185 ,\n",
              "        4.1741924],\n",
              "       [4.174767 , 4.1745205, 4.1742883, ..., 4.174209 , 4.174211 ,\n",
              "        4.174213 ],\n",
              "       ...,\n",
              "       [4.1746445, 4.1743155, 4.174321 , ..., 4.1742435, 4.174258 ,\n",
              "        4.1742716],\n",
              "       [4.174657 , 4.174332 , 4.1743355, ..., 4.174272 , 4.174281 ,\n",
              "        4.1742883],\n",
              "       [4.174667 , 4.1743417, 4.174344 , ..., 4.1742654, 4.174274 ,\n",
              "        4.1742816]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function(dec_target_data[:64], sample_out[0].rnn_output)"
      ],
      "metadata": {
        "id": "KUXbZ_2FYhor",
        "outputId": "4ebf78f2-2788-4f01-cfab-aff43b779554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.3852632>"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  #print(pred,\"fucck\", real)\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss  "
      ],
      "metadata": {
        "id": "CD30sG2dRTLD"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=s2s.encoder,\n",
        "                                 decoder=s2s.dec,\n",
        "                                 seq2seq = s2s)"
      ],
      "metadata": {
        "id": "25sIIGGxRVC2"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " dec_input_data[0]"
      ],
      "metadata": {
        "id": "whKn8yCHhJqx",
        "outputId": "45a48eab-78d3-4b9e-def2-e3f5e1d2fac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 5., 3., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(enc_input_data, dec_input_data, targ):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    print(\"from tf.functio\",enc_input_data.shape, dec_input_data.shape, targ.shape)\n",
        "    dec_input_data = dec_input_data[ : , :-1 ]\n",
        "    out = s2s(enc_input_data, dec_input_data)\n",
        "    #enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    #decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    #decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    #pred = decoder(dec_input, decoder_initial_state)\n",
        "    logits = out[0].rnn_output\n",
        "    #print(logits.item())\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = s2s.variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "N7kEoVlWRqvB"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "steps_per_epoch = len(input)//batch_size\n",
        "print(steps_per_epoch)\n",
        "input_data = tf.data.Dataset.from_tensor_slices((enc_input_data, dec_input_data))\n",
        "target_data =  tf.data.Dataset.from_tensor_slices(dec_target_data)\n",
        "train_dataset  = tf.data.Dataset.zip((input_data, target_data)).batch(batch_size)\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  #enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp[0],inp[1] ,targ )\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "id": "0_7v0t5_TsLX",
        "outputId": "49d51d36-1434-4b7b-d05e-6cae881fc339",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "690\n",
            "from tf.functio (64, 20) (64, 21) (64, 21)\n",
            "fsdfa (64, 20)\n",
            "(64, 20, 16)\n",
            "fucck\n",
            "calles\n",
            "from tf.functio (64, 20) (64, 21) (64, 21)\n",
            "fsdfa (64, 20)\n",
            "(64, 20, 16)\n",
            "fucck\n",
            "calles\n",
            "Epoch 1 Batch 0 Loss 1.2458\n",
            "Epoch 1 Batch 100 Loss 0.9433\n",
            "Epoch 1 Batch 200 Loss 0.8726\n",
            "Epoch 1 Batch 300 Loss 0.7946\n",
            "Epoch 1 Batch 400 Loss 0.8270\n",
            "Epoch 1 Batch 500 Loss 1.1725\n",
            "Epoch 1 Batch 600 Loss 0.8587\n",
            "Epoch 1 Loss 1.0207\n",
            "Time taken for 1 epoch 33.06755042076111 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9792\n",
            "Epoch 2 Batch 100 Loss 0.9401\n",
            "Epoch 2 Batch 200 Loss 0.8784\n",
            "Epoch 2 Batch 300 Loss 0.7667\n",
            "Epoch 2 Batch 400 Loss 0.7683\n",
            "Epoch 2 Batch 500 Loss 1.1255\n",
            "Epoch 2 Batch 600 Loss 0.8203\n",
            "Epoch 2 Loss 0.9704\n",
            "Time taken for 1 epoch 25.664228677749634 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.9330\n",
            "Epoch 3 Batch 100 Loss 0.9260\n",
            "Epoch 3 Batch 200 Loss 0.8411\n",
            "Epoch 3 Batch 300 Loss 0.7536\n",
            "Epoch 3 Batch 400 Loss 0.7239\n",
            "Epoch 3 Batch 500 Loss 1.1069\n",
            "Epoch 3 Batch 600 Loss 0.7989\n",
            "Epoch 3 Loss 0.9435\n",
            "Time taken for 1 epoch 24.51833748817444 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9197\n",
            "Epoch 4 Batch 100 Loss 0.9407\n",
            "Epoch 4 Batch 200 Loss 0.8513\n",
            "Epoch 4 Batch 300 Loss 0.7589\n",
            "Epoch 4 Batch 400 Loss 0.6932\n",
            "Epoch 4 Batch 500 Loss 1.0841\n",
            "Epoch 4 Batch 600 Loss 0.7769\n",
            "Epoch 4 Loss 0.9342\n",
            "Time taken for 1 epoch 30.94863510131836 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.9053\n",
            "Epoch 5 Batch 100 Loss 0.9404\n",
            "Epoch 5 Batch 200 Loss 0.8467\n",
            "Epoch 5 Batch 300 Loss 0.7654\n",
            "Epoch 5 Batch 400 Loss 0.6737\n",
            "Epoch 5 Batch 500 Loss 1.0715\n",
            "Epoch 5 Batch 600 Loss 0.7595\n",
            "Epoch 5 Loss 0.9277\n",
            "Time taken for 1 epoch 25.15251111984253 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.8913\n",
            "Epoch 6 Batch 100 Loss 0.9233\n",
            "Epoch 6 Batch 200 Loss 0.8278\n",
            "Epoch 6 Batch 300 Loss 0.7634\n",
            "Epoch 6 Batch 400 Loss 0.6584\n",
            "Epoch 6 Batch 500 Loss 1.0427\n",
            "Epoch 6 Batch 600 Loss 0.7538\n",
            "Epoch 6 Loss 0.9142\n",
            "Time taken for 1 epoch 25.266188621520996 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.9001\n",
            "Epoch 7 Batch 100 Loss 0.8907\n",
            "Epoch 7 Batch 200 Loss 0.8106\n",
            "Epoch 7 Batch 300 Loss 0.7464\n",
            "Epoch 7 Batch 400 Loss 0.6588\n",
            "Epoch 7 Batch 500 Loss 1.0251\n",
            "Epoch 7 Batch 600 Loss 0.7483\n",
            "Epoch 7 Loss 0.9032\n",
            "Time taken for 1 epoch 26.34493064880371 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.8954\n",
            "Epoch 8 Batch 100 Loss 0.8784\n",
            "Epoch 8 Batch 200 Loss 0.8173\n",
            "Epoch 8 Batch 300 Loss 0.7369\n",
            "Epoch 8 Batch 400 Loss 0.6575\n",
            "Epoch 8 Batch 500 Loss 1.0136\n",
            "Epoch 8 Batch 600 Loss 0.7412\n",
            "Epoch 8 Loss 0.8948\n",
            "Time taken for 1 epoch 24.659401893615723 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.8971\n",
            "Epoch 9 Batch 100 Loss 0.8671\n",
            "Epoch 9 Batch 200 Loss 0.8201\n",
            "Epoch 9 Batch 300 Loss 0.7322\n",
            "Epoch 9 Batch 400 Loss 0.6550\n",
            "Epoch 9 Batch 500 Loss 1.0041\n",
            "Epoch 9 Batch 600 Loss 0.7365\n",
            "Epoch 9 Loss 0.8896\n",
            "Time taken for 1 epoch 25.098833799362183 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.8993\n",
            "Epoch 10 Batch 100 Loss 0.8664\n",
            "Epoch 10 Batch 200 Loss 0.8176\n",
            "Epoch 10 Batch 300 Loss 0.7288\n",
            "Epoch 10 Batch 400 Loss 0.6535\n",
            "Epoch 10 Batch 500 Loss 0.9947\n",
            "Epoch 10 Batch 600 Loss 0.7331\n",
            "Epoch 10 Loss 0.8860\n",
            "Time taken for 1 epoch 24.807291746139526 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s2s.predict([val_enc_input_data,val_dec_input_data])"
      ],
      "metadata": {
        "id": "rspjiEnida96",
        "outputId": "7bcbea59-7118-4ab0-a7f0-1ae9ff60d9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-153-3fb2d9bdfc29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_enc_input_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dec_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   2951\u001b[0m       \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2953\u001b[0;31m           \u001b[0;34mf'Models passed to `{method_name}` can only have `training` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2954\u001b[0m           \u001b[0;34m'and the first argument in `call()` as positional arguments, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m           f'found: {extra_args}.')\n",
            "\u001b[0;31mValueError\u001b[0m: Models passed to `predict` can only have `training` and the first argument in `call()` as positional arguments, found: ['dec_inp']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s2s.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")\n",
        "\n",
        "#s2s.summary()\n",
        "s2s.fit(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "qK-34q8xNQi_",
        "outputId": "ffbc275a-7574-43dc-9140-4c063bedc849"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-52374e7d06fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   2951\u001b[0m       \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2953\u001b[0;31m           \u001b[0;34mf'Models passed to `{method_name}` can only have `training` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2954\u001b[0m           \u001b[0;34m'and the first argument in `call()` as positional arguments, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m           f'found: {extra_args}.')\n",
            "\u001b[0;31mValueError\u001b[0m: Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['dec_inp']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Seq2seq(tf.keras.Model):\n",
        "  def __init__(self, num_encoder_tokens, num_decoder_tokens,embedding_dim,num_of_layers,unit_type, dropout , recurrent_dropout):\n",
        "    super().__init__()\n",
        "    self.encoder_inputs = Input(shape = (None,), name = \"Input_layer_1\")\n",
        "    self.decoder_inputs = keras.Input(shape=(None,), name = \"Input_layer_2\")\n",
        "    self.num_encoder_tokens = num_encoder_tokens\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.dropout = dropout\n",
        "    self.recurrent_dropout = recurrent_dropout\n",
        "    self.num_decoder_tokens = num_decoder_tokens\n",
        "    self.num_of_encoder_layer  =num_of_layers\n",
        "    self.num_of_decoder_layer =num_of_layers\n",
        "    self.type_encoder_unit =unit_type \n",
        "    self.type_decoder_unit =unit_type\n",
        "    self.train_step()\n",
        "    self.build_model()\n",
        "\n",
        "  def get_embedding_layer(self, num_encoder_tokens, embedding_dim,  name):\n",
        "    return Embedding(num_encoder_tokens, embedding_dim, mask_zero = True, name =name )\n",
        "\n",
        "  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n",
        "    #print(cell_type)\n",
        "    if cell_type == \"lstm\":\n",
        "      return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n",
        "    elif cell_type == \"rnn\":\n",
        "      return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "    elif cell_type ==\"gru\":\n",
        "      return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n",
        "    else:\n",
        "      print(f\"Invalid cell type: {cell_type}\")\n",
        "  def get_encoder(self,latent_dim, cell_type = \"lstm\", num_of_layer = 1, name = None ):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim) for i in range(num_of_layer)],), return_sequences=True, return_state=True, name = name)\n",
        "\n",
        "  def get_decoder(self,latent_dim ,cell_type = \"lstm\", num_of_layer = 1, name = None ):\n",
        "    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(cell_type, latent_dim,) for i in range(num_of_layer)]), return_sequences=True, return_state=True)\n",
        "\n",
        "  def get_dense_layer(self, num_decoder_token, activation = \"softmax\"):\n",
        "    return Dense(num_decoder_tokens, activation= activation)\n",
        "\n",
        "  def train_step(self):\n",
        "    self.embedding_layer = self.get_embedding_layer( self.num_encoder_tokens, self.embedding_dim ,name = \"encoder_embedding\")\n",
        "    self.embedding_results = self.embedding_layer(self.encoder_inputs)\n",
        "    print(self.embedding_results.shape)\n",
        "    self.encoder = self.get_encoder( self.embedding_dim,self.type_encoder_unit, self.num_of_encoder_layer , name =\"encoder\" )\n",
        "    encoder_results = self.encoder(self.embedding_results)\n",
        "\n",
        "    self.encoder_outputs, self.encoder_states = encoder_results[0], encoder_results[1:]\n",
        "\n",
        "    self.embedding_layer2 = self.get_embedding_layer( self.num_decoder_tokens, self.embedding_dim, name = \"decoder_embedding\")\n",
        "    self.embedding_results2 = self.embedding_layer2(self.decoder_inputs,)\n",
        "\n",
        "    self.decoder = self.get_decoder( self.embedding_dim, self.type_decoder_unit, self.num_of_decoder_layer,)\n",
        "    self.decoder_results = self.decoder(self.embedding_results2, initial_state=self.encoder_states)\n",
        "\n",
        "    self.decoder_output = self.decoder_results[0]\n",
        "    self.decoder_dense = self.get_dense_layer(self.num_decoder_tokens)\n",
        "    self.dense_output = self.decoder_dense(self.decoder_output)\n",
        "\n",
        "  def build_model(self):\n",
        "    \n",
        "    self.model = keras.Model([self.encoder_inputs, self.decoder_inputs], self.dense_output, name = \"Seq2Seq_model\")\n",
        "    return self.model\n",
        "\n"
      ],
      "metadata": {
        "id": "x9fpzXhN7DMU",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:57:09.043044Z",
          "iopub.execute_input": "2022-06-19T14:57:09.043462Z",
          "iopub.status.idle": "2022-06-19T14:57:09.068515Z",
          "shell.execute_reply.started": "2022-06-19T14:57:09.043428Z",
          "shell.execute_reply": "2022-06-19T14:57:09.067563Z"
        },
        "trusted": true
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, 1024,1,\"rnn\", 0.0, 0.0).build_model()\n",
        "seq2seq.summary()"
      ],
      "metadata": {
        "id": "qJ07iEXsJr6K",
        "outputId": "f82e8d28-64d7-4a30-9437-d3dbaba684b5",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:59:22.231672Z",
          "iopub.execute_input": "2022-06-19T14:59:22.232141Z",
          "iopub.status.idle": "2022-06-19T14:59:22.718772Z",
          "shell.execute_reply.started": "2022-06-19T14:59:22.232106Z",
          "shell.execute_reply": "2022-06-19T14:59:22.717672Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, None, 1024)\n",
            "Model: \"Seq2Seq_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Input_layer_1 (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Input_layer_2 (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 1024)   26624       ['Input_layer_1[0][0]']          \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, None, 1024)   66560       ['Input_layer_2[0][0]']          \n",
            "                                                                                                  \n",
            " encoder (RNN)                  [(None, None, 1024)  2098176     ['encoder_embedding[0][0]']      \n",
            "                                , (None, 1024)]                                                   \n",
            "                                                                                                  \n",
            " rnn (RNN)                      [(None, None, 1024)  2098176     ['decoder_embedding[0][0]',      \n",
            "                                , (None, 1024)]                   'encoder[0][1]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 65)     66625       ['rnn[0][0]']                    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,356,161\n",
            "Trainable params: 4,356,161\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n"
      ],
      "metadata": {
        "id": "HXZ2_xdsAQee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s2s.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")\n",
        "\n",
        "s2s.fit(\n",
        "    [enc_input_data, dec_input_data],\n",
        "    dec_target_data,\n",
        "    batch_size=64,\n",
        "    epochs=1,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T14:21:19.177217Z",
          "iopub.execute_input": "2022-06-19T14:21:19.177828Z",
          "iopub.status.idle": "2022-06-19T14:21:19.197353Z",
          "shell.execute_reply.started": "2022-06-19T14:21:19.177762Z",
          "shell.execute_reply": "2022-06-19T14:21:19.196282Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Haec2CJvrP32",
        "outputId": "3f520910-41d6-464c-bbb4-6ebef57a7b5d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-4e8aeed0a64f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdec_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_call_args\u001b[0;34m(self, method_name)\u001b[0m\n\u001b[1;32m   2951\u001b[0m       \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m       raise ValueError(\n\u001b[0;32m-> 2953\u001b[0;31m           \u001b[0;34mf'Models passed to `{method_name}` can only have `training` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2954\u001b[0m           \u001b[0;34m'and the first argument in `call()` as positional arguments, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m           f'found: {extra_args}.')\n",
            "\u001b[0;31mValueError\u001b[0m: Models passed to `fit` can only have `training` and the first argument in `call()` as positional arguments, found: ['dec_inp']."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearch(keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self, beam_size):\n",
        "    self.beam_size = beam_size\n",
        "\n",
        "  def beam_search_decoder(aelf, data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs = None):\n",
        "    prediction = self.model.predict([val_enc_input_data , val_dec_input_data])\n",
        "    print(prediction.shape)\n",
        "    for i, pred in enumerate(prediction):\n",
        "      beam_search_prediction = self.beam_search_decoder(pred, self.beam_size)\n",
        "      correct_prediction = 0\n",
        "      for k in range(self.beam_size):\n",
        "        #translated_word = \"\\t\"+\"\".join([reverse_target_token[x] for x in beam_search_prediction[k][0][:len(validation_target[i])-1]])\n",
        "        #print(translated_word, validation_target[i])\n",
        "        #print(validation_target[i])\n",
        "        \n",
        "        def idx2char(idx_list):\n",
        "          return \"\".join([reverse_target_token[x] for x in idx_list])\n",
        "\n",
        "        if \"\\t\"+ idx2char(beam_search_prediction[k][0][:len(validation_target[i])-1]) == validation_target[i]:\n",
        "          correct_prediction+=1\n",
        "          break\n",
        "    mul = 10.0**2\n",
        "    logs[\"character_accuracy\"] = ((correct_prediction/prediction.shape[0])*mul)/mul\n",
        "    print(\"- character_accuracy\",logs[\"character_accuracy\"])\n",
        "    #print(f\"Accuracy by Beam Search {correct_prediction/len(validation_target)}\")\n",
        "      # print(len(beam_search_prediction))\n",
        "      # print(beam_search_prediction)\n"
      ],
      "metadata": {
        "id": "KPyDujbe-Brq",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:10:18.730086Z",
          "iopub.execute_input": "2022-06-19T15:10:18.730979Z",
          "iopub.status.idle": "2022-06-19T15:10:18.745337Z",
          "shell.execute_reply.started": "2022-06-19T15:10:18.730935Z",
          "shell.execute_reply": "2022-06-19T15:10:18.744072Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_decoder(data, k):\n",
        "    decodedWords = [[list(), 0.0]]\n",
        "    # walk over each step in sequence\n",
        "    for word in data:\n",
        "      candidates = list()\n",
        "      # expand each current candidate\n",
        "      for sequence in decodedWords:\n",
        "        seq, score = sequence\n",
        "        for j in range(len(word)):\n",
        "          candidate = [seq + [j], score - log(word[j])]\n",
        "          candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(candidates, key=lambda a:a[1])\n",
        "      # select k best\n",
        "      decodedWords = ordered[:k]\n",
        "    return decodedWords\n",
        "  \n",
        "def translate(seq):\n",
        "  sentence = [] \n",
        "  for x in seq:\n",
        "    char = reverse_target_token[x]\n",
        "    sentence.append(char)\n",
        "  return \"\".join(sentence)\n",
        "class WordAccuracyCallback(keras.callbacks.Callback):\n",
        "  def __init__(self,beam_size):\n",
        "    self.beam_size=beam_size\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    pred=self.model.predict([val_enc_input_data , val_dec_input_data])\n",
        "    count=0\n",
        "    for i in range(pred.shape[0]):\n",
        "      pSequences=beam_search_decoder(pred[i],self.beam_size)\n",
        "      for j in range(self.beam_size):\n",
        "        if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n",
        "          count=count+1\n",
        "          break\n",
        "    factor = 10.0 ** 4\n",
        "    logs[\"WordAccuracy\"]=math.trunc((count/pred.shape[0])*factor)/factor\n",
        "    print(\"- wordAccuracy:\",logs[\"WordAccuracy\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:20.414711Z",
          "iopub.execute_input": "2022-06-19T15:22:20.415244Z",
          "iopub.status.idle": "2022-06-19T15:22:20.431301Z",
          "shell.execute_reply.started": "2022-06-19T15:22:20.415207Z",
          "shell.execute_reply": "2022-06-19T15:22:20.429988Z"
        },
        "trusted": true,
        "id": "FE8vtOJOrP33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    'method':'bayes',\n",
        "    'metric': {\n",
        "        'name':'val_accuracy',\n",
        "        'goal':'maximize'\n",
        "    },\n",
        "    'parameters':{\n",
        "    \n",
        "    \"num_of_layer\" : {'values': [1,2,3]},\n",
        "    \"unit_size\": {\"values\":[16,32,64]},\n",
        "    \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n",
        "    \"dropout\": {\"values\": [0.0, 0.2, 0.4]},\n",
        "    'recurrent_dropout':{'values':[0.0,0.3]},\n",
        "    \"beam_size\" : {\"values\":[1,2,3,4]},\n",
        "    \"epochs\":{\"value\":20},  \n",
        "    \"optimizer\":{\"values\": [\"adam\",\"rmsprop\"]}             \n",
        "                   }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "pprint.pprint(sweep_config)"
      ],
      "metadata": {
        "id": "mIW2Ofow5Deo",
        "outputId": "0c99b3cb-eaac-4b1e-acfa-1aff5a6bad9e",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:21:23.223786Z",
          "iopub.execute_input": "2022-06-19T14:21:23.224923Z",
          "iopub.status.idle": "2022-06-19T14:21:23.234457Z",
          "shell.execute_reply.started": "2022-06-19T14:21:23.224875Z",
          "shell.execute_reply": "2022-06-19T14:21:23.233323Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'method': 'bayes',\n 'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n 'parameters': {'beam_size': {'values': [1, 2, 3, 4]},\n                'dropout': {'values': [0.0, 0.2, 0.4]},\n                'epochs': {'value': 20},\n                'num_of_layer': {'values': [1, 2, 3]},\n                'optimizer': {'values': ['adam', 'rmsprop']},\n                'recurrent_dropout': {'values': [0.0, 0.3]},\n                'unit_size': {'values': [16, 32, 64]},\n                'unit_type': {'values': ['lstm', 'rnn', 'gru']}}}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"seq2seq\")"
      ],
      "metadata": {
        "id": "x8YmtLZN_74p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config = None):\n",
        "  with wandb.init(config=config):\n",
        "    config = wandb.config\n",
        "    #print(config)\n",
        "    seq2seq = Seq2seq(num_encoder_tokens,num_decoder_tokens, config.unit_size, config.num_of_layer,config.unit_type , config.dropout,config.recurrent_dropout).build_model()\n",
        "    seq2seq.compile(optimizer=config.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\",])\n",
        "    seq2seq.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=batch_size,\n",
        "        epochs=config.epochs,\n",
        "        validation_data =  ([validation_encoder_input_data , validation_decoder_input_data] ,validation_decoder_target_data),\n",
        "        callbacks = [BeamSearch(config.beam_size), WandbCallback()],verbose = 1, \n",
        "        )\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "qYv2feSRAzW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')]\n",
        ")\n",
        "seq2seq.metrics_names\n",
        "\n"
      ],
      "metadata": {
        "id": "DIrCXZTAGyTL",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:24.717670Z",
          "iopub.execute_input": "2022-06-19T15:22:24.718289Z",
          "iopub.status.idle": "2022-06-19T15:22:24.738490Z",
          "shell.execute_reply.started": "2022-06-19T15:22:24.718252Z",
          "shell.execute_reply": "2022-06-19T15:22:24.737389Z"
        },
        "trusted": true,
        "outputId": "0b540428-89cc-4ac6-ed17-8fc3bffafd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 290,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=seq2seq.predict([val_enc_input_data , val_dec_input_data])\n",
        "count=0\n",
        "for i in range(pred.shape[0]//400):\n",
        "      pSequences=beam_search_decoder(pred[i],3)\n",
        "      for j in range(3):\n",
        "        print({\"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])}, \"original =\", {validation_target[i]} )\n",
        "        if \"\\t\"+translate(pSequences[j][0][:len(validation_target[i])-1])==validation_target[i]:\n",
        "          count=count+1\n",
        "          print(\"yes\")\n",
        "          break\n",
        "factor = 10.0 ** 4\n"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2022-06-19T15:41:09.089308Z",
          "iopub.execute_input": "2022-06-19T15:41:09.089822Z",
          "iopub.status.idle": "2022-06-19T15:41:22.154791Z",
          "shell.execute_reply.started": "2022-06-19T15:41:09.089769Z",
          "shell.execute_reply": "2022-06-19T15:41:22.153760Z"
        },
        "trusted": true,
        "id": "xLTgqB68rP35",
        "outputId": "416e74fe-2665-4117-c579-56eb7c7beade"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "{'\\tकं\\nा'} original = {'\\tअंक\\n'}\n{'\\tवं\\nा'} original = {'\\tअंक\\n'}\n{'\\t्ं\\nा'} original = {'\\tअंक\\n'}\n{'\\tकं\\nा'} original = {'\\tअंक\\n'}\n{'\\tवं\\nा'} original = {'\\tअंक\\n'}\n{'\\t्ं\\nा'} original = {'\\tअंक\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकित\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकों\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tकन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tवन\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\t्न\\n\\n\\n\\n'} original = {'\\tअंकोर\\n'}\n{'\\tकन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tवन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\t्न\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tकन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tवन\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\t्न\\n\\n\\n\\n\\n'} original = {'\\tअंगारक\\n'}\n{'\\tकर\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n{'\\tवर\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n{'\\t्र\\n\\n\\n\\n\\n\\n\\n'} original = {'\\tअंग्रज़ी\\n'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = seq2seq.predict([val_enc_input_data , val_dec_input_data])\n",
        "x.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-06-19T15:33:14.072698Z",
          "iopub.execute_input": "2022-06-19T15:33:14.073169Z",
          "iopub.status.idle": "2022-06-19T15:33:26.484545Z",
          "shell.execute_reply.started": "2022-06-19T15:33:14.073134Z",
          "shell.execute_reply": "2022-06-19T15:33:26.483378Z"
        },
        "trusted": true,
        "id": "qnzLTsbxrP36",
        "outputId": "357821dd-b5f9-4924-add4-0ebd058f94a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 301,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(4502, 17, 65)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "histotry = seq2seq.fit(\n",
        "    [enc_input_data, dec_input_data],\n",
        "    dec_target_data,\n",
        "    batch_size=8192,\n",
        "    epochs=1,\n",
        "    callbacks = [WordAccuracyCallback(3), ],\n",
        ")\n",
        "# Save model\n",
        "seq2seq.save(\"s2s\")\n"
      ],
      "metadata": {
        "id": "ox_fyYUrAQef",
        "outputId": "f4da868c-355c-4132-8481-e4ffcb605de4",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:22:25.394757Z",
          "iopub.execute_input": "2022-06-19T15:22:25.395410Z",
          "iopub.status.idle": "2022-06-19T15:28:55.450107Z",
          "shell.execute_reply.started": "2022-06-19T15:22:25.395375Z",
          "shell.execute_reply": "2022-06-19T15:28:55.448703Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "6/6 [==============================] - 214s 34s/step - loss: 1.1271 - acc: 0.1550\n- wordAccuracy: 0.0\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for key in histotry.history.keys():\n",
        "#       print(key , histotry.history[key])\n",
        "#       #wandb.log({key : histotry.history[key]})"
      ],
      "metadata": {
        "id": "2BnI7lHtQtnT",
        "execution": {
          "iopub.status.busy": "2022-06-19T14:17:37.411280Z",
          "iopub.status.idle": "2022-06-19T14:17:37.411991Z",
          "shell.execute_reply.started": "2022-06-19T14:17:37.411629Z",
          "shell.execute_reply": "2022-06-19T14:17:37.411665Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq.metrics_names"
      ],
      "metadata": {
        "id": "Npqd4if4HVZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ],
      "metadata": {
        "id": "BC5CbwHlAQef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define sampling models\n",
        "# # Restore the model and construct the encoder and decoder.\n",
        "# model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "# encoder_inputs = model.input[0]  # input_1\n",
        "# temp = model.layers[2].output\n",
        "# encoder_outputs, state = temp[0], temp[1:]  # lstm_1\n",
        "# encoder_states = state\n",
        "# encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# decoder_inputs = model.input[1]  # input_2\n",
        "# decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "# decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "# decoder_states_inputs = state\n",
        "# decoder_lstm = model.layers[3]\n",
        "# temp = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "# decoder_outputs, state_dec = temp[0], temp[1:]\n",
        "# decoder_states = state_dec\n",
        "# decoder_dense = model.layers[4]\n",
        "# decoder_outputs = decoder_dense(decoder_outputs)\n",
        "# decoder_model = keras.Model(\n",
        "#     [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        "# )\n",
        "\n",
        "# # Reverse-lookup token index to decode sequences back to\n",
        "# # something readable.\n",
        "# # reverse_input_char_index = dict((i, char) for char, i in num_encoder_tokens.items())\n",
        "# # reverse_target_char_index = dict((i, char) for char, i in num_decoder_tokens.items())\n",
        "# # print(reverse_input_char_index)\n",
        "# # print(input_token_index)\n",
        "\n",
        "# reverse_input_token = dict((i, char) for char, i in input_token.items())\n",
        "# reverse_target_token = dict((i, char) for char, i in target_token.items())\n",
        "# def decode_sequence(input_seq):\n",
        "#     # Encode the input as state vectors.\n",
        "#     states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "#     # Generate empty target sequence of length 1.\n",
        "#     target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#     # Populate the first character of target sequence with the start character.\n",
        "#     target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "#     # Sampling loop for a batch of sequences\n",
        "#     # (to simplify, here we assume a batch of size 1).\n",
        "#     stop_condition = False\n",
        "#     decoded_sentence = \"\"\n",
        "#     while not stop_condition:\n",
        "#         temp = decoder_model.predict([target_seq] + states_value)\n",
        "#         output_tokens, state = temp[0],temp[1:]\n",
        "\n",
        "#         # Sample a token\n",
        "#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "#         #print(reverse_target_char_index)\n",
        "#         sampled_char = reverse_target_token[sampled_token_index]\n",
        "#         decoded_sentence += sampled_char\n",
        "\n",
        "#         # Exit condition: either hit max length\n",
        "#         # or find stop character.\n",
        "#         if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "#             stop_condition = True\n",
        "\n",
        "#         # Update the target sequence (of length 1).\n",
        "#         target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "#         target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "#         # Update states\n",
        "#         states_value = state\n",
        "#     return decoded_sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "meuadqgkAQeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now generate decoded sentences as such:\n"
      ],
      "metadata": {
        "id": "nSNoQ5AvAQeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for seq_index in range(20):\n",
        "#     # Take one sequence (part of the training set)\n",
        "#     # for trying out decoding.\n",
        "#     input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "#     decoded_sentence = decode_sequence(input_seq)\n",
        "#     print(\"-\")\n",
        "#     print(\"Input sentence:\", input_texts[seq_index])\n",
        "#     print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "metadata": {
        "id": "wjp__2oJAQeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git log"
      ],
      "metadata": {
        "id": "ygwmClalB8vI",
        "execution": {
          "iopub.status.busy": "2022-06-19T15:44:04.633143Z",
          "iopub.execute_input": "2022-06-19T15:44:04.633593Z",
          "iopub.status.idle": "2022-06-19T15:44:05.689112Z",
          "shell.execute_reply.started": "2022-06-19T15:44:04.633552Z",
          "shell.execute_reply": "2022-06-19T15:44:05.687915Z"
        },
        "trusted": true,
        "outputId": "09f6fff2-5b5d-42a4-e7b8-2e20df103971"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lzaKcIYKrP39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oX52e6mtrP39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}