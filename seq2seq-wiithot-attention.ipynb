{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Copyright 2020 The TensorFlow Authors.","metadata":{"id":"5aElYAKlV2Mi"}},{"cell_type":"code","source":"#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"cellView":"form","id":"wmYJlt6LWVOU","execution":{"iopub.status.busy":"2022-06-25T18:11:14.354574Z","iopub.execute_input":"2022-06-25T18:11:14.355029Z","iopub.status.idle":"2022-06-25T18:11:14.376039Z","shell.execute_reply.started":"2022-06-25T18:11:14.354935Z","shell.execute_reply":"2022-06-25T18:11:14.374796Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism\n\n<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n      <td>\n    <a href=\"https://storage.googleapis.com/tensorflow_docs/addons/docs/tutorials/networks_seq2seq_nmt.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n  </td>\n</table>","metadata":{"id":"L-8q8rRRWcp6"}},{"cell_type":"markdown","source":"## Overview\nThis notebook gives a brief introduction into the ***Sequence to Sequence Model Architecture***\nIn this noteboook you broadly cover four essential topics necessary for Neural Machine Translation:\n\n\n* **Data cleaning**\n* **Data preparation**\n* **Neural Translation Model with Attention**\n* **Final Translation with ```tf.addons.seq2seq.BasicDecoder``` and ```tf.addons.seq2seq.BeamSearchDecoder```** \n\nThe basic idea behind such a model though, is only the encoder-decoder architecture. These networks are usually used for a variety of tasks like text-summerization, Machine translation, Image Captioning, etc. This tutorial provideas a hands-on understanding of the concept, explaining the technical jargons wherever necessary. You focus on the task of Neural Machine Translation (NMT) which was the very first testbed for seq2seq models.\n","metadata":{"id":"9n0dcDw1Wszw"}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"MpySVYWJhxaV"}},{"cell_type":"code","source":"!pip install tensorflow-addons==0.11.2","metadata":{"id":"_kxfdP4hJUPB","outputId":"e371e45f-4028-40c8-ac95-de62f14c734e","execution":{"iopub.status.busy":"2022-06-25T18:11:14.942308Z","iopub.execute_input":"2022-06-25T18:11:14.943336Z","iopub.status.idle":"2022-06-25T18:11:43.623789Z","shell.execute_reply.started":"2022-06-25T18:11:14.943283Z","shell.execute_reply":"2022-06-25T18:11:43.622308Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow-addons==0.11.2\n  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons==0.11.2) (2.13.3)\nInstalling collected packages: tensorflow-addons\n  Attempting uninstall: tensorflow-addons\n    Found existing installation: tensorflow-addons 0.14.0\n    Uninstalling tensorflow-addons-0.14.0:\n      Successfully uninstalled tensorflow-addons-0.14.0\nSuccessfully installed tensorflow-addons-0.11.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom IPython.display import HTML as html_print\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport unicodedata\nimport re\nimport numpy as np\nimport os\nimport io\nimport time\nimport wandb\nimport os\nimport io\nfrom wandb.keras import WandbCallback\nimport time\nimport sys\nfrom kaggle_secrets import UserSecretsClient\nfrom tensorflow.keras.layers import Embedding, SimpleRNNCell, GRUCell, Dense, LSTMCell\n# user_secrets = UserSecretsClient()\n# wandb_api = user_secrets.get_secret(\"wandb_api\")\n\n# #wandb.login(key=wandb_api)\n# ! wandb login $wandb_api\n\n# os.environ[\"WANDB_SILENT\"] = \"false\"\nwandb.login()","metadata":{"id":"tnxXKDjq3jEL","outputId":"910703ab-e297-4b00-9eae-d78e1a0b69b4","execution":{"iopub.status.busy":"2022-06-25T18:11:43.627296Z","iopub.execute_input":"2022-06-25T18:11:43.627883Z","iopub.status.idle":"2022-06-25T18:12:22.464085Z","shell.execute_reply.started":"2022-06-25T18:11:43.627811Z","shell.execute_reply":"2022-06-25T18:12:22.462560Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.6.4 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  UserWarning,\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Cleaning and Data Preparation \n\n","metadata":{"id":"Ii_vg-XNXTil"}},{"cell_type":"code","source":"tf.__version__","metadata":{"id":"ckjyipboLFwf","execution":{"iopub.status.busy":"2022-06-25T18:12:22.465507Z","iopub.execute_input":"2022-06-25T18:12:22.466187Z","iopub.status.idle":"2022-06-25T18:12:22.475363Z","shell.execute_reply.started":"2022-06-25T18:12:22.466148Z","shell.execute_reply":"2022-06-25T18:12:22.473356Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'2.6.4'"},"metadata":{}}]},{"cell_type":"code","source":"# def download_nmt():\n#     path_to_zip = tf.keras.utils.get_file(\n#     'dakshina_dataset_v1.0.tar', origin='https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar',\n#     extract=True, untar = True)\n\n#     path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n#     return path_to_file\n# download_nmt()","metadata":{"id":"PvRnGWnvXm6l","outputId":"2374ffd1-9669-44f1-9215-4a6fc79be8c1","execution":{"iopub.status.busy":"2022-06-25T18:12:22.479290Z","iopub.execute_input":"2022-06-25T18:12:22.479750Z","iopub.status.idle":"2022-06-25T18:12:22.485854Z","shell.execute_reply.started":"2022-06-25T18:12:22.479716Z","shell.execute_reply":"2022-06-25T18:12:22.484887Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Define a DakshinaDataset class with necessary functions to follow Step 1 to Step 4. \nThe ```call()``` will return:\n1. ```train_dataset```  and ```val_dataset``` : ```tf.data.Dataset``` objects\n2. ```inp_lang_tokenizer``` and ```targ_lang_tokenizer``` : ```tf.keras.preprocessing.text.Tokenizer``` objects ","metadata":{"id":"NFKB2c_tX4wU"}},{"cell_type":"code","source":"!wget  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n!tar -xf 'dakshina_dataset_v1.0.tar'\ntrain_file_path = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\nval_file_path= \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\ntest_file_path  = \"./dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"","metadata":{"id":"rZal_kwQLurS","outputId":"2f7ac694-ee61-4587-84d4-1a9536b22042","execution":{"iopub.status.busy":"2022-06-25T18:12:22.486811Z","iopub.execute_input":"2022-06-25T18:12:22.487156Z","iopub.status.idle":"2022-06-25T18:13:02.684292Z","shell.execute_reply.started":"2022-06-25T18:12:22.487123Z","shell.execute_reply":"2022-06-25T18:13:02.682952Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"--2022-06-25 18:12:23--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 108.177.12.128, 74.125.141.128, 142.250.97.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|108.177.12.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G  66.4MB/s    in 31s     \n\n2022-06-25 18:12:55 (61.2 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"class DakshinaDataset:\n    def __init__(self, problem_type='en-spa'):\n        self.problem_type = 'en-spa'\n        self.inp_lang_tokenizer = None\n        self.targ_lang_tokenizer = None\n        self.num_of_train = 0\n        self.num_of_test = 0\n        self.num_of_val = 0\n    \n\n    def unicode_to_ascii(self, s):\n        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\n    ## Step 1 and Step 2 \n    def preprocess_sentence(self, w):\n        # w = self.unicode_to_ascii(w.lower().strip())\n\n        # # creating a space between a word and the punctuation following it\n        # # eg: \"he is a boy.\" => \"he is a boy .\"\n        # # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n        # w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n        # w = re.sub(r'[\" \"]+', \" \", w)\n\n        # # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n        # w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n\n        # w = w.strip()\n\n        # adding a start and an end token to the sentence\n        # so that the model know when to start and stop predicting.\n        w = '\\t' + w + '\\n'\n        return w\n    \n    def create_dataset(self, path, data_name):\n        # path : path to spa-eng.txt file\n        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n        lines = io.open(path, encoding='UTF-8').read().split('\\n')\n        #print(lines)\n        if data_name == \"train\":\n          self.num_of_train = len(lines) -1\n        elif data_name == \"val\":\n          self.num_of_val = len(lines) -1\n        else:\n          self.num_of_test = len(lines) -1\n        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:len(lines)-1]]\n        #print(word_pairs)\n\n        \n        return zip(*word_pairs)\n\n    # Step 3 and Step 4\n    def tokenize(self, lang):\n        # lang = list of sentences in a language\n        \n        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = True)\n        lang_tokenizer.fit_on_texts(lang)\n\n        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n        tensor = lang_tokenizer.texts_to_sequences(lang) \n\n        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n        ## and pads the sequences to match the longest sequences in the given input\n        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n\n        return tensor, lang_tokenizer\n\n    def load_dataset(self, path, data_name = None, ):\n        targ_lang, inp_lang ,_= self.create_dataset(path, data_name)\n        if data_name == \"train\":\n            # creating cleaned input, output pairs\n            \n            #print(targ_lang, inp_lang)\n            input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n            #print(input_tensor, inp_lang_tokenizer.word_index)\n            target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n            return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n        else:\n            \n            #print(targ_lang, inp_lang)\n            input_tensor= self.inp_lang_tokenizer.texts_to_sequences(inp_lang)\n            #print(input_tensor)\n            input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, padding='post', maxlen = 22)\n            #print(input_tensor, inp_lang_tokenizer.word_index)\n            target_tensor = self.targ_lang_tokenizer.texts_to_sequences(targ_lang)\n            target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post', maxlen =21)\n            #print(target_tensor, targ_lang_tokenizer.word_index)\n            return input_tensor, target_tensor\n        \n\n    def call(self,  BUFFER_SIZE, BATCH_SIZE):\n        file_path = train_file_path\n        input_tensor_train, target_tensor_train, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(train_file_path, data_name =\"train\" )\n        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n        print(\"val\")\n        file_path = val_file_path\n        input_tensor_val, target_tensor_val = self.load_dataset(val_file_path, \"val\")\n        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n        val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n        print(\"test\")\n        file_path = test_file_path\n        input_tensor_test, target_tensor_test = self.load_dataset(test_file_path,  \"test\")\n        test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, target_tensor_test))\n        test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n        # val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n        # val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n        return train_dataset, val_dataset, test_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer","metadata":{"id":"JMAHz7kJXc5N","execution":{"iopub.status.busy":"2022-06-25T18:13:02.686578Z","iopub.execute_input":"2022-06-25T18:13:02.686947Z","iopub.status.idle":"2022-06-25T18:13:02.738921Z","shell.execute_reply.started":"2022-06-25T18:13:02.686910Z","shell.execute_reply":"2022-06-25T18:13:02.737342Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 32000\nBATCH_SIZE = 64\n# Let's limit the #training examples for faster training\nnum_examples = 500\n\ndataset_creator = DakshinaDataset('en-hi')\ntrain_dataset, val_dataset, test_dataset, inp_lang, targ_lang = dataset_creator.call( BUFFER_SIZE, BATCH_SIZE)","metadata":{"id":"EIW4NVBmJ25k","outputId":"8cab0886-a0dc-4761-f6af-a70fec351403","execution":{"iopub.status.busy":"2022-06-25T18:13:25.556692Z","iopub.execute_input":"2022-06-25T18:13:25.559310Z","iopub.status.idle":"2022-06-25T18:13:27.842062Z","shell.execute_reply.started":"2022-06-25T18:13:25.559212Z","shell.execute_reply":"2022-06-25T18:13:27.840470Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-06-25 18:13:27.382726: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"name":"stdout","text":"val\ntest\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_creator.num_of_val","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:13:27.844300Z","iopub.execute_input":"2022-06-25T18:13:27.845281Z","iopub.status.idle":"2022-06-25T18:13:27.855074Z","shell.execute_reply.started":"2022-06-25T18:13:27.845220Z","shell.execute_reply":"2022-06-25T18:13:27.853319Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"4502"},"metadata":{}}]},{"cell_type":"code","source":"next(iter(val_dataset))","metadata":{"id":"Qi37WHhrNtEr","execution":{"iopub.status.busy":"2022-06-25T18:13:27.857034Z","iopub.execute_input":"2022-06-25T18:13:27.857589Z","iopub.status.idle":"2022-06-25T18:13:27.947151Z","shell.execute_reply.started":"2022-06-25T18:13:27.857537Z","shell.execute_reply":"2022-06-25T18:13:27.945627Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(<tf.Tensor: shape=(64, 22), dtype=int32, numpy=\n array([[ 2,  1, 21, ...,  0,  0,  0],\n        [ 2, 15,  8, ...,  0,  0,  0],\n        [ 2, 13,  1, ...,  0,  0,  0],\n        ...,\n        [ 2,  7, 10, ...,  0,  0,  0],\n        [ 2,  8, 15, ...,  0,  0,  0],\n        [ 2,  1, 16, ...,  0,  0,  0]], dtype=int32)>,\n <tf.Tensor: shape=(64, 21), dtype=int32, numpy=\n array([[ 1, 40, 22, ...,  0,  0,  0],\n        [ 1, 44, 10, ...,  0,  0,  0],\n        [ 1,  8, 14, ...,  0,  0,  0],\n        ...,\n        [ 1,  4, 15, ...,  0,  0,  0],\n        [ 1, 10, 24, ...,  0,  0,  0],\n        [ 1, 31, 14, ...,  0,  0,  0]], dtype=int32)>)"},"metadata":{}}]},{"cell_type":"code","source":"example_input_batch, example_target_batch = next(iter(train_dataset))\nexample_input_batch.shape, example_target_batch.shape","metadata":{"id":"w2lCTy4vKOkB","outputId":"8abb8fb9-a1d0-468e-94ed-0e048a7b772f","execution":{"iopub.status.busy":"2022-06-25T18:13:27.949609Z","iopub.execute_input":"2022-06-25T18:13:27.949951Z","iopub.status.idle":"2022-06-25T18:13:28.073734Z","shell.execute_reply.started":"2022-06-25T18:13:27.949918Z","shell.execute_reply":"2022-06-25T18:13:28.072089Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(TensorShape([64, 22]), TensorShape([64, 21]))"},"metadata":{}}]},{"cell_type":"markdown","source":"### Some important parameters","metadata":{"id":"rgCLkfv5uO3d"}},{"cell_type":"code","source":"vocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\nmax_length_input = example_input_batch.shape[1]\nmax_length_output = example_target_batch.shape[1]\n\nembedding_dim = 256\nunits = 1024\n\n","metadata":{"id":"TqHsArVZ3jFS","execution":{"iopub.status.busy":"2022-06-25T18:13:28.117556Z","iopub.execute_input":"2022-06-25T18:13:28.117980Z","iopub.status.idle":"2022-06-25T18:13:28.125906Z","shell.execute_reply.started":"2022-06-25T18:13:28.117945Z","shell.execute_reply":"2022-06-25T18:13:28.124480Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(\"max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\")\nmax_length_input, max_length_output, vocab_inp_size, vocab_tar_size","metadata":{"id":"g-yY9c6aIu1h","outputId":"6142b2ed-7809-4304-a75b-40bd2aa85368","execution":{"iopub.status.busy":"2022-06-25T18:13:28.529748Z","iopub.execute_input":"2022-06-25T18:13:28.531081Z","iopub.status.idle":"2022-06-25T18:13:28.539118Z","shell.execute_reply.started":"2022-06-25T18:13:28.531043Z","shell.execute_reply":"2022-06-25T18:13:28.538138Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"max_length_english, max_length_spanish, vocab_size_english, vocab_size_spanish\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(22, 21, 29, 66)"},"metadata":{}}]},{"cell_type":"code","source":"##### \n\nclass Encoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, num_of_layers, enc_unit_type, dropout, recurrent_dropout):\n    super(Encoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.enc_units = enc_units\n    self.num_of_layers = num_of_layers\n    self.enc_unit_type = enc_unit_type\n    self.dropout = dropout\n    self.recurrent_dropout = recurrent_dropout\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n    ##-------- LSTM layer in Encoder ------- ##\n    self.encoder_layer = self.get_encoder_layer(self.enc_units,\n                                                self.num_of_layers, self.enc_unit_type)\n    \n  def get_encoder_layer(self, enc_units, num_of_layers, enc_unit_type):\n    return tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells( [self.get_cell(enc_unit_type, \n                                                                                 enc_units) for i in range(num_of_layers)],),\n                                  return_sequences=True, return_state=True, name = \"Encoder\")\n  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n      #print(cell_type)\n      if cell_type == \"lstm\":\n        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n      elif cell_type == \"rnn\":\n        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      elif cell_type ==\"gru\":\n        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      else:\n        print(f\"Invalid cell type: {cell_type}\")\n\n  def call(self, x, hidden):\n    x = self.embedding(x)\n    output = self.encoder_layer(x, initial_state = hidden)\n    return output[0], output[1:]\n\n  def initialize_hidden_state(self):\n    if self.enc_unit_type == 'rnn' or self.enc_unit_type == \"gru\":\n        return [tf.zeros((self.batch_sz, self.enc_units))]*self.num_of_layers\n    else:\n        return [[tf.zeros((self.batch_sz, self.enc_units)),tf.zeros((self.batch_sz, self.enc_units))]]*self.num_of_layers","metadata":{"id":"nZ2rI24i3jFg","execution":{"iopub.status.busy":"2022-06-25T18:13:30.375237Z","iopub.execute_input":"2022-06-25T18:13:30.375993Z","iopub.status.idle":"2022-06-25T18:13:30.393039Z","shell.execute_reply.started":"2022-06-25T18:13:30.375954Z","shell.execute_reply":"2022-06-25T18:13:30.392048Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# ## Test Encoder Stack\n\n# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, 1, \"lstm\", 0.2,0.2)\n\n\n# # sample input\n# sample_hidden = encoder.initialize_hidden_state()\n# sample_output, sample_state= encoder(example_input_batch, sample_hidden)\n# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n# print(len(sample_state))\n# # print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_state[0].shape))\n# # print ('Encoder c vector shape: (batch size, units) {}'.format(sample_state[1].shape))","metadata":{"id":"60gSVh05Jl6l","outputId":"35b35447-3edb-48e6-dab0-16749b0737d9","execution":{"iopub.status.busy":"2022-06-25T18:13:30.742877Z","iopub.execute_input":"2022-06-25T18:13:30.743729Z","iopub.status.idle":"2022-06-25T18:13:30.749805Z","shell.execute_reply.started":"2022-06-25T18:13:30.743676Z","shell.execute_reply":"2022-06-25T18:13:30.748295Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Decoder(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, num_of_layers, dec_unit_type, dropout, recurrent_dropout, attention_type=None,):\n    super(Decoder, self).__init__()\n    self.batch_sz = batch_sz\n    self.dec_units = dec_units\n    self.attention_type = attention_type\n    self.num_of_layers = num_of_layers\n    self.dec_unit_type = dec_unit_type\n    self.dropout = dropout\n    self.recurrent_dropout = recurrent_dropout\n    # Embedding Layer\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n    \n    #Final Dense layer on which softmax will be applied\n    self.fc = tf.keras.layers.Dense(vocab_size)\n\n    # Define the fundamental cell for decoder recurrent structure\n    self.decoder_rnn_cell =  self.get_stacked_rnn_cell()\n   \n\n\n    # Sampler\n    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n    \n    if self.attention_type:\n        # Create attention mechanism with memory = None\n        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n                                                                  None, self.batch_sz*[max_length_input], self.attention_type)\n\n        # Wrap attention mechanism with the fundamental rnn cell of decoder\n        self.rnn_cell = self.build_rnn_cell(batch_sz)\n\n        # Define the decoder with respect to fundamental rnn cell\n        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n    else:\n        self.decoder =tfa.seq2seq.BasicDecoder(self.decoder_rnn_cell, self.sampler, self.fc)\n    \n  def build_rnn_cell(self, batch_sz):\n    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n    return rnn_cell\n\n  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n    # ------------- #\n    # typ: Which sort of attention (Bahdanau, Luong)\n    # dec_units: final dimension of attention outputs \n    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n\n    if(attention_type=='bahdanau'):\n      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n    else:\n      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n\n  def build_initial_state(self, batch_sz, encoder_state, Dtype): \n    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n    return decoder_initial_state \n\n\n  def call(self, inputs, initial_state):\n    x = self.embedding(inputs)\n    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n    return outputs\n  def get_cell(self, cell_type = \"lstm\", num_of_cell = 1, name = None):\n      #print(cell_type)\n      if cell_type == \"lstm\":\n        return LSTMCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout, )\n      elif cell_type == \"rnn\":\n        return SimpleRNNCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      elif cell_type ==\"gru\":\n        return GRUCell(num_of_cell, dropout = self.dropout, recurrent_dropout = self.recurrent_dropout)\n      else:\n        print(f\"Invalid cell type: {cell_type}\")\n\n  def get_stacked_rnn_cell(self,):\n    return tf.keras.layers.StackedRNNCells( [self.get_cell(self.dec_unit_type, self.dec_units,) for i in range(self.num_of_layers)])\n","metadata":{"id":"yJ_B3mhW3jFk","execution":{"iopub.status.busy":"2022-06-25T18:13:30.911159Z","iopub.execute_input":"2022-06-25T18:13:30.911614Z","iopub.status.idle":"2022-06-25T18:13:30.935463Z","shell.execute_reply.started":"2022-06-25T18:13:30.911578Z","shell.execute_reply":"2022-06-25T18:13:30.933775Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# tf.expand_dims([targ_lang.word_index[\"\\t\"]]*1, 1)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:13:31.041737Z","iopub.execute_input":"2022-06-25T18:13:31.042170Z","iopub.status.idle":"2022-06-25T18:13:31.048803Z","shell.execute_reply.started":"2022-06-25T18:13:31.042135Z","shell.execute_reply":"2022-06-25T18:13:31.047099Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Seq2Seq():\n    def __init__(self,vocab_inp_size, vocab_targ_size, encoder_embedding_dim, decoder_embedding_dim, units, batch_sz, num_of_layers, unit_type, dropout, recurrent_dropout, optimizer,metric, attention_type = None):\n        self.encoder = Encoder(vocab_inp_size,encoder_embedding_dim,units,batch_sz,num_of_layers,unit_type,dropout,recurrent_dropout)\n        self.decoder = Decoder(vocab_targ_size,decoder_embedding_dim,units,batch_sz,num_of_layers,unit_type,dropout,recurrent_dropout,attention_type )\n        self.optimizer = optimizer\n        self.metric = metric\n        self.attention_type = None\n    @tf.function\n    def val_step(self,inp, targ, enc_hidden):\n      loss = 0\n\n      with tf.GradientTape() as tape:\n        enc_output, enc_state= self.encoder(inp, enc_hidden)\n\n\n        dec_input = targ[ : , :-1 ] # Ignore <end> token\n        real = targ[ : , 1: ]         # ignore <start> token\n\n        # Set the AttentionMechanism object with encoder_outputs\n        if self.attention_type:\n            self.decoder.attention_mechanism.setup_memory(enc_output)\n\n            # Create AttentionWrapperState as initial_state for decoder\n            decoder_initial_state = self.decoder.build_initial_state(BATCH_SIZE, tuple(enc_state) ,tf.float32)\n            pred = self.decoder(dec_input, decoder_initial_state)\n        else:\n            #print(\"without_attention\")\n            pred = self.decoder(dec_input, tuple(enc_state))\n        logits = pred.rnn_output\n        loss = self.loss_function(real, logits)\n        self.metric.update_state(real, logits)\n      return loss, self.metric.result().numpy()\n\n    @tf.function\n    def train_step(self,inp, targ, enc_hidden):\n      loss = 0\n\n      with tf.GradientTape() as tape:\n        enc_output, enc_state= self.encoder(inp, enc_hidden)\n\n\n        dec_input = targ[ : , :-1 ] # Ignore <end> token\n        real = targ[ : , 1: ]         # ignore <start> token\n\n        # Set the AttentionMechanism object with encoder_outputs\n\n        if self.attention_type:\n            self.decoder.attention_mechanism.setup_memory(enc_output)\n\n            # Create AttentionWrapperState as initial_state for decoder\n            decoder_initial_state = self.decoder.build_initial_state(BATCH_SIZE, tuple(enc_state) ,tf.float32)\n            pred = self.decoder(dec_input, decoder_initial_state)\n        else:\n            print(\"without_attention\")\n            pred = self.decoder(dec_input, tuple(enc_state))\n        logits = pred.rnn_output\n        loss = self.loss_function(real, logits)\n        self.metric.update_state(real, logits)\n\n      variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n      gradients = tape.gradient(loss, variables)\n      self.optimizer.apply_gradients(zip(gradients, variables))\n\n      return loss, self.metric.result().numpy()\n    \n    def loss_function(self,real, pred):\n        # real shape = (BATCH_SIZE, max_length_output)\n        # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n        loss = cross_entropy(y_true=real, y_pred=pred)\n        mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n        mask = tf.cast(mask, dtype=loss.dtype)\n        loss = mask* loss\n        loss = tf.reduce_mean(loss)\n        return loss\n\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:16:19.694842Z","iopub.execute_input":"2022-06-25T18:16:19.695292Z","iopub.status.idle":"2022-06-25T18:16:19.718344Z","shell.execute_reply.started":"2022-06-25T18:16:19.695230Z","shell.execute_reply":"2022-06-25T18:16:19.716834Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# sample_x","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:15:17.267911Z","iopub.execute_input":"2022-06-25T18:15:17.268954Z","iopub.status.idle":"2022-06-25T18:15:17.275154Z","shell.execute_reply.started":"2022-06-25T18:15:17.268890Z","shell.execute_reply":"2022-06-25T18:15:17.274187Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# # Test decoder stack\n\n# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE,1, \"lstm\", 0.2,0.2,)\n# sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n# # decoder.attention_mechanism.setup_memory(sample_output)\n# # initial_state = decoder.build_initial_state(BATCH_SIZE,tuple(sample_state), tf.float32)\n\n\n# sample_decoder_outputs = decoder(sample_x, tuple(sample_state))\n\n# print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n","metadata":{"id":"DaiO0Z6_Ml1c","outputId":"68f9cfef-f720-4e75-9e08-bba8a546c46b","execution":{"iopub.status.busy":"2022-06-25T18:15:18.496316Z","iopub.execute_input":"2022-06-25T18:15:18.496759Z","iopub.status.idle":"2022-06-25T18:15:18.504420Z","shell.execute_reply.started":"2022-06-25T18:15:18.496724Z","shell.execute_reply":"2022-06-25T18:15:18.502538Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Define the optimizer and the loss function","metadata":{"id":"_ch_71VbIRfK"}},{"cell_type":"code","source":"# optimizer = tf.keras.optimizers.Adam()\n\n\n# def loss_function(real, pred):\n#   # real shape = (BATCH_SIZE, max_length_output)\n#   # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n#   cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n#   loss = cross_entropy(y_true=real, y_pred=pred)\n#   mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n#   mask = tf.cast(mask, dtype=loss.dtype)  \n#   loss = mask* loss\n#   loss = tf.reduce_mean(loss)\n#   return loss  ","metadata":{"id":"WmTHr5iV3jFr","execution":{"iopub.status.busy":"2022-06-25T18:15:19.639395Z","iopub.execute_input":"2022-06-25T18:15:19.640030Z","iopub.status.idle":"2022-06-25T18:15:19.644840Z","shell.execute_reply.started":"2022-06-25T18:15:19.639994Z","shell.execute_reply":"2022-06-25T18:15:19.643319Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Checkpoints (Object-based saving)","metadata":{"id":"DMVWzzsfNl4e"}},{"cell_type":"code","source":"# checkpoint_dir = './training_checkpoints'\n# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n# checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n#                                  encoder=encoder,\n#                                  decoder=decoder)","metadata":{"id":"Zj8bXQTgNwrF","execution":{"iopub.status.busy":"2022-06-25T18:15:25.918510Z","iopub.execute_input":"2022-06-25T18:15:25.919002Z","iopub.status.idle":"2022-06-25T18:15:25.926205Z","shell.execute_reply.started":"2022-06-25T18:15:25.918956Z","shell.execute_reply":"2022-06-25T18:15:25.924539Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## One train_step operations","metadata":{"id":"8Bw95utNiFHa"}},{"cell_type":"code","source":"# @tf.function\n# def val_step(inp, targ, enc_hidden):\n#   loss = 0\n\n#   with tf.GradientTape() as tape:\n#     enc_output, enc_state= encoder(inp, enc_hidden)\n\n\n#     dec_input = targ[ : , :-1 ] # Ignore <end> token\n#     real = targ[ : , 1: ]         # ignore <start> token\n\n#     # Set the AttentionMechanism object with encoder_outputs\n#     if attention:\n#         decoder.attention_mechanism.setup_memory(enc_output)\n\n#         # Create AttentionWrapperState as initial_state for decoder\n#         decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, tuple(enc_state) ,tf.float32)\n#         pred = decoder(dec_input, decoder_initial_state)\n#     else:\n#         print(\"without_attention\")\n#         pred = decoder(dec_input, tuple(enc_state))\n#     logits = pred.rnn_output\n#     loss = loss_function(real, logits)\n#     metric.update_state(real, logits)\n#   return loss, metric.result().numpy()","metadata":{"id":"Rz9qNDCYcWUj","execution":{"iopub.status.busy":"2022-06-25T18:13:34.330225Z","iopub.execute_input":"2022-06-25T18:13:34.331305Z","iopub.status.idle":"2022-06-25T18:13:34.338633Z","shell.execute_reply.started":"2022-06-25T18:13:34.331226Z","shell.execute_reply":"2022-06-25T18:13:34.336704Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# @tf.function\n# def train_step(inp, targ, enc_hidden):\n#   loss = 0\n\n#   with tf.GradientTape() as tape:\n#     enc_output, enc_state= encoder(inp, enc_hidden)\n\n\n#     dec_input = targ[ : , :-1 ] # Ignore <end> token\n#     real = targ[ : , 1: ]         # ignore <start> token\n\n#     # Set the AttentionMechanism object with encoder_outputs\n    \n#     if attention:\n#         decoder.attention_mechanism.setup_memory(enc_output)\n\n#         # Create AttentionWrapperState as initial_state for decoder\n#         decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, tuple(enc_state) ,tf.float32)\n#         pred = decoder(dec_input, decoder_initial_state)\n#     else:\n#         print(\"without_attention\")\n#         pred = decoder(dec_input, tuple(enc_state))\n#     logits = pred.rnn_output\n#     loss = loss_function(real, logits)\n#     metric.update_state(real, logits)\n\n#   variables = encoder.trainable_variables + decoder.trainable_variables\n#   gradients = tape.gradient(loss, variables)\n#   optimizer.apply_gradients(zip(gradients, variables))\n\n#   return loss, metric.result().numpy()","metadata":{"id":"sC9ArXSsVfqn","execution":{"iopub.status.busy":"2022-06-25T18:13:34.631965Z","iopub.execute_input":"2022-06-25T18:13:34.633330Z","iopub.status.idle":"2022-06-25T18:13:34.641051Z","shell.execute_reply.started":"2022-06-25T18:13:34.633249Z","shell.execute_reply":"2022-06-25T18:13:34.639687Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Train the model","metadata":{"id":"pey8eb9piMMg"}},{"cell_type":"code","source":"BUFFER_SIZE = 32000\nBATCH_SIZE = 512\n# Let's limit the #training examples for faster training\n#num_examples = 500\n\ndataset_creator = DakshinaDataset('en-hi')\ntrain_dataset, val_dataset, test_dataset, inp_lang, targ_lang = dataset_creator.call( BUFFER_SIZE, BATCH_SIZE)\n\n\nexample_input_batch, example_target_batch = next(iter(train_dataset))\nexample_input_batch.shape, example_target_batch.shape\n\n\nvocab_inp_size = len(inp_lang.word_index)+1\nvocab_tar_size = len(targ_lang.word_index)+1\nmax_length_input = example_input_batch.shape[1]\nmax_length_output = example_target_batch.shape[1]\n\nembedding_dim = 256\nunits = 1024\n\nprint(\"max_length_english, max_length_hindi, vocab_size_english, vocab_size_hindi\")\nprint(max_length_input, max_length_output, vocab_inp_size, vocab_tar_size)\n\n\ndef train(config = None):\n    with wandb.init(config = config):\n        config = wandb.config\n\n        if config.optimizer == \"adam\":\n            optimizer = tf.keras.optimizers.Adam()\n        else:\n            optimizer = tf.keras.optimizers.RMSprop()\n\n        metric = tf.keras.metrics.SparseCategoricalAccuracy()\n        seq2seq = Seq2Seq(vocab_inp_size,vocab_tar_size,config.encoder_embedding_dim,config.decoder_embedding_dim,config.unit_size,BATCH_SIZE,config.num_of_layer,config.unit_type,config.dropout,config.recurrent_dropout,optimizer,metric)\n        encoder = seq2seq.encoder\n        decoder = seq2seq.decoder\n\n\n        EPOCHS =config.epochs\n        tf.config.run_functions_eagerly(True)\n        checkpoint_dir = './training_checkpoints'\n        checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n        checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                     encoder=encoder,\n                                     decoder=decoder)\n        step_per_val_epoch = dataset_creator.num_of_val//BATCH_SIZE\n        steps_per_epoch = dataset_creator.num_of_train//BATCH_SIZE\n        for epoch in range(EPOCHS):\n            start = time.time()\n            enc_hidden = encoder.initialize_hidden_state()\n            total_loss = 0\n            total_accuracy = 0\n            # print(enc_hidden[0].shape, enc_hidden[1].shape)\n            seq2seq.metric.reset_states()\n            print(\"=\"*80)\n            print(\"TRAINING\")\n            for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n                batch_loss, batch_acc= seq2seq.train_step(inp, targ, enc_hidden)\n                total_loss += batch_loss\n                total_accuracy+=batch_acc\n                if batch % 10 == 0:\n                    #break\n                    print('\\t Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n                                                          batch,\n                                                          batch_loss.numpy(), batch_acc*100 ))\n\n            seq2seq.metric.reset_states()\n            total_val_loss = 0\n            total_val_accuracy = 0\n            print(\"=\"*80)\n            print(\"VALIDATING\")\n            for (batch, (inp, targ)) in enumerate(val_dataset.take(step_per_val_epoch)):\n                val_batch_loss, val_batch_acc= seq2seq.val_step(inp, targ, enc_hidden)\n                total_val_loss += val_batch_loss\n                total_val_accuracy += val_batch_acc\n            print(f\"Validatiion loss:  {total_val_loss/  step_per_val_epoch}\")\n            print((f\"Validatiion Acc:  {(total_val_accuracy/  step_per_val_epoch)*100}\"))\n\n            if (epoch + 1) % 2 == 0:\n                checkpoint.save(file_prefix = checkpoint_prefix)\n            print(\"Accuracy \",(total_accuracy/steps_per_epoch) *100)\n            print('Epoch {} Loss {:.4f} Acc {:.4f}'.format(epoch + 1,\n                                              total_loss / steps_per_epoch,\n                                              (total_accuracy/ steps_per_epoch)*100\n                                              ))\n            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n            wandb.log({\"Epoch\": epoch + 1,\n                \"Train loss\": total_loss / steps_per_epoch,\n                 \"Train Accuracy\": (total_accuracy/steps_per_epoch) *100,\n                 \"Val Accuracy\": (total_val_accuracy/  step_per_val_epoch)*100,\n                 \"Val Loss\": total_val_loss/  step_per_val_epoch\n                })","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:16:20.500151Z","iopub.execute_input":"2022-06-25T18:16:20.500632Z","iopub.status.idle":"2022-06-25T18:16:22.727540Z","shell.execute_reply.started":"2022-06-25T18:16:20.500596Z","shell.execute_reply":"2022-06-25T18:16:22.726068Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"val\ntest\nmax_length_english, max_length_hindi, vocab_size_english, vocab_size_hindi\n22 21 29 66\n","output_type":"stream"}]},{"cell_type":"code","source":"# train()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:16:22.729689Z","iopub.execute_input":"2022-06-25T18:16:22.730053Z","iopub.status.idle":"2022-06-25T18:16:22.735421Z","shell.execute_reply.started":"2022-06-25T18:16:22.730019Z","shell.execute_reply":"2022-06-25T18:16:22.734153Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\nsweep_config = {\n\n    'method':'bayes',\n    'metric': {\n        'name':'Val Accuracy',\n        'goal':'maximize'\n    },\n    'parameters':{\n\n    \"num_of_layer\" : {'values': [1,2,3]},\n    \"unit_size\": {\"values\":[16,64,256]},\n    \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n    \"dropout\": {\"values\": [0.0, 0.2, 0.3]},\n    'recurrent_dropout':{'values':[0.0,0.2,0.3]},\n    \"epochs\":{\"value\":15},\n    \"encoder_embedding_dim\":{\"values\": [64,256, 1024]},\n        \"decoder_embedding_dim\":{\"values\": [64,256, 1024]},\n    \"optimizer\":{\"values\": [\"rmsprop\",\"adam\"]}\n                   }\n}\n\nsweep_id = wandb.sweep(sweep_config, project=\"Sweep_without_Attention2\")\nwandb.agent(sweep_id, train)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:16:24.669975Z","iopub.execute_input":"2022-06-25T18:16:24.670967Z","iopub.status.idle":"2022-06-25T18:24:32.921503Z","shell.execute_reply.started":"2022-06-25T18:16:24.670922Z","shell.execute_reply":"2022-06-25T18:24:32.920127Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Create sweep with ID: ypl9gshh\nSweep URL: https://wandb.ai/aslan/Sweep_without_Attention2/sweeps/ypl9gshh\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i33y9mhy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_embedding_dim: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_embedding_dim: 1024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_layer: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \trecurrent_dropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tunit_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tunit_type: gru\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.12.19 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.18"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220625_181628-i33y9mhy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/aslan/Sweep_without_Attention2/runs/i33y9mhy\" target=\"_blank\">autumn-sweep-1</a></strong> to <a href=\"https://wandb.ai/aslan/Sweep_without_Attention2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/aslan/Sweep_without_Attention2/sweeps/ypl9gshh\" target=\"_blank\">https://wandb.ai/aslan/Sweep_without_Attention2/sweeps/ypl9gshh</a>"},"metadata":{}},{"name":"stdout","text":"================================================================================\nTRAINING\nwithout_attention\n\t Epoch 1 Batch 0 Loss 1.5316 Accuracy 1.3770\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 10 Loss 1.2073 Accuracy 5.8256\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 20 Loss 1.1500 Accuracy 6.2351\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 30 Loss 1.1383 Accuracy 6.5244\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 40 Loss 1.1065 Accuracy 6.8640\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 50 Loss 1.0491 Accuracy 7.1818\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 60 Loss 1.0166 Accuracy 7.4848\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 70 Loss 0.9861 Accuracy 7.7934\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n\t Epoch 1 Batch 80 Loss 0.9436 Accuracy 8.0923\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\n================================================================================\nVALIDATING\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nwithout_attention\nValidatiion loss:  0.8922887444496155\nValidatiion Acc:  10.220431350171566\nAccuracy  6.836344979616792\nEpoch 1 Loss 1.1003 Acc 6.8363\nTime taken for 1 epoch 470.165518283844 sec\n\n================================================================================\nTRAINING\nwithout_attention\n\t Epoch 2 Batch 0 Loss 0.9515 Accuracy 11.2891\nwithout_attention\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁</td></tr><tr><td>Train loss</td><td>▁</td></tr><tr><td>Val Accuracy</td><td>▁</td></tr><tr><td>Val Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>1</td></tr><tr><td>Train Accuracy</td><td>6.83634</td></tr><tr><td>Train loss</td><td>1.10028</td></tr><tr><td>Val Accuracy</td><td>10.22043</td></tr><tr><td>Val Loss</td><td>0.89229</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced <strong style=\"color:#cdcd00\">autumn-sweep-1</strong>: <a href=\"https://wandb.ai/aslan/Sweep_without_Attention2/runs/i33y9mhy\" target=\"_blank\">https://wandb.ai/aslan/Sweep_without_Attention2/runs/i33y9mhy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20220625_181628-i33y9mhy/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# EPOCHS = 40\n# metric = tf.keras.metrics.SparseCategoricalAccuracy()\n# tf.config.run_functions_eagerly(True)\n\n# step_per_val_epoch = 500//BATCH_SIZE\n# steps_per_epoch = 500//BATCH_SIZE\n\n# # step_per_val_epoch = 500//BATCH_SIZE\n# # steps_per_epoch = 500//BATCH_SIZE\n\n# for epoch in range(EPOCHS):\n#   start = time.time()\n  \n#   enc_hidden = encoder.initialize_hidden_state()\n#   total_loss = 0\n#   total_accuracy = 0\n#   # print(enc_hidden[0].shape, enc_hidden[1].shape)\n#   metric.reset_state()\n#   print(\"=\"*80)\n#   print(\"TRAINING\")\n#   for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n#     batch_loss, batch_acc= train_step(inp, targ, enc_hidden)\n#     total_loss += batch_loss\n#     total_accuracy+=batch_acc\n\n#     if batch % 100 == 0:\n#       print('\\t Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n#                                                    batch,\n#                                                    batch_loss.numpy(), batch_acc*100 ))\n#   # saving (checkpoint) the model every 2 epochs\n#   metric.reset_state()\n#   total_val_loss = 0\n#   total_val_accuracy = 0\n#   print(\"=\"*80)\n#   print(\"VALIDATING\")\n#   for (batch, (inp, targ)) in enumerate(val_dataset.take(steps_per_epoch)):\n#     val_batch_loss, val_batch_acc= val_step(inp, targ, enc_hidden)\n#     total_val_loss += val_batch_loss\n#     total_val_accuracy += val_batch_acc\n  \n#   print(f\"Validatiion loss:  {total_val_loss.numpy()/  step_per_val_epoch}\")\n#   print((f\"Validatiion Acc:  {(total_val_accuracy/  step_per_val_epoch)*100}\"))\n#   if (epoch + 1) % 2 == 0:\n#     checkpoint.save(file_prefix = checkpoint_prefix)\n#   print(\"Accuracy \",(total_accuracy/steps_per_epoch) *100)\n#   print('Epoch {} Loss {:.4f} Acc {:.4f}'.format(epoch + 1,\n#                                       total_loss / steps_per_epoch,\n#                                       (total_accuracy/ steps_per_epoch)*100\n#                                       ))\n#   print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))","metadata":{"id":"ddefjBMa3jF0","outputId":"a771c350-d805-47d0-94fc-0fef65fa2df0","scrolled":true,"execution":{"iopub.status.busy":"2022-06-25T18:14:23.577842Z","iopub.execute_input":"2022-06-25T18:14:23.578357Z","iopub.status.idle":"2022-06-25T18:14:23.592464Z","shell.execute_reply.started":"2022-06-25T18:14:23.578298Z","shell.execute_reply":"2022-06-25T18:14:23.590932Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"#sweeping\n","metadata":{"id":"fiAXTLtggXt1"}},{"cell_type":"code","source":"# import pprint\n# sweep_config = {\n    \n#     'method':'bayes',\n#     'metric': {\n#         'name':'Val Accuracy',\n#         'goal':'maximize'\n#     },\n#     'parameters':{\n    \n#     \"num_of_layer\" : {'values': [1,2,3]},\n#     \"unit_size\": {\"values\":[16,32,64]},\n#     \"unit_type\": {\"values\":[\"lstm\",\"rnn\",\"gru\"]},\n#     \"dropout\": {\"values\": [0.0, 0.2, 0.4]},\n#     'recurrent_dropout':{'values':[0.0,0.3]},\n#     \"epochs\":{\"value\":10},\n#     \"encoder_embedding_dim\":{\"values\": [64, 128, 1024]},\n#     \"decoder_embedding_dim\":{\"values\": [64, 128, 1024]},\n#     \"optimizer\":{\"values\": [\"adam\",\"rmsprop\"]}             \n#                    }\n# }\n# pprint.pprint(sweep_config)","metadata":{"id":"mWIPOOy-gXLd","execution":{"iopub.status.busy":"2022-06-25T18:13:04.259700Z","iopub.status.idle":"2022-06-25T18:13:04.260379Z","shell.execute_reply.started":"2022-06-25T18:13:04.259924Z","shell.execute_reply":"2022-06-25T18:13:04.259948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sweep_id = wandb.sweep(sweep_config, project=\"Sweep_with_Attention2\")","metadata":{"id":"JJwKH2M2hM7o","execution":{"iopub.status.busy":"2022-06-25T18:13:04.262063Z","iopub.status.idle":"2022-06-25T18:13:04.262794Z","shell.execute_reply.started":"2022-06-25T18:13:04.262350Z","shell.execute_reply":"2022-06-25T18:13:04.262384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attention = None","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:13:04.265107Z","iopub.status.idle":"2022-06-25T18:13:04.265755Z","shell.execute_reply.started":"2022-06-25T18:13:04.265408Z","shell.execute_reply":"2022-06-25T18:13:04.265436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def train(config = None):\n#     optimizer = tf.keras.optimizers.RMSprop()\n#     attention = None\n\n   \n    \n#     #config = wandb.config\n#     encoder = Encoder(vocab_inp_size, 1024, 64, BATCH_SIZE, 1, \"lstm\", 0.0,0.0)\n#     decoder = Decoder(vocab_tar_size, 1024, 64, BATCH_SIZE, 1, \"lstm\", 0.0,0.0, 'luong')\n#     EPOCHS = 40\n    \n#     checkpoint_dir = './training_checkpoints'\n#     checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n#     checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n#                                      encoder=encoder,\n#                                      decoder=decoder)\n#     metric = tf.keras.metrics.SparseCategoricalAccuracy()\n#     tf.config.run_functions_eagerly(True)\n\n#     step_per_val_epoch = 500//BATCH_SIZE\n#     steps_per_epoch = 500//BATCH_SIZE\n\n#     # step_per_val_epoch = 500//BATCH_SIZE\n#     # steps_per_epoch = 500//BATCH_SIZE\n\n#     for epoch in range(EPOCHS):\n#       start = time.time()\n      \n#       enc_hidden = encoder.initialize_hidden_state()\n#       total_loss = 0\n#       total_accuracy = 0\n#       # print(enc_hidden[0].shape, enc_hidden[1].shape)\n#       metric.reset_states()\n#       print(\"=\"*80)\n#       print(\"TRAINING\")\n#       for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n#         batch_loss, batch_acc= train_step(inp, targ, enc_hidden)\n#         total_loss += batch_loss\n#         total_accuracy+=batch_acc\n\n#         if batch % 100 == 0:\n#           #break\n#           print('\\t Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1,\n#                                                       batch,\n#                                                       batch_loss.numpy(), batch_acc*100 ))\n#       # saving (checkpoint) the model every 2 epochs\n#       metric.reset_states()\n#       total_val_loss = 0\n#       total_val_accuracy = 0\n#       print(\"=\"*80)\n#       print(\"VALIDATING\")\n#       for (batch, (inp, targ)) in enumerate(val_dataset.take(step_per_val_epoch)):\n#         val_batch_loss, val_batch_acc= val_step(inp, targ, enc_hidden)\n#         total_val_loss += val_batch_loss\n#         total_val_accuracy += val_batch_acc\n      \n#       print(f\"Validatiion loss:  {total_val_loss.numpy()/  step_per_val_epoch}\")\n#       print((f\"Validatiion Acc:  {(total_val_accuracy/  step_per_val_epoch)*100}\"))\n#       if (epoch + 1) % 2 == 0:\n#         checkpoint.save(file_prefix = checkpoint_prefix)\n#       print(\"Accuracy \",(total_accuracy/steps_per_epoch) *100)\n#       print('Epoch {} Loss {:.4f} Acc {:.4f}'.format(epoch + 1,\n#                                           total_loss / steps_per_epoch,\n#                                           (total_accuracy/ steps_per_epoch)*100\n#                                           ))\n#       print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n# #       wandb.log({\"Epoch\": epoch + 1,\n# #                 \"Train loss\": total_loss / steps_per_epoch,\n# #                  \"Train Accuracy\": (total_accuracy/steps_per_epoch) *100,\n# #                  \"Val Accuracy\": (total_val_accuracy/  step_per_val_epoch)*100,\n# #                  \"Val Loss\": total_val_loss/  step_per_val_epoch\n# #                 })\n        \n# train()\n# #wandb.agent(sweep_id, train)       ","metadata":{"id":"O368F0QHhWfU","scrolled":true,"execution":{"iopub.status.busy":"2022-06-25T18:13:04.269548Z","iopub.status.idle":"2022-06-25T18:13:04.270328Z","shell.execute_reply.started":"2022-06-25T18:13:04.269870Z","shell.execute_reply":"2022-06-25T18:13:04.269895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use tf-addons BasicDecoder for decoding\n","metadata":{"id":"mU3Ce8M6I3rz"}},{"cell_type":"code","source":"next(iter(val_dataset))","metadata":{"id":"j1b5xs5OfjzS","execution":{"iopub.status.busy":"2022-06-25T18:13:04.272602Z","iopub.status.idle":"2022-06-25T18:13:04.273310Z","shell.execute_reply.started":"2022-06-25T18:13:04.273055Z","shell.execute_reply":"2022-06-25T18:13:04.273079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate_sentence(sentence):\n#   print(\"from evaluate\",sentence)\n#   sentence = dataset_creator.preprocess_sentence(sentence)\n\n#   inputs = [inp_lang.word_index[i] for i in sentence]\n#   inputs = [inputs for _ in range(64)]\n#   inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n#                                                           maxlen=max_length_input,\n#                                                           padding='post')\n#   inputs = tf.convert_to_tensor(inputs)\n#   #print(inputs)\n#   inference_batch_size = 64\n#   result = ''\n\n#   enc_start_state =  [[tf.zeros((inference_batch_size, units)),tf.zeros((inference_batch_size, units))]]*1\n#   enc_out, enc_state  = encoder(inputs, enc_start_state)\n\n#   # dec_h = enc_h\n#   # dec_c = enc_c\n\n#   start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['\\t'])\n#   end_token = targ_lang.word_index['\\n']\n\n#   greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler(decoder.embedding)\n#   if attention:\n#   # Instantiate BasicDecoder object\n#       decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc, maximum_iterations=25)\n#       # Setup Memory in decoder stack\n#       decoder.attention_mechanism.setup_memory(enc_out)\n\n#       # set decoder_initial_state\n#       decoder_initial_state = decoder.build_initial_state(inference_batch_size,tuple(enc_state), tf.float32)\n\n\n#       ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n#       ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n#       ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n\n#       decoder_embedding_matrix = decoder.embedding.variables\n\n#       outputs, _, _ = decoder_instance(None, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n#       return outputs.sample_id.numpy(), outputs\n#   else:\n#       print(\"yaa\")\n#       dec_out = decoder(tf.ones((inference_batch_size, max_length_input)), tuple(enc_state))\n#       outputs = np.argmax(dec_out.rnn_output[0], axis =1 )\n#       #decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoder.decoder_rnn_cell, sampler = greedy_sampler, output_layer=decoder.fc, maximum_iterations=25)\n#       #outputs, _, _ = decoder_instance(None, start_tokens = start_tokens, end_token= end_token, initial_state=tuple(enc_state))\n#       print(outputs)\n#       inv_map = {v: k for k, v in targ_lang.word_index.items()}\n#       print(\"res\",\"\".join([inv_map[int(x)] for x in list(outputs)[:]]))\n      \n#       print(targ_lang.sequences_to_texts(list(outputs)[:]))\n        \n#       return outputs, outputs\n\n# def translate(sentence):\n#   result,_= evaluate_sentence(sentence)\n#   print(\"-\"*80)\n#   print(result[1])\n#   result = \"\".join(\"\".join(targ_lang.sequences_to_texts(result[:1])).split(\" \"))\n#   print('Input: %s' % (sentence))\n#   print('Predicted translation: {}'.format(result))\n# translate('aahiste')","metadata":{"id":"EbQpyYs13jF_","outputId":"c56aa349-021a-4ebd-8c10-c2e88cfb3854","execution":{"iopub.status.busy":"2022-06-25T18:13:04.275633Z","iopub.status.idle":"2022-06-25T18:13:04.276101Z","shell.execute_reply.started":"2022-06-25T18:13:04.275880Z","shell.execute_reply":"2022-06-25T18:13:04.275902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# targ_lang.word_index","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:13:04.277476Z","iopub.status.idle":"2022-06-25T18:13:04.277941Z","shell.execute_reply.started":"2022-06-25T18:13:04.277722Z","shell.execute_reply":"2022-06-25T18:13:04.277744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Restore the latest checkpoint and test","metadata":{"id":"n250XbnjOaqP"}},{"cell_type":"code","source":"# targ_lang.word_index[\"\\t\"]","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:13:04.280613Z","iopub.status.idle":"2022-06-25T18:13:04.281249Z","shell.execute_reply.started":"2022-06-25T18:13:04.280933Z","shell.execute_reply":"2022-06-25T18:13:04.280966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # restoring the latest checkpoint in checkpoint_dir\n# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"id":"UJpT9D5_OgP6","outputId":"cab7a8bb-8f9b-42ab-d466-80b99853b2af","execution":{"iopub.status.busy":"2022-06-25T18:13:04.283161Z","iopub.status.idle":"2022-06-25T18:13:04.283802Z","shell.execute_reply.started":"2022-06-25T18:13:04.283486Z","shell.execute_reply":"2022-06-25T18:13:04.283519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"WYmYhNN_faR5","outputId":"40cbf808-cdb4-4795-eeb2-38b137bdb6c3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# translate(u'esta es mi vida.')","metadata":{"id":"zSx2iM36EZQZ","outputId":"d2b0f03b-454d-452b-eb7e-ead9b77812a5","execution":{"iopub.status.busy":"2022-06-25T18:13:04.291487Z","iopub.status.idle":"2022-06-25T18:13:04.292169Z","shell.execute_reply.started":"2022-06-25T18:13:04.291937Z","shell.execute_reply":"2022-06-25T18:13:04.291961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# translate(u'¿todavia estan en casa?')","metadata":{"id":"A3LLCx3ZE0Ls","outputId":"321c8929-9f30-4b5d-d8da-1592376fe07d","execution":{"iopub.status.busy":"2022-06-25T18:13:04.294691Z","iopub.status.idle":"2022-06-25T18:13:04.296191Z","shell.execute_reply.started":"2022-06-25T18:13:04.295661Z","shell.execute_reply":"2022-06-25T18:13:04.295707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # wrong translation\n# translate(u'trata de averiguarlo.')","metadata":{"id":"DUQVLVqUE1YW","outputId":"419ed858-c13d-487b-f22e-53d1344e17e9","execution":{"iopub.status.busy":"2022-06-25T18:13:04.299115Z","iopub.status.idle":"2022-06-25T18:13:04.299660Z","shell.execute_reply.started":"2022-06-25T18:13:04.299428Z","shell.execute_reply":"2022-06-25T18:13:04.299452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"aBNwVbNo1wnb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Use tf-addons BeamSearchDecoder \n","metadata":{"id":"IRUuNDeY0HiC"}},{"cell_type":"code","source":"# def beam_evaluate_sentence(sentence, beam_width=3):\n#   sentence = dataset_creator.preprocess_sentence(sentence)\n\n#   inputs = [inp_lang.word_index[i] for i in sentence]\n#   inputs = [inputs for _ in range(64)]\n#   inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n#                                                           maxlen=max_length_input,\n#                                                           padding='post')\n#   inputs = tf.convert_to_tensor(inputs)\n#   print(inputs)\n#   inference_batch_size = inputs.shape[0]\n#   result = ''\n\n#   enc_start_state = [[tf.zeros((inference_batch_size, units)),tf.zeros((inference_batch_size, units))]]*1\n#   enc_out, enc_state = encoder(inputs, enc_start_state)\n\n#   start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['\\t'])\n#   end_token = targ_lang.word_index['\\n']\n\n#   # From official documentation\n#   # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n#   # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n#   # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n#   # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n\n#   enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n#   decoder.attention_mechanism.setup_memory(enc_out)\n#   print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n\n#   # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n#   hidden_state = tfa.seq2seq.tile_batch(tuple(enc_state), multiplier=beam_width)\n#   decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n#   decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n\n#   # Instantiate BeamSearchDecoder\n#   decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc, embedding_fn = decoder.embedding)\n#   decoder_embedding_matrix = decoder.embedding.variables[:]\n\n#   # The BeamSearchDecoder object's call() function takes care of everything.\n#   outputs, final_state, sequence_lengths = decoder_instance(None, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n#   # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n#   # The final beam predictions are stored in outputs.predicted_id\n#   # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n#   # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n#   # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n\n  \n#   # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n#   # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n#   # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n#   final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n#   beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n  \n#   return final_outputs.numpy(), beam_scores.numpy()\n# def beam_translate(sentence):\n#   result, beam_scores = beam_evaluate_sentence(sentence)\n#   print(result.shape, beam_scores.shape)\n#   for beam, score in zip(result, beam_scores):\n#     print(beam.shape, score.shape)\n#     output = targ_lang.sequences_to_texts(beam)\n#     output = [a[:a.index('\\n')] for a in output]\n#     beam_score = [a.sum() for a in score]\n#     print('Input: %s' % (sentence))\n#     for i in range(len(output)):\n#       print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n# beam_translate(\"aande\")","metadata":{"id":"AJ-RTQ0hsJNL","outputId":"3564ca64-23c0-46c7-a120-acd49d3dd213","execution":{"iopub.status.busy":"2022-06-25T18:13:04.301576Z","iopub.status.idle":"2022-06-25T18:13:04.302012Z","shell.execute_reply.started":"2022-06-25T18:13:04.301793Z","shell.execute_reply":"2022-06-25T18:13:04.301814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# decoder.fc.get_config()","metadata":{"id":"FxUg4eSOyvQ6","outputId":"44d00eaf-000b-4fbd-d128-19d4c4adfcf1","execution":{"iopub.status.busy":"2022-06-25T18:13:04.305536Z","iopub.status.idle":"2022-06-25T18:13:04.306306Z","shell.execute_reply.started":"2022-06-25T18:13:04.305946Z","shell.execute_reply":"2022-06-25T18:13:04.305982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for (_, (inp, targ) )  in enumerate(train_dataset.take(64)):\n\n#   enc_start_state = [[tf.zeros((64, units)),tf.zeros((64, units))]]*1\n\n#   enc_output, enc_state= encoder(inp , enc_start_state)\n\n\n#   dec_input = targ[ : , :-1 ] # Ignore <end> token\n#   real = targ[ : , 1: ]         # ignore <start> token\n\n#       # Set the AttentionMechanism object with encoder_outputs\n#   decoder.attention_mechanism.setup_memory(enc_output)\n\n#   # Create AttentionWrapperState as initial_state for decoder\n#   decoder_initial_state = decoder.build_initial_state(64, tuple(enc_state) ,tf.float32)\n#   pred = decoder(dec_input, decoder_initial_state)\n","metadata":{"id":"g_LvXGvX8X-O","execution":{"iopub.status.busy":"2022-06-25T18:13:04.308161Z","iopub.status.idle":"2022-06-25T18:13:04.309213Z","shell.execute_reply.started":"2022-06-25T18:13:04.308851Z","shell.execute_reply":"2022-06-25T18:13:04.308879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # get html element\n# def cstr(s, color='black'):\n# \tif s == ' ':\n    \n# \t\treturn \"<text style=color:#000;padding-left:10px;background-color:{}> </text>\".format(color, s)\n# \telse:\n\n# \t\treturn \"<text style=color:#000;background-color:{}>{} </text>\".format(color, s)\n\t\n# # print html\n# def print_color(t):\n# \tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n\n# # get appropriate color for value\n# def get_clr(value):\n# \tcolors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n# \t\t'#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n# \t\t'#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n# \t\t'#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n# \tvalue = int((value * 18) )\n# \t#print(\"color value\",value)\n# \treturn colors[value]\n\n# # sigmoid function\n# def sigmoid(x):\n# \tz = 1/(1 + np.exp(-x)) \n# \treturn z\n","metadata":{"id":"TODnXBleDzzO","execution":{"iopub.status.busy":"2022-06-25T18:13:04.310861Z","iopub.status.idle":"2022-06-25T18:13:04.311419Z","shell.execute_reply.started":"2022-06-25T18:13:04.311124Z","shell.execute_reply":"2022-06-25T18:13:04.311147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"uTsudkrv35MS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def visualize(output_values, result_list, cell_no, predicted_char):\n#     #print( result_list)\n#     #print(\"\\nPredicted Char : \", predicted_char)\n#     print(f\"Importance of {predicted_char}\")\n#     text_colours = []\n#     for i in range(len(result_list)):\n#       #print(i, cell_no)\n#       #print(result_list[i])\n#       #print(output_values[i])\n#       #print(output_values[i][cell_no])\n#       #print(output_values[i][cell_no])\n#       #print(output_values[i][cell_no])\n#       print(int(output_values[i][cell_no]*18))\n#       text = (result_list[i], get_clr(output_values[i][cell_no]))\n#       text_colours.append(text)\n#     print_color(text_colours)","metadata":{"id":"W2qZ8Ml_0-ZL","execution":{"iopub.status.busy":"2022-06-25T18:13:04.313805Z","iopub.status.idle":"2022-06-25T18:13:04.314403Z","shell.execute_reply.started":"2022-06-25T18:13:04.314079Z","shell.execute_reply":"2022-06-25T18:13:04.314111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize([[0.1,0.9,0.9]],['a'],2,'q')","metadata":{"id":"im_mV_8VR8Mk","outputId":"41d638fa-8db1-46fa-cdc0-e886d7af00dc","execution":{"iopub.status.busy":"2022-06-25T18:13:04.316118Z","iopub.status.idle":"2022-06-25T18:13:04.316703Z","shell.execute_reply.started":"2022-06-25T18:13:04.316446Z","shell.execute_reply":"2022-06-25T18:13:04.316469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tx = 0\n# def translate(sentence):\n#   print(sentence)\n#   result, output = evaluate_sentence(sentence)\n#   print(\"-\"*80)\n#   print(result[0])\n#   word_list =\"\".join(targ_lang.sequences_to_texts(result[:1])).split(\" \")\n#   print('Input: %s' % (sentence))\n#   print('Predicted translation: {}'.format(word_list))\n#   #print(output.rnn_output)\n#   print(\"word_list\", word_list)\n#   print(\"result \", result[0])\n#   output_values = []\n#   for time_step in output.rnn_output[0]:\n#     step = []\n#     for char_index in list(result)[0]:\n#       #print(char_index)\n#       step.append(sigmoid(time_step[char_index]))\n#     output_values.append(step)\n#   output_values = np.array(output_values)\n#   #print(output_values.shape)\n#   output_values = output_values.transpose()\n#   scaler = MinMaxScaler()\n#   scaler.fit(output_values)\n#   output_values =scaler.transform(output_values)\n#   #print(output_values.shape)\n#   #print(word_list)\n#   for i,char in enumerate(word_list[:-1]):\n#     visualize(output_values[:i+1], word_list[:i+1], i,char )\n#   return output.rnn_output\n\n# tx =translate('bbjkal')","metadata":{"id":"OilmC68U6zKL","outputId":"619f575f-f682-4b56-a849-5acb30c38aff","execution":{"iopub.status.busy":"2022-06-25T18:13:04.318539Z","iopub.status.idle":"2022-06-25T18:13:04.320023Z","shell.execute_reply.started":"2022-06-25T18:13:04.319658Z","shell.execute_reply":"2022-06-25T18:13:04.319701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# targ_lang.word_index","metadata":{"id":"kjk8922N8tQ9","outputId":"01002dc9-bc4d-4809-998e-e92d8e5140fd","execution":{"iopub.status.busy":"2022-06-25T18:13:04.322096Z","iopub.status.idle":"2022-06-25T18:13:04.322625Z","shell.execute_reply.started":"2022-06-25T18:13:04.322385Z","shell.execute_reply":"2022-06-25T18:13:04.322408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.preprocessing import MinMaxScaler\n\n# def get_connectivity(word):\n#   print(\"Input word : \", word)\n#   inputs = [inp_lang.word_index[i] for i in word]\n#   inputs = [inputs for _ in range(64)]\n#   inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n#                                                           maxlen=max_length_input,\n#                                                           padding='post')\n#   inputs = tf.convert_to_tensor(inputs)\n#   print(inputs)\n#   #print(index_list)\n\n#   enc_start_state = [[tf.zeros((64, units)),tf.zeros((64, units))]]*1\n\n#   enc_output, enc_state= encoder(inp , enc_start_state)\n\n\n#   dec_input = targ[ : , :-1 ] # Ignore <end> token\n#   real = targ[ : , 1: ]         # ignore <start> token\n\n#       # Set the AttentionMechanism object with encoder_outputs\n#   decoder.attention_mechanism.setup_memory(enc_output)\n\n#   # Create AttentionWrapperState as initial_state for decoder\n#   decoder_initial_state = decoder.build_initial_state(64, tuple(enc_state) ,tf.float32)\n#   pred = decoder(dec_input, decoder_initial_state)\n  \n#   output = s2s.call(enc_inp, dec_input)\n#   temp_list = []\n#   #for i in range(len(index_list)):\n#   input_char_list = list(word)\n#   first_prediction = output[0].rnn_output[0]\n#   pred_char_index = (argmax(output[0].rnn_output[0], axis =1))\n#   #print(\"pred_char_index\",pred_char_index)\n#   scaler = MinMaxScaler()\n#   for i,  pred_char in enumerate(index_list):\n    \n#     output_values = []  \n#     for time_step in first_prediction:\n#         #print(time_step.shape)\n        \n#         prob = []\n#         for index in pred_char_index:\n#           #print(index)\n#           prob.append(time_step[index].numpy())\n#         #print(prob)\n#         output_values.append(prob)\n#     scaler.fit(output_values)\n#     output_values  = scaler.transform(output_values)\n#     #print(np.array(output_values).shape)\n#     #print(len(input_char_list))\n#     #print(\"pred_char_index\", pred_char_index)\n#     out_char_list = list(idx_to_word(pred_char_index))\n\n#     temp_list.append(idx_to_word(pred_char_index))\n\n#     visualize(output_values, input_char_list[:i],i, out_char_list[i])\n#   pred_word = \"\".join(out_char_list)\n#   print(f\"\\nTransliterate word of {word[:-1]} is {pred_word[:i]}\")\n# get_connectivity(\"ande\")","metadata":{"id":"xY0NMGTv3_i1","execution":{"iopub.status.busy":"2022-06-25T18:13:04.324457Z","iopub.status.idle":"2022-06-25T18:13:04.325111Z","shell.execute_reply.started":"2022-06-25T18:13:04.324700Z","shell.execute_reply":"2022-06-25T18:13:04.324723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"J6xRXikU4Tuw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# beam_translate(u'¿todavia estan en casa?')","metadata":{"id":"_BezQwENFY3L","outputId":"1a5fe75a-7ad0-43b5-fe59-4cce436ebd1a","execution":{"iopub.status.busy":"2022-06-25T18:13:04.326803Z","iopub.status.idle":"2022-06-25T18:13:04.329074Z","shell.execute_reply.started":"2022-06-25T18:13:04.327675Z","shell.execute_reply":"2022-06-25T18:13:04.327715Z"},"trusted":true},"execution_count":null,"outputs":[]}]}